<!DOCTYPE html><html lang="ko" data-astro-cid-zetdm5md> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name="theme-color" content="#3b82f6"><meta name="color-scheme" content="light dark"><!-- Mobile optimizations --><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="apple-touch-fullscreen" content="yes"><meta name="format-detection" content="telephone=no,date=no,email=no,address=no"><!-- Icons and Manifest --><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="manifest" href="/manifest.json"><!-- PWA iOS Meta Tags --><meta name="apple-mobile-web-app-title" content="Jay's Blog"><link rel="apple-touch-icon" sizes="57x57" href="/icon-57x57.png"><link rel="apple-touch-icon" sizes="72x72" href="/icon-72x72.png"><link rel="apple-touch-icon" sizes="96x96" href="/icon-96x96.png"><link rel="apple-touch-icon" sizes="128x128" href="/icon-128x128.png"><link rel="apple-touch-icon" sizes="144x144" href="/icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/icon-180x180.png"><link rel="apple-touch-icon" sizes="192x192" href="/icon-192x192.png"><!-- RSS and Sitemap --><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Jay Lee - All Posts" href="https://jayleekr.github.io/rss.xml"><link rel="alternate" type="application/rss+xml" title="Jay Lee - English Posts" href="https://jayleekr.github.io/rss/en.xml"><!-- Preload Critical Resources --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Performance Optimization: Font Loading --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="//github.com"><!-- Canonical URL --><link rel="canonical" href="https://jayleekr.github.io/404/"><!-- Primary Meta Tags --><title>페이지를 찾을 수 없습니다</title><meta name="title" content="페이지를 찾을 수 없습니다"><meta name="description" content="요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요!"><meta name="author" content="Jay Lee"><meta name="robots" content="index, follow"><meta name="language" content="ko-KR"><meta name="generator" content="Astro v5.12.3"><!-- Google Search Console Verification --><meta name="google-site-verification" content=""><!-- To be filled with actual verification code when setting up GSC --><!-- Bing Webmaster Tools Verification --><meta name="msvalidate.01" content=""><!-- Yandex Verification --><meta name="yandex-verification" content=""><!-- Enhanced Keywords for articles --><meta name="keywords" content="Jay Lee, Software Engineer, Basketball, Technology, Development, Programming, Blog, Tutorial"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:site_name" content="Jay Lee"><meta property="og:url" content="https://jayleekr.github.io/404/"><meta property="og:title" content="페이지를 찾을 수 없습니다"><meta property="og:description" content="요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요!"><meta property="og:image" content="https://jayleekr.github.io/assets/blog-placeholder-1.Bx0Zcyzv.jpg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image:alt" content="페이지를 찾을 수 없습니다"><meta property="og:image:type" content="image/jpeg"><meta property="og:locale" content="ko_KR"><meta property="og:locale:alternate" content="en_US"><!-- Article specific Open Graph --><!-- Twitter --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@jayleekr"><meta name="twitter:creator" content="@jayleekr"><meta name="twitter:url" content="https://jayleekr.github.io/404/"><meta name="twitter:title" content="페이지를 찾을 수 없습니다"><meta name="twitter:description" content="요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요!"><meta name="twitter:image" content="https://jayleekr.github.io/assets/blog-placeholder-1.Bx0Zcyzv.jpg"><meta name="twitter:image:alt" content="페이지를 찾을 수 없습니다"><!-- Enhanced SEO Meta Tags --><meta name="format-detection" content="telephone=no,date=no,email=no,address=no"><meta name="revisit-after" content="7 days"><meta name="distribution" content="global"><meta name="rating" content="general"><meta name="referrer" content="origin-when-cross-origin"><!-- Performance hints --><meta name="renderer" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge"><!-- Security headers --><meta http-equiv="X-Content-Type-Options" content="nosniff"><meta http-equiv="X-Frame-Options" content="DENY"><!-- Geo meta tags --><meta name="geo.region" content="KR"><meta name="geo.placename" content="Seoul, South Korea"><!-- Content classification --><meta name="classification" content="Technology Blog"><meta name="category" content="Software Engineering, Basketball, Technology"><!-- Theme Initialization Script (prevents FOUC) --><script>
	// Check for saved theme preference or default to system preference
	(function() {
		function getStoredTheme() {
			if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
				return localStorage.getItem('theme');
			}
			return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
		}
		
		const theme = getStoredTheme();
		
		if (theme === 'dark') {
			document.documentElement.classList.add('dark');
		} else {
			document.documentElement.classList.remove('dark');
		}
		
		// Add no-transition class temporarily to prevent animation on page load
		document.documentElement.classList.add('no-transition');
		
		// Remove no-transition class after page load
		window.addEventListener('load', () => {
			document.documentElement.classList.remove('no-transition');
		});
	})();
</script><!-- Enhanced JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"페이지를 찾을 수 없습니다","description":"요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요!","url":"https://jayleekr.github.io/404/","potentialAction":{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https://jayleekr.github.io/?search={search_term_string}"},"query-input":"required name=search_term_string"},"author":{"@type":"Person","name":"Jay Lee","url":"https://jayleekr.github.io/","gender":"Male","jobTitle":"Software Engineer","workFor":{"@type":"Organization","name":"Sonatus"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University Name"}],"knowsAbout":["Software Engineering","Web Development","Basketball","Startup"],"sameAs":["https://github.com/jayleekr","https://linkedin.com/in/jayleekr"]},"publisher":{"@type":"Organization","name":"Jay Lee","url":"https://jayleekr.github.io/","description":"요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요!","founder":{"@type":"Person","name":"Jay Lee","url":"https://jayleekr.github.io/","gender":"Male","jobTitle":"Software Engineer","workFor":{"@type":"Organization","name":"Sonatus"},"alumniOf":[{"@type":"CollegeOrUniversity","name":"University Name"}],"knowsAbout":["Software Engineering","Web Development","Basketball","Startup"],"sameAs":["https://github.com/jayleekr","https://linkedin.com/in/jayleekr"]},"logo":{"@type":"ImageObject","url":"https://jayleekr.github.io/favicon.svg","width":32,"height":32},"sameAs":["https://github.com/jayleekr","https://linkedin.com/in/jayleekr"]},"inLanguage":["ko-KR","en-US"],"audience":{"@type":"Audience","audienceType":"Software Developers, Basketball Enthusiasts"}}</script><!-- Additional Schema.org markup for breadcrumbs --><!-- Google Analytics --><!-- Development reminder -->
  <script>
    if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
      console.log('GA4 Analytics: Set GOOGLE_ANALYTICS_ID environment variable to enable tracking');
    }
  </script><!-- Advanced Conversion Tracking --><!-- Web Vitals Performance Monitoring --><script>
  // Web Vitals 측정 및 Google Analytics로 전송
  function sendToGoogleAnalytics({name, delta, value, id}) {
    // GA4가 로드되어 있는 경우에만 전송
    if (typeof gtag !== 'undefined') {
      gtag('event', name, {
        event_category: 'Web Vitals',
        event_label: id,
        value: Math.round(name === 'CLS' ? delta * 1000 : delta),
        non_interaction: true,
      });
    }
    
    // 콘솔에도 로그 (개발 환경에서)
    if (window.location.hostname === 'localhost') {
      console.log(`${name}: ${value} (delta: ${delta})`);
    }
  }

  // Web Vitals 라이브러리를 동적으로 로드
  async function measureWebVitals() {
    try {
      const { getCLS, getFID, getFCP, getLCP, getTTFB } = await import('https://unpkg.com/web-vitals@3/dist/web-vitals.js');
      
      getCLS(sendToGoogleAnalytics);
      getFID(sendToGoogleAnalytics);
      getFCP(sendToGoogleAnalytics);
      getLCP(sendToGoogleAnalytics);
      getTTFB(sendToGoogleAnalytics);
    } catch (error) {
      console.warn('Web Vitals measurement failed:', error);
    }
  }

  // 페이지 로드 후 측정 시작
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', measureWebVitals);
  } else {
    measureWebVitals();
  }
</script><!-- PWA Service Worker Registration --><script>
	// Register service worker for PWA functionality
	if ('serviceWorker' in navigator) {
		window.addEventListener('load', async () => {
			try {
				const registration = await navigator.serviceWorker.register('/sw.js');
				console.log('[PWA] Service Worker registered successfully:', registration.scope);
				
				// Handle service worker updates
				registration.addEventListener('updatefound', () => {
					const newWorker = registration.installing;
					if (newWorker) {
						newWorker.addEventListener('statechange', () => {
							if (newWorker.state === 'installed' && navigator.serviceWorker.controller) {
								// New version available, show update notification
								if (confirm('새로운 버전이 사용 가능합니다. 지금 업데이트하시겠습니까?')) {
									newWorker.postMessage({ action: 'skipWaiting' });
									window.location.reload();
								}
							}
						});
					}
				});
				
				// Listen for service worker messages
				navigator.serviceWorker.addEventListener('message', (event) => {
					if (event.data.action === 'reload') {
						window.location.reload();
					}
				});
				
			} catch (error) {
				console.error('[PWA] Service Worker registration failed:', error);
			}
		});
	}
	
	// Install prompt handling
	let deferredPrompt;
	window.addEventListener('beforeinstallprompt', (e) => {
		// Prevent Chrome 67 and earlier from automatically showing the prompt
		e.preventDefault();
		// Stash the event so it can be triggered later
		deferredPrompt = e;
		
		// Show install button if not already installed
		if (!window.matchMedia('(display-mode: standalone)').matches) {
			showInstallButton();
		}
	});
	
	function showInstallButton() {
		// Create install button dynamically
		const installButton = document.createElement('button');
		installButton.textContent = '앱 설치';
		installButton.className = 'fixed bottom-20 right-4 z-50 bg-primary-600 text-white px-4 py-2 rounded-lg shadow-lg hover:bg-primary-700 transition-colors';
		installButton.style.display = 'none';
		
		// Show button after 5 seconds on mobile
		if (window.innerWidth < 768) {
			setTimeout(() => {
				installButton.style.display = 'block';
			}, 5000);
		}
		
		installButton.addEventListener('click', async () => {
			if (deferredPrompt) {
				// Show the install prompt
				deferredPrompt.prompt();
				// Wait for the user to respond to the prompt
				const { outcome } = await deferredPrompt.userChoice;
				console.log(`[PWA] User response to the install prompt: ${outcome}`);
				// Clear the deferredPrompt variable
				deferredPrompt = null;
				// Hide the install button
				installButton.style.display = 'none';
			}
		});
		
		document.body.appendChild(installButton);
	}
	
	// Handle app installed event
	window.addEventListener('appinstalled', () => {
		console.log('[PWA] App was installed successfully');
		// Hide install button if visible
		const installButton = document.querySelector('button[style*="bg-primary-600"]');
		if (installButton) {
			installButton.style.display = 'none';
		}
	});
</script><!-- Code Enhancement Script --><script src="/scripts/code-enhancements.js" defer></script><!-- Mobile Enhancement Script --><script src="/scripts/mobile-enhancements.js" defer></script><link rel="stylesheet" href="/assets/_slug_.CjWJjBV3.css">
<style>@keyframes sway{0%,to{transform:rotate(-2deg)}50%{transform:rotate(2deg)}}@keyframes blink{0%,90%,to{opacity:1}95%{opacity:0}}@keyframes talk{0%,to{transform:translate(-50%) scaleX(1)}50%{transform:translate(-50%) scaleX(1.2)}}@keyframes wave{0%,to{transform:rotate(0)}50%{transform:rotate(-20deg)}}@keyframes wave-reverse{0%,to{transform:rotate(0)}50%{transform:rotate(20deg)}}@keyframes float{0%,to{transform:translateY(0)}50%{transform:translateY(-10px)}}.animate-sway[data-astro-cid-zetdm5md]{animation:sway 3s ease-in-out infinite}.animate-blink[data-astro-cid-zetdm5md]{animation:blink 4s infinite}.animate-talk[data-astro-cid-zetdm5md]{animation:talk 2s ease-in-out infinite}.animate-wave[data-astro-cid-zetdm5md]{animation:wave 2s ease-in-out infinite}.animate-wave-reverse[data-astro-cid-zetdm5md]{animation:wave-reverse 2s ease-in-out infinite}.animate-float[data-astro-cid-zetdm5md]{animation:float 3s ease-in-out infinite}@media (prefers-reduced-motion: reduce){.animate-bounce[data-astro-cid-zetdm5md],.animate-sway[data-astro-cid-zetdm5md],.animate-blink[data-astro-cid-zetdm5md],.animate-talk[data-astro-cid-zetdm5md],.animate-wave[data-astro-cid-zetdm5md],.animate-wave-reverse[data-astro-cid-zetdm5md],.animate-float[data-astro-cid-zetdm5md]{animation:none!important}}
html{transition:background-color .3s ease,color .3s ease}html *{transition:background-color .3s ease,color .3s ease,border-color .3s ease}html.no-transition *{transition:none!important}@keyframes theme-sparkle{0%{opacity:0;transform:scale(0) translateY(0) rotate(0)}50%{opacity:1;transform:scale(1.2) translateY(-20px) rotate(180deg)}to{opacity:0;transform:scale(.5) translateY(-40px) rotate(360deg)}}
.line-clamp-2[data-astro-cid-axnn5did]{display:-webkit-box;-webkit-line-clamp:2;-webkit-box-orient:vertical;overflow:hidden}mark[data-astro-cid-axnn5did]{background-color:#fef08acc}.dark[data-astro-cid-axnn5did] mark[data-astro-cid-axnn5did]{background-color:#854d0ecc;color:#fef08a}
</style></head> <body class="bg-white dark:bg-gray-900 transition-colors duration-300" data-astro-cid-zetdm5md> <!-- Skip to main content link for accessibility --><a href="#main-content" class="skip-to-content"> 본문으로 건너뛰기 </a> <header class="bg-white dark:bg-gray-900 border-b border-gray-200 dark:border-gray-700 sticky top-0 z-40"> <nav class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex justify-between items-center h-16"> <div class="flex items-center"> <a href="/" class="flex items-center gap-3 group" aria-label="Home"> <svg class="w-10 h-10 transition-transform group-hover:scale-110" viewBox="0 0 100 100" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Jay Lee Logo"> <!-- Background circle --> <circle cx="50" cy="50" r="45" class="fill-primary-500 dark:fill-primary-400"></circle> <!-- J letter --> <path d="M 35 25 L 35 60 Q 35 75 50 75 Q 65 75 65 60 L 65 40" class="stroke-white dark:stroke-gray-900" stroke-width="6" stroke-linecap="round" fill="none"></path> <!-- L letter (overlapping) --> <path d="M 40 35 L 40 65 L 60 65" class="stroke-white dark:stroke-gray-900 opacity-80" stroke-width="5" stroke-linecap="round" fill="none"></path> <!-- Decorative dot --> <circle cx="70" cy="30" r="4" class="fill-white dark:fill-gray-900"></circle> </svg> <span class="text-xl font-bold text-gray-900 dark:text-white group-hover:text-primary-600 dark:group-hover:text-primary-400 transition-colors"> Jay Lee </span> </a> </div> <div class="hidden md:flex items-center space-x-8"> <a href="/" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500"> 홈 </a> <a href="/about" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500"> 소개 </a> <a href="/blog" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500"> Blog </a> <a href="/categories" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500"> Categories </a> <button id="open-search" class="inline-flex items-center gap-2 rounded-lg border border-gray-300 dark:border-gray-600 bg-white dark:bg-gray-800 px-3 py-2 text-sm text-gray-500 dark:text-gray-400 hover:bg-gray-50 dark:hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-primary-500 transition-colors" aria-label="사이트 검색" title="사이트 검색 (⌘K)"> <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path> </svg> <span class="hidden sm:inline">검색</span> <kbd class="hidden sm:inline-flex h-5 w-5 items-center justify-center rounded border border-gray-200 dark:border-gray-700 bg-gray-100 dark:bg-gray-900 text-xs font-semibold text-gray-600 dark:text-gray-400">⌘K</kbd> </button> <button id="theme-toggle" data-testid="theme-toggle" type="button" class="inline-flex items-center justify-center p-2 rounded-lg text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-primary-500 focus:ring-offset-2 focus:ring-offset-white dark:focus:ring-offset-gray-900 transition-all duration-200 hover:scale-110 hover:rotate-12 whimsy-button" aria-label="Toggle between light and dark mode" aria-pressed="false" title="Toggle theme (current: light)" role="switch" data-astro-cid-x3pjskd3> <!-- Sun icon (visible in dark mode) --> <svg id="theme-toggle-dark-icon" class="hidden w-5 h-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" data-astro-cid-x3pjskd3> <path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd" data-astro-cid-x3pjskd3></path> </svg> <!-- Moon icon (visible in light mode) --> <svg id="theme-toggle-light-icon" class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" data-astro-cid-x3pjskd3> <path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z" data-astro-cid-x3pjskd3></path> </svg> </button> <script type="module">document.addEventListener("DOMContentLoaded",()=>{const n=document.getElementById("theme-toggle"),r=document.getElementById("theme-toggle-dark-icon"),i=document.getElementById("theme-toggle-light-icon");function s(e){const t=document.createElement("div");t.setAttribute("aria-live","polite"),t.setAttribute("aria-atomic","true"),t.className="sr-only",t.textContent=e,document.body.appendChild(t),setTimeout(()=>{document.body.removeChild(t)},1e3)}if(!n||!r||!i)return;function m(){return typeof localStorage<"u"&&localStorage.getItem("theme")?localStorage.getItem("theme")||"light":window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function a(e){e==="dark"?(document.documentElement.classList.add("dark"),i?.classList.add("hidden"),r?.classList.remove("hidden"),n?.setAttribute("aria-pressed","true"),n?.setAttribute("title","Switch to light mode (current: dark)"),n?.setAttribute("aria-label","Switch to light mode")):(document.documentElement.classList.remove("dark"),i?.classList.remove("hidden"),r?.classList.add("hidden"),n?.setAttribute("aria-pressed","false"),n?.setAttribute("title","Switch to dark mode (current: light)"),n?.setAttribute("aria-label","Switch to dark mode")),typeof localStorage<"u"&&localStorage.setItem("theme",e),s(e==="dark"?"Dark mode enabled":"Light mode enabled")}const l=m();a(l);function h(e,t){const o=e.getBoundingClientRect(),u=t==="dark"?"🌙":"☀️";for(let c=0;c<8;c++)setTimeout(()=>{const d=document.createElement("div");d.textContent=u,d.style.cssText=`
						position: fixed;
						left: ${o.left+o.width/2+(Math.random()-.5)*60}px;
						top: ${o.top+o.height/2+(Math.random()-.5)*60}px;
						font-size: 16px;
						pointer-events: none;
						z-index: 9999;
						animation: theme-sparkle 1.5s ease-out forwards;
					`,document.body.appendChild(d),setTimeout(()=>{d.parentNode&&d.parentNode.removeChild(d)},1500)},c*100)}n.addEventListener("click",()=>{const t=(document.documentElement.classList.contains("dark")?"dark":"light")==="dark"?"light":"dark";a(t),h(n,t)}),n.addEventListener("keydown",e=>{if(e.key===" "||e.key==="Enter"){e.preventDefault();const o=(document.documentElement.classList.contains("dark")?"dark":"light")==="dark"?"light":"dark";a(o)}}),typeof window<"u"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{localStorage.getItem("theme")||a(e.matches?"dark":"light")})});</script>  <div class="relative"> <button id="language-switcher" class="flex items-center gap-2 px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-200 hover:text-gray-900 dark:hover:text-white transition-colors" aria-expanded="false" aria-haspopup="true"> <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 5h12M9 3v2m1.048 9.5A18.022 18.022 0 016.412 9m6.088 9h7M11 21l5-10 5 10M12.751 5C11.783 10.77 8.07 15.61 3 18.129"></path> </svg> <span>한국어</span> <svg class="w-4 h-4 transition-transform" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path> </svg> </button> <div id="language-menu" class="absolute right-0 mt-2 w-32 bg-white dark:bg-gray-800 rounded-md shadow-lg border border-gray-200 dark:border-gray-700 invisible opacity-0 transition-all duration-200 z-50"> <a href="/en/404/" class="block px-4 py-2 text-sm text-gray-700 dark:text-gray-200 hover:bg-gray-100 dark:hover:bg-gray-700 transition-colors"> English </a> </div> </div> <script type="module">document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("language-switcher"),t=document.getElementById("language-menu");e&&t&&(e.addEventListener("click",()=>{const i=e.getAttribute("aria-expanded")==="true";e.setAttribute("aria-expanded",(!i).toString()),i?t.classList.add("invisible","opacity-0"):t.classList.remove("invisible","opacity-0")}),document.addEventListener("click",i=>{!e.contains(i.target)&&!t.contains(i.target)&&(e.setAttribute("aria-expanded","false"),t.classList.add("invisible","opacity-0"))}))});</script> </div> <!-- Mobile menu button --> <div class="md:hidden flex items-center gap-4"> <button id="open-search" class="inline-flex items-center gap-2 rounded-lg border border-gray-300 dark:border-gray-600 bg-white dark:bg-gray-800 px-3 py-2 text-sm text-gray-500 dark:text-gray-400 hover:bg-gray-50 dark:hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-primary-500 transition-colors" aria-label="사이트 검색" title="사이트 검색 (⌘K)"> <svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path> </svg> <span class="hidden sm:inline">검색</span> <kbd class="hidden sm:inline-flex h-5 w-5 items-center justify-center rounded border border-gray-200 dark:border-gray-700 bg-gray-100 dark:bg-gray-900 text-xs font-semibold text-gray-600 dark:text-gray-400">⌘K</kbd> </button> <button id="theme-toggle" data-testid="theme-toggle" type="button" class="inline-flex items-center justify-center p-2 rounded-lg text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-primary-500 focus:ring-offset-2 focus:ring-offset-white dark:focus:ring-offset-gray-900 transition-all duration-200 hover:scale-110 hover:rotate-12 whimsy-button" aria-label="Toggle between light and dark mode" aria-pressed="false" title="Toggle theme (current: light)" role="switch" data-astro-cid-x3pjskd3> <!-- Sun icon (visible in dark mode) --> <svg id="theme-toggle-dark-icon" class="hidden w-5 h-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" data-astro-cid-x3pjskd3> <path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd" data-astro-cid-x3pjskd3></path> </svg> <!-- Moon icon (visible in light mode) --> <svg id="theme-toggle-light-icon" class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" data-astro-cid-x3pjskd3> <path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z" data-astro-cid-x3pjskd3></path> </svg> </button>   <div class="relative"> <button id="language-switcher" class="flex items-center gap-2 px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-200 hover:text-gray-900 dark:hover:text-white transition-colors" aria-expanded="false" aria-haspopup="true"> <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 5h12M9 3v2m1.048 9.5A18.022 18.022 0 016.412 9m6.088 9h7M11 21l5-10 5 10M12.751 5C11.783 10.77 8.07 15.61 3 18.129"></path> </svg> <span>한국어</span> <svg class="w-4 h-4 transition-transform" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path> </svg> </button> <div id="language-menu" class="absolute right-0 mt-2 w-32 bg-white dark:bg-gray-800 rounded-md shadow-lg border border-gray-200 dark:border-gray-700 invisible opacity-0 transition-all duration-200 z-50"> <a href="/en/404/" class="block px-4 py-2 text-sm text-gray-700 dark:text-gray-200 hover:bg-gray-100 dark:hover:bg-gray-700 transition-colors"> English </a> </div> </div>  <button id="mobile-menu-button" class="inline-flex items-center justify-center p-2 rounded-md text-gray-400 hover:text-gray-500 hover:bg-gray-100 dark:hover:bg-gray-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-primary-500" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="block h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path> </svg> <svg class="hidden h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path> </svg> </button> </div> </div> <!-- Mobile menu --> <div id="mobile-menu" class="md:hidden hidden"> <div class="px-2 pt-2 pb-3 space-y-1 sm:px-3 border-t border-gray-200 dark:border-gray-700"> <a href="/" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500 block px-3 py-2 text-base font-medium"> 홈 </a> <a href="/about" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500 block px-3 py-2 text-base font-medium"> 소개 </a> <a href="/blog" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500 block px-3 py-2 text-base font-medium"> Blog </a> <a href="/categories" class="px-3 py-2 text-sm font-medium text-gray-700 dark:text-gray-300 hover:text-primary-600 dark:hover:text-primary-400 transition-colors border-b-2 border-transparent hover:border-primary-500 block px-3 py-2 text-base font-medium"> Categories </a> </div> </div> </nav> </header> <!-- Global Search Modal --> <!-- Search Modal --><div id="search-modal" class="fixed inset-0 z-50 hidden bg-black bg-opacity-50 backdrop-blur-sm" role="dialog" aria-modal="true" aria-labelledby="search-title" data-astro-cid-axnn5did> <div class="flex min-h-full items-start justify-center p-4 pt-16" data-astro-cid-axnn5did> <div class="w-full max-w-2xl rounded-lg bg-white dark:bg-gray-800 shadow-xl" data-astro-cid-axnn5did> <!-- Search Header --> <div class="border-b border-gray-200 dark:border-gray-700 p-4" data-astro-cid-axnn5did> <div class="flex items-center justify-between" data-astro-cid-axnn5did> <h2 id="search-title" class="text-lg font-semibold text-gray-900 dark:text-gray-100" data-astro-cid-axnn5did> 사이트 검색 </h2> <button id="close-search" class="rounded-lg p-2 text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 hover:text-gray-500 dark:hover:text-gray-300" aria-label="검색 닫기" data-astro-cid-axnn5did> <svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-axnn5did> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" data-astro-cid-axnn5did></path> </svg> </button> </div> <!-- Search Input --> <div class="relative mt-4" data-astro-cid-axnn5did> <input id="global-search-input" type="text" placeholder="검색어를 입력하세요..." class="w-full rounded-lg border border-gray-300 dark:border-gray-600 bg-white dark:bg-gray-900 px-4 py-3 pl-10 text-gray-900 dark:text-gray-100 placeholder-gray-500 dark:placeholder-gray-400 focus:border-primary-500 focus:outline-none focus:ring-2 focus:ring-primary-500" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" data-search-input data-astro-cid-axnn5did> <svg class="absolute left-3 top-1/2 h-4 w-4 -translate-y-1/2 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-axnn5did> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" data-astro-cid-axnn5did></path> </svg> </div> <!-- Search Stats --> <div class="mt-2 text-sm text-gray-500 dark:text-gray-400" data-astro-cid-axnn5did> <span id="search-stats" data-astro-cid-axnn5did>결과를 보려면 검색어를 입력하세요</span> </div> </div> <!-- Search Results --> <div class="max-h-96 overflow-y-auto" data-astro-cid-axnn5did> <div id="search-results" class="p-4" data-astro-cid-axnn5did> <!-- Results will be populated here --> </div> <!-- No Results Message --> <div id="no-search-results" class="hidden p-8 text-center" data-astro-cid-axnn5did> <svg class="mx-auto h-12 w-12 text-gray-300 dark:text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-axnn5did> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" data-astro-cid-axnn5did></path> </svg> <h3 class="mt-2 text-sm font-medium text-gray-900 dark:text-gray-100" data-astro-cid-axnn5did>검색 결과가 없습니다</h3> <p class="mt-1 text-sm text-gray-500 dark:text-gray-400" data-astro-cid-axnn5did>다른 검색어로 시도해보세요</p> </div> <!-- Recent Searches --> <div id="recent-searches" class="border-t border-gray-200 dark:border-gray-700 p-4" data-astro-cid-axnn5did> <h3 class="text-sm font-medium text-gray-900 dark:text-gray-100 mb-2" data-astro-cid-axnn5did>최근 검색</h3> <div id="recent-searches-list" class="space-y-1" data-astro-cid-axnn5did> <!-- Recent searches will be populated here --> </div> </div> </div> </div> </div> </div> <script>(function(){const searchIndex = [{"id":"collaboration/toyprojects/2019-04-17-toyproject1","title":"토이 프로젝트 #1","description":"","content":"### [J-YoutubeDL는 뭔가유?]\n\n유튜브 링크(URL)을 복사해서 붙여넣으면 해당 유튜브 링크에서 지원하는 영상 및 음악 퀄리티들을 원하는대루 다운가능한 Windows용 프로그램이다.\n\n### [왜만들었나유?]\n\n웹에서 링크입력해서 이용하다가 다른 퀄리티로 받고싶어서 직접 만듬\n\n### [설치&실행 어찌하나유?]\n\n[**설치 파일 링크**](https://github.com/jayleekr/YoutubeDownloaderWPF/releases/tag/1.0.1904)\n\n![Desktop View](/assets/img/post1/github.png)\n\n위 링크에 들어가면 소스코드부터 설치파일, .Net40 다 있다.\n\ndotnetfx40 어쩌구 프로그램은 윈도우 피씨에 해당 프레임워크가 깔려있으면 안깔아도되고, 심지어 설치할때 웹으로 연결해서 자동 다운을 받아준다. 혹시나 해서 올려놓은 파일이니 필요한 사람은 다운받길 바란다. (**dotNetFx40_Client_x86_x64.exe**)\n\n자 Release1.0.1904.zip을 다운받아 압축을 풀고 setup.exe를 틀어보면 아래와 같은 화면이 뜬당.\n\n![Desktop View](/assets/img/post1/setup.png)\n\n기호에 맞게 설치위치를 설정하고 웬만하면 그냥 다음 다음을 눌러서 설치를 마치자.\n\n설치를 마치면 아래와 같은 아이콘이 바탕화면과 시작프로그램에에 생성된다.\n\n![Desktop View](/assets/img/post1/icon.png)\n\n프로그램을 실행하면 아래와 같은 화면이 나온다.\n![Desktop View](/assets/img/post1/main.png)\n\n### [하우투 사용?]\n\n사용법은 간단하다. <br/>\n<br/>\nwww.youtube.com 에 접속한 후 원하는 영상의 url을 클립보드에 복사한 후 프로그램의 URL 칸에 붙여넣기 하면 해당 영상의 추출 가능한 영상 및 음악 포멧과 품질 리스트가 생성된다.<br/>\n![Desktop View](/assets/img/post1/howtogeturl.png)\n\n<br/>\n<br/>\n<br/>\n\n원하는 세팅값을 선택하고 GO! 버튼을 누르면 \"저장위치\" 에 파일을 생성한다.\n![Desktop View](/assets/img/post1/workwell1.png)\n![Desktop View](/assets/img/post1/workwell2.png)\n\n\n참고로 \"저장위치\" 의 기본값은 프로그램의 **설치위치** 이다.\n\n원하는 위치로 변경하길 권장한다.\n\n매우 간단한 기능만 포함하고 있으니 원하시는 기능을 **댓글** 로 남겨주시면 심심할 때 또 구현해서 넣기로 하겠다.\n\n그럼 Adios","categories":["Collaboration","ToyProjects"],"tags":["ToyProjects","C#","Windows","Youtube","Downloader","유튜브","다운로더","유튜버"],"pubDate":"2019-04-17T03:00:00.000Z","url":"/blog/collaboration/toyprojects/2019-04-17-toyproject1/"},{"id":"deepthinking/ai/2021-04-10-preferrednetworks","title":"Preferred Networks 소개","description":"","content":"## About this post\n\n이 글은 Bain&Company 출신 Jihyo Lee님의 포스팅을 퍼온 것입니다.\n\n## Preferred Networks\n\n최근에 본 무수히 많은 AI 스타트업 중에 가장 스케일이 크고 대담한 업체. 일본기업이라 숨어있어서 그렇지 엄청나다 못해 황당한 수준이다. \n\n초기엔 Tensorflow같은 AI framework을 자체적으로 만들었고, 아래로는 CuDNN같은 library를, 위로는 AI 모델의 hyperparameter 최적화하는 tool 등으로 확장하더니만 아예 각잡고 자기 전용 GPU cluster를 짓고 나서는, 결국엔 전용 AI chip까지 찍어서 AI슈퍼컴퓨터 cluster를 구축. Chip도 대단한데 2018년 찍은 칩인데 FP32 성능이 131 TFLOPS다. \n\n참고로 NVIDIA의 최신 A100이 FP32 성능은 19 TFLOPS. Chiplet써서 die 네개 붙이는 방식의 구현도 감탄스럽고... 이걸로 만든 cluster는 아예 2020년 기준 글로벌 슈퍼컴퓨터 Green500 순위 공인 1위. (한국에서 AI 칩만든다고 떠드는 애들은 쎄고 쌨지만 제대로 나온거 하나도 없고 제대로 된 training 만드는 애는 아예 0인거 생각하면 참...)\n\n이 모든 infrastructure의 HW와 SW stack을 통째로 내재화한 다음에 자기 전용의 데이터센터 위에서 직접 AI application들을 개발한다. \n\n파트너/고객으로 일본의 주요 대기업은 다 붙어있다. \n\n자동차는 Toyota, 로보트는 FANUC, 소비재는 KAO, 통신은 NTT. @_@ 벌이고 있는 사업의 방향성이나 그림이야 뭐 그럴 수 있는데, 이걸 진짜로 여기까지 해놓은거 보고 움찔했다. \n\n아니 이런 녀석이 도대체 왜 안알려져있지? \n\n스타트업이 구글과 다이다이 붙는 수준(반도체-AI데이터센터-AI프레임웍-AI어플리케이션까지...)으로 일을 벌이고 있다. \n\n일본 무시할 게 아니다. \n\n내수만으로 저런 스케일이 나오니 외부에 안알려질 뿐이다. \n\n한국의 AI스타트업들 다 합쳐도 택도 없는 scale.\n\n우와... 우와... 세상은 넓고 겸손해야 된다... 잠이 다 안오네..","categories":["DeepThinking","AI"],"tags":["DeepThinking","ToyProjects","TensorFlow","AI","DL","ML","Preferred Networks","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2021-04-09T15:00:00.000Z","url":"/blog/deepthinking/ai/2021-04-10-preferrednetworks/"},{"id":"deepthinking/ai/2021-08-22-teslaaiday","title":"Tesla AI Day","description":"","content":"[Tesla AI Day 후기]\n\n개인적으로 플랫폼을 만들고 있는 엔지니어 입장에서 꽤 구미가 당기는 Tech Talk이었다.\n\nIn-house 에서 반도체 설계부터 공정까지 하고 있고 이에 맞는 컴파일러, 개발환경, 소프트웨어까지 모두 하다 보니 그 혁신을 밖에서 볼 수 밖에 없는 나같은 엔지니어들은 호기심이 생길수밖에...\n질의 응답시간에 나오는 대부분의 질문은 이런 영역이었다. \n\n단순히 프레임워크를 이용하고 그 위에서 새로운 모델링을 적용하여 더 효율적인 AI 모델을 찾는데에서 지겨워진(?) 사람들은 이렇게 Next Generation 플랫폼을 원할 수 밖에 없다. ML 이 Boom 된지 오래되진 않았지만 단순히 High level 소프트웨어 스택에서만 발전하기엔 무리가 있는 것이 사실이라는 건 GPT-3에서 이미 기정 사실화 됐다. \n\n![Desktop View](/assets/img/aiday/1.png)\n\nIntel, Tesla, Apple, Google, MS 과 같은 회사들이 세상을 바꾸고 있는 방식이 단순히 플랫폼을 가지고 있어서 가 아니라, \n차세대 플랫폼을 만들 수 있는 Researcher & Engineer 들을 품을 수 있는 역량이 있어서 라는 것을 단연 보여주는 Tech Talk이었다.\n![Desktop View](/assets/img/aiday/2.png)\n\n또 이런 차세대 플랫폼은 전세계 모든 소프트웨어 및 비지니스의 기반이 되고 우리는 거기서 벗어날 수 없겠지 ㅋㅋ...\n\n이런 엔지니어들은 이제는 막을 수 없는 오픈소스의 흐름에서 움직이기 때문에, 새로운 비전과 새 아키텍쳐를 보여주지 않으면 경쟁사에게 금방 빼앗길 수 밖에 없다. (사실 이러면서 오픈소스 소스의 흐름이 되려 커지는 것 같다)\n이런 엄청난 비전을 볼 수 있고 보여줄 수 있는 사람들이 한국에 많이 없는 건 항상 안타까운 현실인 것 같다.\n\n인간에게 큰 발전을 가져다준 Logical thinking에서 Computational thinking으로의 문화적 진화는, 이제는 AI based thinking으로 발전하고 있고 이를 Drive 하는 것은 Computational thinking이다. \n\nComputational & AI based thinking을 기반으로 세상을 좋게 바꾸는데 기여하는 사람들이 한국에도 많았으면 좋겠다는 생각이 든다.","categories":["DeepThinking","AI"],"tags":["DeepThinking","AI","GithubPage","Retrospect","CPU","GPU","SoC","AP","Semiconductor","Tesla"],"pubDate":"2021-08-21T15:00:00.000Z","url":"/blog/deepthinking/ai/2021-08-22-teslaaiday/"},{"id":"2025-07-24-ai-workflow-productivity","title":"AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (1편)","description":"월 100달러짜리 AI 스택으로 혼돈 속에서 시스템을 찾아가는 여정","content":"![AI Workflow Productivity](/assets/img/ai-workflow.jpg)\n\n# AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (1편)\n\n안녕하세요! 오늘부터 좀 특별한 시리즈를 시작해보려고 합니다. \n\n요즘 컴퓨터 앞에 있을 때 진짜 **ADHD도 이런 ADHD가 없다**고 느낄 정도로 여러 AI 도구들을 동시에 돌리고 있어서, 이 혼돈 속에서 나름의 시스템을 찾아가는 과정을 공유해보려고 합니다.\n\n## 🤖 현재 돌리고 있는 AI 스택 (월 $100+)\n\n매달 **100달러 넘게** 쓰면서 이 녀석들을 매일 돌리고 있습니다:\n\n- **Cursor Pro** - 메인 코딩 환경\n- **ChatGPT Pro** - 감정노동과 고객 대응용 (ㅋㅋ)\n- **Claude Code** - 공격적인 코딩 작업용\n- **Gemini CLI** - 문서화 전문\n- **Gemini Pro 2.5** - 리서치 및 새 프로젝트 기획\n\n확실히 많이 쓰기도 하지만, **AI 강의를 매주 하면서** 알게 된 노하우들을 녹여서 쓰다 보니 조금씩 Productivity가 향상됨을 느끼고 있습니다. \n\n어느 정도 각을 잡아가는 듯해서 이 기록을 남기기로 했습니다.\n\n## 🎯 전체 워크플로우 개요\n\n```mermaid\ngraph TB\n    subgraph \"AI 스택 (월 $100+)\"\n        A1[\"🔵 Cursor Pro<br/>메인 코딩\"]\n        A2[\"🟢 ChatGPT Pro<br/>감정노동 & 고객대응\"]\n        A3[\"🟣 Claude Code<br/>공격적 코딩\"]\n        A4[\"🔴 Gemini CLI<br/>문서화 전문\"]\n        A5[\"🟡 Gemini Pro 2.5<br/>리서치 & 기획\"]\n    end\n    \n    subgraph \"공통 워크플로우\"\n        B1[\"1️⃣ Context Window<br/>새로 열기\"]\n        B2[\"2️⃣ 정리된 Context<br/>제공하기\"]\n        B3[\"3️⃣ 답변 포맷<br/>생각하게 하기\"]\n        B4[\"4️⃣ 명확한<br/>Request\"]\n        B5[\"5️⃣ Feedback<br/>과정\"]\n        B6[\"6️⃣ Context 문서<br/>업데이트\"]\n        \n        B1 --> B2 --> B3 --> B4 --> B5 --> B6\n    end\n    \n    subgraph \"워크스페이스 구성 (5-8개)\"\n        C1[\"📡 Remote SSH\"]\n        C2[\"💻 Native\"]\n        \n        C1 --> C11[\"Product Code\"]\n        C1 --> C12[\"Build Server 1-2개\"]\n        C1 --> C13[\"Test Environment 1-2개\"]\n        \n        C2 --> C21[\"Blog/Documentation\"]\n        C2 --> C22[\"강의자료 1-2개\"]\n    end\n    \n    subgraph \"도구별 특화\"\n        D1[\"Claude Code<br/>💸 3시간 제한\"] --> D11[\"Aggressive<br/>코딩 작업\"]\n        D2[\"Cursor + 대화창\"] --> D22[\"Agentic Mode<br/>Ping-ponging\"]\n        D3[\"ChatGPT Pro\"] --> D33[\"이메일<br/>감정노동\"]\n        D4[\"Gemini 2.5 Pro\"] --> D44[\"Documentation<br/>기똥참\"]\n    end\n    \n    A1 -.-> B1\n    A3 -.-> D1\n    A4 -.-> D4\n    \n    style A1 fill:#e1f5fe\n    style A2 fill:#e8f5e8\n    style A3 fill:#f3e5f5\n    style A4 fill:#ffebee\n    style A5 fill:#fffde7\n```\n\n위 다이어그램이 제가 현재 운영하고 있는 AI 멀티플렉싱 워크플로우의 전체 구조입니다!\n\n## 💡 왜 이렇게 많은 AI 도구를 써야 하나?\n\n솔직히 처음에는 저도 \"이게 맞나?\" 싶었습니다. 하지만 실제로 써보니 **각 도구마다 확실히 특화된 영역**이 있더라고요.\n\n- **코딩할 때**: Cursor가 압도적\n- **문서 작성할 때**: Gemini가 진짜 기똥참\n- **고객 이메일 쓸 때**: ChatGPT가 감정노동을 대신해줌 ㅋㅋ\n- **무거운 코딩할 때**: Claude Code가 필요악\n\n결국 **하나로 모든 걸 다 하려고 하면** 오히려 비효율적이더라는 게 결론입니다.\n\n## 🏗️ 기본 운영 철학\n\n### 1. 모든 워크스페이스는 Git Repo\n**5~8개의 Cursor 인스턴스**가 항상 떠있는데, 각각이 독립된 git repo입니다:\n- Remote SSH 기반: Product Code, Build Server, Test Environment\n- Native 기반: Blog, Documentation, 강의자료\n\n### 2. Context 관리가 핵심\nAI들이 들쭉날쭉 하는 이유 중 하나가 **Context 관리를 못해서**입니다. \n- project config, workflow 등을 담은 markdown들을 계속 업데이트\n- 대화가 길어지면 과감히 새 창으로 시작\n\n### 3. 각 도구의 한계를 인정하고 활용\n예를 들어, Claude Code는 **3시간마다 Usage 제한**이 있어서 짜증나지만, 그래서 더 전략적으로 사용하게 됩니다.\n\n## 📋 앞으로 다룰 내용들\n\n이 시리즈에서 다룰 예정인 내용들입니다:\n\n### 2편: \"공통 워크플로우 방법론\" (다음주 예정)\n- 6단계 워크플로우 상세 설명\n- 실제 프롬프트 예시와 Before/After\n- Context Window 관리 실전 팁\n\n### 3편: \"워크스페이스 운영 실전기\"\n- Remote SSH vs Native 개발환경 구성\n- Git Repo 기반 워크스페이스 관리법\n- 5~8개 인스턴스를 효율적으로 운영하는 법\n\n### 4편: \"도구별 프롬프트 전략\"\n- Cursor Pro 활용 고급 기법\n- Claude Code 3시간 제한 극복법\n- Gemini CLI 문서화 자동화\n\n### 5편: \"PRD 방법론으로 새 프로젝트 시작하기\"\n- Gemini 2.5 Pro로 Research 돌리는 법\n- 들쭉날쭉한 AI를 더 잘하게 만드는 PRD 작성법\n- 실제 프로젝트 적용 사례\n\n## 🔚 마무리하며...\n\n사실 **Claude Code usage가 다시 찰 때까지만** 이 글을 쓰려고 한 거였는데... 쓰다 보니 시리즈로 만들어야겠더라고요 ㅋㅋㅋ\n\n각 편마다 실제 사용하는 프롬프트나 설정 파일들도 공유할 예정이니, 비슷한 고민을 하시는 분들에게 도움이 되길 바랍니다!\n\n다음 편은 **공통 워크플로우 방법론**에 대해 더 자세히 다뤄보겠습니다. 여러분도 AI 도구들 어떻게 활용하고 계신가요? 댓글로 공유해주세요!\n\n---\n\n*이 글은 실제 AI 도구들을 매일 활용하며 얻은 경험을 바탕으로 작성되었으며, 비슷한 고민을 하는 개발자들에게 도움이 되길 바랍니다.*","categories":["Tech","AI"],"tags":["AI","Productivity","Cursor","ChatGPT","Claude","Gemini","Workflow"],"pubDate":"2025-07-24T00:00:00.000Z","url":"/blog/2025-07-24-ai-workflow-productivity/"},{"id":"2025-07-27-ai-prompt-strategies","title":"AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (4편) - 도구별 프롬프트 전략","description":"Cursor Pro, Claude Code, Gemini CLI 각각에 최적화된 실전 프롬프트 노하우 대공개","content":"![AI Prompt Strategies](/assets/img/ai-workflow.jpg)\n\n# AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (4편)\n## 도구별 프롬프트 전략\n\n안녕하세요! 시리즈도 벌써 4편이네요. [1편](./2025-07-24-ai-workflow-productivity.md)에서 AI 스택을, [2편](./2025-07-25-ai-workflow-common-methods.md)에서 공통 워크플로우를, [3편](./2025-07-26-ai-workspace-management.md)에서 워크스페이스 운영을 다뤘다면, 이번에는 **각 AI 도구별로 최적화된 프롬프트 전략**을 공개합니다!\n\n이건 진짜 **몇 개월간 시행착오**를 거쳐서 정리한 것들이라, 바로 써먹으실 수 있을 거예요. 각 도구의 특성에 맞는 프롬프트 패턴들을 실제 예시와 함께 공유해드릴게요!\n\n## 🔵 Cursor Pro: 메인 코딩 환경 마스터하기\n\n### Cursor의 특성 이해하기\n\n**Cursor는 코드 컨텍스트를 가장 잘 이해하는** AI 도구입니다. 파일 전체를 보면서 작업하기 때문에, 다른 도구들과는 다른 접근이 필요해요.\n\n### 🎯 Cursor 특화 프롬프트 패턴\n\n#### 1. 프로젝트 전체 맥락 활용 패턴\n\n**❌ 비효율적인 방식:**\n```\n이 함수 리팩토링해줘\n\nfunction getUserData(id: string) {\n  // 복잡한 로직...\n}\n```\n\n**✅ Cursor 최적화 방식:**\n```\n현재 프로젝트의 TypeScript 타입 정의와 에러 핸들링 패턴을 참고해서,\n이 getUserData 함수를 프로젝트 표준에 맞게 리팩토링해줘.\n\n특히 다음을 고려해서:\n1. src/types/ 폴더의 User 타입 정의 활용\n2. src/utils/errorHandler.ts의 에러 처리 패턴 적용  \n3. 다른 service 파일들과 일관된 구조로 변경\n\n@getUserData 함수 위치 표시\n```\n\n#### 2. 파일 간 연관성 활용 패턴\n\n**실제 제가 쓰는 프롬프트:**\n```\n@src/services/userService.ts\n@src/types/User.ts\n@src/utils/apiClient.ts\n\n위 파일들의 패턴을 참고해서 새로운 productService.ts를 만들어줘.\n\n요구사항:\n- User 관련 코드와 동일한 구조\n- 같은 에러 핸들링 방식\n- apiClient 활용 패턴 동일하게 적용\n- TypeScript 타입 안정성 최우선\n\nProduct 타입은 다음과 같아:\ninterface Product {\n  id: string;\n  name: string;\n  price: number;\n  // ... 기타 필드\n}\n```\n\n#### 3. 코드 리뷰 및 개선 패턴\n\n**제가 자주 쓰는 코드 리뷰 프롬프트:**\n```\n@선택된_파일_또는_코드\n\n프로젝트의 다른 파일들과 비교해서 이 코드를 리뷰해줘.\n\n리뷰 관점:\n1. 🔍 **일관성**: 프로젝트 내 다른 코드와 패턴이 일치하는가?\n2. 🏗️ **아키텍처**: 전체 구조에 잘 맞는가?\n3. 🔒 **타입 안정성**: TypeScript 활용이 충분한가?\n4. 🚀 **성능**: 최적화할 부분이 있는가?\n5. 🧪 **테스트**: 테스트하기 쉬운 구조인가?\n\n각 항목별로 구체적인 개선사항과 이유를 제시해줘.\n```\n\n### 🔧 Cursor Chat vs Inline Edit 활용법\n\n#### Chat에서 쓰는 프롬프트 (복잡한 설계)\n\n```\n프로젝트 전체 구조를 고려해서 새로운 기능을 설계해보자.\n\n**기능**: 사용자 권한 관리 시스템\n**요구사항**: \n- 역할 기반 접근 제어 (RBAC)\n- 기존 User 시스템과 통합\n- API 엔드포인트별 권한 체크\n\n현재 프로젝트 구조에서 이 기능을 어떻게 통합할지 설계안을 제시해줘:\n1. 파일 구조 제안\n2. 기존 코드 수정점\n3. 새로 만들 컴포넌트들\n4. 데이터베이스 스키마 변경사항\n```\n\n#### Inline Edit에서 쓰는 프롬프트 (구체적 수정)\n\n```\n// Cmd+K로 Inline Edit 모드에서\n이 함수를 async/await 패턴으로 변경하고, \n프로젝트의 에러 핸들링 표준에 맞게 try-catch 추가해줘\n```\n\n## 🟣 Claude Code: 3시간 제한 극복 전략\n\n### Claude Code의 특성과 한계\n\n**3시간마다 Usage 리셋**되는 Claude Code... 비싸게 주고도 이런 제약이 있어서 진짜 전략적으로 써야 해요. \n\n### 💡 Usage 효율성 극대화 패턴\n\n#### 1. 한 번에 최대한 많이 뽑아내기\n\n**❌ 비효율적 (Usage 낭비):**\n```\n대화 1: \"이 함수 리팩토링해줘\"\n대화 2: \"아 그리고 테스트 코드도 만들어줘\" \n대화 3: \"문서화도 해줘\"\n대화 4: \"타입 정의도 개선해줘\"\n```\n\n**✅ 효율적 (한 번에 해결):**\n```\n다음 작업을 한 번에 모두 처리해줘:\n\n**대상**: UserService 클래스 전체 개선\n**Context**: TypeScript + NestJS 백엔드 프로젝트\n\n**요청사항 (한 번에 모두):**\n1. 🔧 리팩토링: 함수별 단일 책임 원칙 적용\n2. 🧪 테스트: Jest 기반 유닛 테스트 작성  \n3. 📝 문서화: JSDoc + README 업데이트\n4. 🔒 타입: 더 엄격한 TypeScript 타입 정의\n5. 🚀 성능: 불필요한 DB 쿼리 최적화\n\n모든 결과물을 단계별로 정리해서 한 번에 제공해줘.\n```\n\n#### 2. 복잡한 로직은 Claude Code에 몰아주기\n\n**Claude Code가 특히 잘하는 것들:**\n- 🧠 **복잡한 비즈니스 로직** 설계\n- 🏗️ **아키텍처 설계** 및 패턴 적용  \n- 🔍 **코드 리뷰** 및 품질 분석\n- 📚 **문서화** 및 가이드 작성\n\n**실제 제가 Claude Code에 맡기는 작업:**\n```\n마이크로서비스 간 통신을 위한 Event-Driven Architecture를 설계해줘.\n\n**현재 상황:**\n- 5개의 독립 서비스 (User, Product, Order, Payment, Notification)\n- 각각 별도 DB 및 API\n- 서비스 간 RESTful API로 동기 통신 중\n\n**목표:**\n- 비동기 이벤트 기반 통신으로 전환\n- 장애 복구 및 재시도 로직 포함\n- 모니터링 및 로깅 체계 구축\n\n**결과물:**\n1. 📋 전체 아키텍처 다이어그램 (Mermaid)\n2. 🔧 구현 예시 코드 (TypeScript + Node.js)\n3. 📝 Migration 가이드\n4. 🧪 테스트 전략\n5. 📊 모니터링 방안\n\n한 번에 완전한 설계안을 제공해줘.\n```\n\n#### 3. 시간대별 전략적 사용\n\n**제가 쓰는 Claude Code 시간 관리:**\n```\n🌅 오전 (9-12시): 새로운 기능 설계\n- 복잡한 아키텍처 설계\n- 새 프로젝트 구조 설계\n\n🌞 오후 (13-15시): 기존 코드 개선  \n- 레거시 코드 리팩토링\n- 성능 최적화\n\n🌆 저녁 (18-20시): 문서화 및 정리\n- API 문서 자동 생성\n- 프로젝트 가이드 작성\n```\n\n### 🚀 Claude Code 전용 프롬프트 템플릿\n\n```markdown\n# Claude Code 전용 대화 시작 템플릿\n\n## 프로젝트 Context\n- **기술 스택**: [상세 기술 스택]\n- **프로젝트 규모**: [팀 규모, 코드 라인 수 등]\n- **현재 상황**: [해결하려는 문제]\n- **목표**: [달성하려는 결과]\n\n## 이번 세션 목표\n- [ ] 주요 작업 1\n- [ ] 주요 작업 2  \n- [ ] 주요 작업 3\n\n## 제약사항\n- 기존 API 호환성 유지\n- 특정 라이브러리 사용 필수\n- 성능 요구사항: [구체적 수치]\n\n## 원하는 결과물 형태\n1. 📋 설계 문서 (아키텍처 다이어그램 포함)\n2. 🔧 구현 코드 (주석 포함)\n3. 🧪 테스트 코드\n4. 📝 사용 가이드\n\n---\n[구체적 요청 시작]\n```\n\n## 🔴 Gemini CLI: 문서화 자동화 마스터\n\n### Gemini의 강력한 문서화 능력\n\n**진짜 Gemini 2.5 Pro는 문서화가 기똥차요.** 특히 CLI에서 쓰면 더 편합니다.\n\n### 📝 Gemini CLI 설치 및 설정\n\n```bash\n# Gemini CLI 설치\nnpm install -g @google/generative-ai-cli\n\n# API 키 설정\nexport GEMINI_API_KEY=\"your-api-key\"\n\n# 또는 ~/.bashrc에 추가\necho 'export GEMINI_API_KEY=\"your-api-key\"' >> ~/.bashrc\n```\n\n### 🎯 문서화 특화 프롬프트 패턴\n\n#### 1. API 문서 자동 생성\n\n**실제 제가 쓰는 명령어:**\n```bash\n# 코드 파일을 분석해서 API 문서 생성\ngemini \"\n다음 TypeScript 코드를 분석해서 완전한 API 문서를 생성해줘:\n\n$(cat src/controllers/userController.ts)\n\n**생성할 문서 형태:**\n1. 📋 엔드포인트 개요 테이블\n2. 🔧 각 엔드포인트 상세 (Request/Response)\n3. 📝 사용 예시 (curl + JavaScript)  \n4. ⚠️ 에러 응답 정의\n5. 🔒 인증 요구사항\n\n**포맷**: OpenAPI 3.0 spec + 한국어 설명\n\"\n```\n\n#### 2. README 자동 생성 및 업데이트\n\n```bash\n# 프로젝트 전체를 분석해서 README 생성\ngemini \"\n다음 프로젝트 구조를 분석해서 전문적인 README.md를 생성해줘:\n\n**프로젝트 구조:**\n$(tree -I 'node_modules|.git' -L 3)\n\n**package.json 정보:**  \n$(cat package.json)\n\n**주요 설정 파일들:**\n$(cat tsconfig.json)\n$(cat .env.example)\n\n**생성할 README 섹션:**\n1. 🎯 프로젝트 개요 및 특징\n2. ⚡ Quick Start 가이드  \n3. 🏗️ 아키텍처 설명\n4. 🔧 설치 및 설정 방법\n5. 📝 API 사용법\n6. 🧪 테스트 실행 방법\n7. 🚀 배포 가이드\n8. 🤝 기여 방법\n\n**톤**: 전문적이면서도 친근한 개발자 톤\n\"\n```\n\n#### 3. 코드 주석 및 문서화\n\n```bash\n# 복잡한 함수에 JSDoc 주석 자동 생성\ngemini \"\n다음 TypeScript 함수에 완전한 JSDoc 주석을 달아줘:\n\n$(cat src/utils/complexAlgorithm.ts)\n\n**주석 요구사항:**\n1. 📝 함수 목적 및 동작 원리 설명\n2. 📋 모든 매개변수 타입 및 설명\n3. 🔄 반환값 타입 및 설명  \n4. ⚠️ 발생 가능한 예외사항\n5. 📖 사용 예시 코드\n6. 🔗 관련 함수/문서 링크\n\n**스타일**: JSDoc 표준 + 이모티콘으로 가독성 향상\n\"\n```\n\n### 🔄 Gemini CLI 자동화 스크립트\n\n**docs-update.sh:**\n```bash\n#!/bin/bash\n\necho \"📝 자동 문서화 시작...\"\n\n# 1. API 문서 업데이트\necho \"🔧 API 문서 생성 중...\"\ngemini \"\n$(cat src/controllers/*.ts | head -200)\n\n위 컨트롤러들을 분석해서 docs/api.md를 업데이트해줘.\n변경된 엔드포인트만 반영하고, 기존 문서 구조는 유지해줘.\n\" > docs/api-temp.md\n\n# 2. README 업데이트  \necho \"📋 README 업데이트 중...\"\ngemini \"\n현재 README.md:\n$(cat README.md)\n\n최신 package.json:\n$(cat package.json)\n\nREADME를 최신 정보로 업데이트해줘. \n기존 톤은 유지하되, 변경된 dependencies나 scripts 정보만 반영해줘.\n\" > README-temp.md\n\n# 3. 변경사항 검토 후 적용\necho \"📊 문서 변경사항 검토...\"\necho \"API 문서 변경사항:\"\ndiff docs/api.md docs/api-temp.md || true\n\necho \"README 변경사항:\"  \ndiff README.md README-temp.md || true\n\nread -p \"변경사항을 적용하시겠습니까? (y/n): \" confirm\nif [ \"$confirm\" = \"y\" ]; then\n    mv docs/api-temp.md docs/api.md\n    mv README-temp.md README.md\n    echo \"✅ 문서 업데이트 완료\"\nelse\n    rm docs/api-temp.md README-temp.md\n    echo \"❌ 변경사항 취소됨\"\nfi\n```\n\n## 🟢 ChatGPT Pro: 감정노동 전문가\n\n### ChatGPT의 독특한 강점\n\n**고객 대응이나 이메일 작성**에서 ChatGPT만큼 좋은 게 없어요. 감정적인 뉘앙스를 잘 이해하고 적절한 톤으로 응답해줍니다.\n\n### 💼 비즈니스 커뮤니케이션 프롬프트\n\n#### 1. 고객 이메일 응답\n\n**실제 제가 쓰는 패턴:**\n```\n다음 상황에서 고객에게 보낼 이메일을 작성해줘:\n\n**상황**: \n- 고객이 API 응답 속도 이슈로 불만 제기\n- 실제로 일시적인 서버 과부하가 있었음\n- 현재는 해결된 상태\n- 재발 방지책도 마련됨\n\n**고객 톤**: 약간 화난 상태, 비즈니스 파트너\n**원하는 응답 톤**: 사과하되 전문적, 해결책 중심\n\n**포함할 내용**:\n1. 🙏 상황 인정 및 사과\n2. 🔍 원인 설명 (기술적이지만 이해하기 쉽게)\n3. ✅ 해결 완료 확인\n4. 🛡️ 재발 방지책 설명\n5. 📞 추가 지원 안내\n\n**이메일 길이**: 간결하게 3-4 문단\n```\n\n#### 2. 내부 커뮤니케이션\n\n```\n팀 내부 회의록을 정리해줘:\n\n**회의 내용** (음성녹음 → 텍스트):\n[복잡하고 정리되지 않은 회의 내용들...]\n\n**정리 요구사항**:\n1. 📋 주요 논의사항 (우선순위별)\n2. ✅ 결정된 사항들\n3. 📋 액션 아이템 (담당자, 마감일 포함)  \n4. ❓ 추후 논의 필요 사항\n5. 📅 다음 미팅 일정\n\n**톤**: 팀 내부용, 간결하고 명확하게\n```\n\n## 🔧 도구 간 연계 활용 패턴\n\n### 협업 워크플로우\n\n실제로는 **도구들을 연계해서** 쓸 때 가장 효과적입니다.\n\n#### 패턴 1: 설계 → 구현 → 문서화\n\n```\n1. 🟣 Claude Code: 전체 아키텍처 설계\n   \"마이크로서비스 아키텍처 설계해줘\"\n\n2. 🔵 Cursor Pro: 실제 코드 구현  \n   \"Claude가 설계한 구조를 바탕으로 UserService 구현\"\n\n3. 🔴 Gemini CLI: 문서화\n   \"구현된 코드를 분석해서 API 문서 생성\"\n\n4. 🟢 ChatGPT: 사용자 가이드\n   \"기술 문서를 일반 사용자용 가이드로 변환\"\n```\n\n#### 패턴 2: 문제 해결 → 소통 → 정리\n\n```\n1. 🔵 Cursor Pro: 버그 분석 및 수정\n   \"이 에러 로그 분석해서 원인 찾고 수정해줘\"\n\n2. 🟢 ChatGPT: 고객 응답 작성\n   \"버그 수정 완료를 고객에게 알리는 이메일 작성\"\n\n3. 🔴 Gemini CLI: 포스트모템 문서\n   \"이번 이슈의 포스트모템 문서 작성\"\n```\n\n## 📊 효과 측정 및 개선\n\n### 프롬프트 성능 추적\n\n**제가 쓰는 간단한 평가 기준:**\n\n```markdown\n## 프롬프트 평가 기준\n\n각 응답에 대해 1-5점으로 평가:\n\n1. **정확성** (1-5): 요청한 내용을 정확히 수행했는가?\n2. **완성도** (1-5): 추가 요청 없이 바로 사용 가능한가?  \n3. **효율성** (1-5): 최소한의 프롬프트로 최대 결과를 얻었는가?\n4. **일관성** (1-5): 비슷한 요청에서 일관된 품질인가?\n\n**목표**: 모든 항목 평균 4점 이상 유지\n```\n\n### 지속적인 프롬프트 개선\n\n**월간 리뷰 프로세스:**\n1. **사용 빈도** 높은 프롬프트 패턴 정리\n2. **실패 사례** 분석 및 개선 방안 도출  \n3. **새로운 패턴** 실험 및 검증\n4. **팀 공유** 및 피드백 수집\n\n## 🔚 마무리하며...\n\n이렇게 **각 AI 도구별로 최적화된 프롬프트 전략**을 정리해봤습니다. \n\n핵심은 **각 도구의 특성을 이해하고 그에 맞는 프롬프트를 쓰는 것**입니다. Cursor는 코드 컨텍스트를, Claude는 복잡한 사고를, Gemini는 문서화를, ChatGPT는 커뮤니케이션을 각각 잘하니까요.\n\n마지막 5편에서는 **PRD 방법론으로 새 프로젝트 시작하기**를 다뤄보겠습니다. Gemini로 Research를 돌려서 들쭉날쭉한 AI를 더 잘하게 만드는 PRD 작성법과 실제 적용 사례를 공유할 예정입니다!\n\n여러분은 어떤 프롬프트 패턴을 사용하고 계신가요? 특히 잘 되는 패턴이 있으면 댓글로 공유해주세요!\n\n---\n\n*이 시리즈의 다른 글들:*\n- *[1편: AI 스택 소개와 전체 개요](./2025-07-24-ai-workflow-productivity.md)*\n- *[2편: 공통 워크플로우 방법론](./2025-07-25-ai-workflow-common-methods.md)*\n- *[3편: 워크스페이스 운영 실전기](./2025-07-26-ai-workspace-management.md)*\n- *5편: PRD 방법론으로 새 프로젝트 시작하기 (곧 공개)*","categories":["Tech","AI"],"tags":["AI","Productivity","Prompt","Cursor","Claude","Gemini","ChatGPT"],"pubDate":"2025-07-27T00:00:00.000Z","url":"/blog/2025-07-27-ai-prompt-strategies/"},{"id":"2025-07-25-ai-workflow-common-methods","title":"AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (2편) - 공통 워크플로우 방법론","description":"모든 LLM에게 통하는 6단계 워크플로우 실전 가이드","content":"![AI Workflow Methods](/assets/img/ai-workflow.jpg)\n\n# AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (2편)\n## 공통 워크플로우 방법론\n\n안녕하세요! [1편](./2025-07-24-ai-workflow-productivity.md)에서 AI 스택 소개와 전체 개요를 다뤘다면, 이번 2편에서는 **모든 LLM에게 공통으로 적용하는 워크플로우 방법론**을 구체적으로 파헤쳐보겠습니다.\n\n이 방법론은 제가 **AI 강의를 매주 하면서** + **실제 업무에서 매일 적용하면서** 다듬어진 것들이라, 어떤 AI 도구를 쓰든 상관없이 적용할 수 있습니다.\n\n## 🎯 6단계 워크플로우 전체 구조\n\n```mermaid\ngraph LR\n    A[\"1️⃣ Context Window<br/>새로 열기\"] --> B[\"2️⃣ 정리된 Context<br/>제공하기\"]\n    B --> C[\"3️⃣ 답변 포맷<br/>생각하게 하기\"] \n    C --> D[\"4️⃣ 명확한<br/>Request\"]\n    D --> E[\"5️⃣ Feedback<br/>과정\"]\n    E --> F[\"6️⃣ Context 문서<br/>업데이트\"]\n    F -.-> A\n    \n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#f3e5f5\n    style D fill:#ffebee\n    style E fill:#fffde7\n    style F fill:#fce4ec\n```\n\n경험상 **어떤 AI든 상관없이** 이 방식으로 일을 시키면 훨씬 좋은 결과가 나옵니다. 이제 각 단계를 실제 예시와 함께 자세히 살펴보겠습니다!\n\n## 1️⃣ Context Window를 자주 새로 연다\n\n### 왜 이게 중요한가?\n\nAI들은 대화가 길어질수록 **초기 맥락을 잃어버리는** 경향이 있습니다. 특히 복잡한 코딩 작업이나 긴 문서 작업을 할 때 이 현상이 심해져요.\n\n### 실전 적용법\n\n**❌ 나쁜 예시:**\n```\n(50번의 대화가 이어진 후)\n사용자: 아 그리고 처음에 말한 TypeScript 설정도 다시 봐줘\nAI: 죄송합니다. 어떤 TypeScript 설정을 말씀하시는 건지...?\n```\n\n**✅ 좋은 예시:**\n```\n(새 창을 열고)\n사용자: 다음 프로젝트 맥락을 참고해서 TypeScript 설정을 검토해줘:\n[프로젝트 맥락 + 기존 대화 요약 + 구체적 요청]\n```\n\n### 언제 새 창을 여는가?\n- **대화 횟수가 20번 넘어갈 때**\n- **주제가 바뀔 때** (코딩 → 문서화)\n- **AI가 맥락을 놓치기 시작할 때**\n\n## 2️⃣ 미리 정리된 Context를 던져서 생각하게 한다\n\n이게 **가장 중요한 단계**입니다. 바로 답을 달라고 하지 말고, 먼저 상황을 파악하게 해야 해요.\n\n### Before & After 비교\n\n**❌ 개발자들이 자주 하는 실수:**\n```\n코드 리뷰해줘.\n\nclass UserService {\n  constructor(private db: Database) {}\n  \n  async createUser(userData: any) {\n    return this.db.users.create(userData);\n  }\n}\n```\n\n**✅ 개선된 방식:**\n```\n다음 맥락을 먼저 파악해줘:\n\n**프로젝트**: TypeScript + NestJS 기반 백엔드 API\n**현재 상황**: User 도메인 서비스 레이어 구현 중\n**요구사항**: 타입 안정성, 에러 핸들링, 유효성 검사 필요\n**현재 이슈**: any 타입 사용으로 타입 안정성 부족\n\n위 맥락에서 다음 코드의 문제점과 개선 방향을 먼저 분석해줘:\n\n[코드]\n\n그 다음에 구체적인 개선된 코드를 제안해줘.\n```\n\n### 제가 자주 쓰는 Context 템플릿\n\n```markdown\n## 프로젝트 맥락\n- **기술 스택**: [구체적인 기술 스택]\n- **현재 작업**: [지금 하고 있는 일]\n- **목표**: [달성하려는 것]\n- **제약사항**: [고려해야 할 제약들]\n\n## 현재 상황\n[구체적인 현재 상황 설명]\n\n## 요청사항\n[구체적인 요청 + 원하는 결과물 형태]\n```\n\n## 3️⃣ 좋은 답변 포맷을 미리 제시하거나 생각하게 한다\n\n### 핵심 아이디어\n\n**\"이런 형태로 답변해줘\"** 보다는 **\"어떤 형태로 답변하는 게 좋을지 먼저 생각해봐\"**\n\n### 실전 예시\n\n**❌ 지시적인 방식:**\n```\n다음 형태로 답변해줘:\n1. 문제점 3가지\n2. 해결방안 3가지  \n3. 코드 예시\n```\n\n**✅ 사고하게 만드는 방식:**\n```\n이 상황에서 가장 도움이 될 만한 답변 구조를 먼저 제안해봐.\n어떤 순서로, 어떤 형태로 정보를 제공하는 게 좋을까?\n\n그 다음에 제안한 구조에 따라 실제 답변을 해줘.\n```\n\n### 왜 이렇게 하는가?\n\nAI가 **스스로 생각하는 과정**을 거치면 더 체계적이고 맥락에 맞는 답변을 합니다. 단순히 틀에 맞춰 답하는 것보다 훨씬 질이 높아져요.\n\n## 4️⃣ 명확한 Request를 한다\n\n### 애매한 요청의 문제점\n\n**❌ 애매한 요청들:**\n- \"코드 좀 봐줘\"\n- \"이거 어떻게 생각해?\"\n- \"더 좋게 만들어줘\"\n\n**✅ 명확한 요청들:**\n- \"TypeScript 타입 안정성 관점에서 이 코드를 검토하고, 구체적인 개선 코드를 제안해줘\"\n- \"성능 최적화 관점에서 이 React 컴포넌트의 리렌더링 이슈를 분석하고 해결방안을 제시해줘\"\n- \"유지보수성을 높이기 위해 이 함수를 3개 이하의 작은 함수로 분리해줘\"\n\n### 좋은 Request의 구성요소\n\n1. **관점/목적** (성능? 가독성? 보안?)\n2. **구체적 작업** (분석? 구현? 리팩토링?)\n3. **결과물 형태** (코드? 문서? 가이드?)\n4. **제약조건** (기존 코드 유지, 특정 라이브러리 사용 등)\n\n## 5️⃣ Feedback 과정을 통해 계속 생각하게 만든다\n\n### 핵심 철학\n\n**한 번에 완벽한 답을 기대하지 말고**, 피드백을 통해 점진적으로 개선하는 것이 훨씬 효과적입니다.\n\n### 실전 Feedback 패턴\n\n```\n1차 답변 후:\n\"좋은 방향이야. 그런데 [구체적 피드백]. \n이 부분을 고려해서 다시 개선해줄 수 있을까?\"\n\n2차 답변 후:\n\"거의 다 됐는데, [세부 조정 요청].\n실제 프로덕션에서 사용할 때 [구체적 상황]도 고려해서 마지막으로 다듬어줘.\"\n```\n\n### 효과적인 Feedback 방법\n\n1. **좋은 점 먼저 인정**: \"접근법은 좋은데...\"\n2. **구체적인 개선점 제시**: \"○○ 부분에서 ××를 고려하면...\"\n3. **맥락 추가**: \"실제로는 ○○한 상황도 있어서...\"\n\n## 6️⃣ Context 문서들을 지속적으로 업데이트한다\n\n### 왜 이게 필요한가?\n\nAI들이 들쭉날쭉하는 이유 중 하나가 **프로젝트 맥락을 매번 새로 설명해야 하기 때문**입니다. 이를 해결하기 위해 체계적인 Context 문서를 관리해야 해요.\n\n### 제가 관리하는 Context 문서들\n\n```\n📁 .ai-context/\n├── project-config.md          # 프로젝트 기본 정보\n├── tech-stack.md             # 기술 스택 상세 정보  \n├── coding-standards.md       # 코딩 컨벤션\n├── current-tasks.md          # 현재 진행 중인 작업들\n├── common-issues.md          # 자주 발생하는 이슈들\n└── prompt-templates.md       # 자주 쓰는 프롬프트 모음\n```\n\n### project-config.md 예시\n\n```markdown\n# 프로젝트 설정\n\n## 기본 정보\n- **프로젝트명**: Service Interface Mock\n- **기술 스택**: Node.js + TypeScript + Express\n- **목적**: 마이크로서비스 개발시 의존성 Mock 서버\n\n## 현재 아키텍처\n- REST API 기반\n- JSON 설정 파일로 응답 정의\n- Docker 컨테이너로 배포\n\n## 개발 철학\n- 타입 안정성 최우선\n- 간단하고 직관적인 설정\n- 개발 생산성 중심\n\n## 제약사항\n- 기존 API 호환성 유지\n- 성능보다는 유연성 우선\n- 외부 의존성 최소화\n```\n\n### 업데이트 시점\n\n- **새로운 요구사항 추가될 때**\n- **기술 스택 변경될 때**  \n- **반복되는 질문이 생길 때**\n- **프로젝트 방향성이 바뀔 때**\n\n## 🚀 실전 적용 사례\n\n### 전체 워크플로우 적용 예시\n\n실제로 제가 **새로운 API 엔드포인트를 개발**할 때 이 워크플로우를 어떻게 적용하는지 보여드릴게요:\n\n**1단계: 새 창 열기**\n```\n(Claude Code에서 새 대화 시작)\n```\n\n**2단계: Context 제공**\n```\n다음 프로젝트 맥락을 파악해줘:\n\n[project-config.md 내용 첨부]\n\n현재 상황: 사용자 인증 API 엔드포인트 추가 필요\n요구사항: JWT 토큰 기반, 입력 유효성 검사, 에러 핸들링 포함\n```\n\n**3단계: 답변 포맷 생각하게 하기**\n```\n이런 API 엔드포인트 개발 요청에 대해 \n어떤 순서와 구조로 답변하는 게 가장 도움이 될까?\n```\n\n**4단계: 명확한 Request**\n```\n제안한 구조에 따라서:\n1. TypeScript 타입 안정성을 고려한 엔드포인트 설계\n2. 입력 유효성 검사 로직\n3. JWT 토큰 처리 로직\n4. 통합된 최종 코드\n를 제공해줘.\n```\n\n**5단계: Feedback**\n```\n좋은 방향이야! 그런데 실제 운영환경에서는 \nrate limiting도 고려해야 할 것 같아. \n이 부분도 추가해서 개선해줄 수 있을까?\n```\n\n**6단계: Context 업데이트**\n```\n# current-tasks.md에 추가\n- [x] 사용자 인증 API 구현\n- 다음: rate limiting 미들웨어 구현\n```\n\n## 💡 각 AI 도구별 적용 팁\n\n### Cursor Pro에서\n- **프로젝트 전체 맥락**을 Chat에서 제공\n- **파일 단위**로 구체적인 요청\n\n### Claude Code에서  \n- **복잡한 로직**은 단계별로 나누어서\n- **3시간 제한** 고려해서 핵심만 집중\n\n### Gemini에서\n- **문서화 작업**시 이 워크플로우가 가장 효과적\n- **Research** 단계에서 Context가 특히 중요\n\n### ChatGPT에서\n- **감정노동** 관련 작업에도 이 구조 적용\n- **이메일 작성**시 Context = 상대방 정보 + 목적\n\n## 🔚 마무리하며...\n\n이 6단계 워크플로우는 **어떤 AI든 상관없이** 적용할 수 있는 범용적인 방법론입니다. \n\n가장 중요한 건 **AI를 단순한 답변 기계로 쓰지 말고**, **함께 생각하는 파트너로 활용**하는 것입니다.\n\n다음 3편에서는 **워크스페이스 운영 실전기**를 다뤄보겠습니다. 실제로 5~8개의 Cursor 인스턴스를 어떻게 효율적으로 관리하는지, Remote SSH vs Native 환경을 어떻게 구성하는지 상세히 공유할 예정입니다!\n\n여러분은 AI와 대화할 때 어떤 패턴을 사용하고 계신가요? 댓글로 공유해주세요!\n\n---\n\n*이 시리즈의 다른 글들:*\n- *[1편: AI 스택 소개와 전체 개요](./2025-07-24-ai-workflow-productivity.md)*\n- *3편: 워크스페이스 운영 실전기 (곧 공개)*","categories":["Tech","AI"],"tags":["AI","Productivity","Workflow","Prompt","Context"],"pubDate":"2025-07-25T00:00:00.000Z","url":"/blog/2025-07-25-ai-workflow-common-methods/"},{"id":"2025-07-28-prd-methodology","title":"AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (5편 완결) - PRD 방법론으로 새 프로젝트 시작하기","description":"Gemini 2.5 Pro로 Research를 돌려서 들쭉날쭉한 AI를 더 잘하게 만드는 PRD 작성법과 실제 적용 사례","content":"![PRD Methodology](/assets/img/ai-workflow.jpg)\n\n# AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (5편 완결)\n## PRD 방법론으로 새 프로젝트 시작하기\n\n안녕하세요! 드디어 이 시리즈의 마지막 편입니다. [1편](./2025-07-24-ai-workflow-productivity.md)부터 [4편](./2025-07-27-ai-prompt-strategies.md)까지 AI 스택, 워크플로우, 워크스페이스, 프롬프트 전략을 다뤘다면, 이번 5편에서는 **PRD 방법론으로 새 프로젝트를 체계적으로 시작하는 법**을 공유합니다.\n\n특히 **Gemini 2.5 Pro로 Research를 돌려서** 들쭉날쭉한 AI Agent들을 조금이나마 더 잘하게 만드는 방법과, 실제 적용 사례까지 모두 공개할게요!\n\n## 🤔 왜 PRD가 중요한가?\n\n### AI가 들쭉날쭉한 이유\n\n**AI들이 일을 잘하기도 하고 못하기도 하는** 가장 큰 이유는 **맥락(Context) 부족** 때문입니다. \n\n```\n❌ 일반적인 요청:\n\"사용자 관리 시스템 만들어줘\"\n\n🤖 AI 응답: \n\"네, 어떤 기능이 필요한가요? 기술 스택은 뭘로 할까요? \n사용자 규모는 어느 정도인가요? 보안 요구사항은...?\"\n```\n\n이런 상황이 계속 반복되면서 **핑퐁이 너무 많아지고**, 결국 AI도 헷갈리고 우리도 지치게 됩니다.\n\n### PRD의 힘\n\n**PRD(Product Requirements Document)를 제대로 작성하면:**\n- ✅ AI가 **한 번에 정확한 결과물** 생성\n- ✅ **프로젝트 방향성** 명확화\n- ✅ **팀 커뮤니케이션** 효율성 증대\n- ✅ **개발 중 변경사항** 최소화\n\n## 📋 제가 사용하는 PRD 템플릿\n\n### 기본 구조\n\n**Gemini 2.5 Pro Research를 통해** 여러 PRD 방법론을 연구한 결과, 다음 구조가 가장 효과적이었습니다:\n\n```markdown\n# 📋 PRD: [프로젝트명]\n\n## 🎯 1. 프로젝트 개요\n### 1.1 목적 및 배경\n### 1.2 핵심 문제 정의\n### 1.3 성공 지표\n\n## 👥 2. 사용자 및 시장 분석  \n### 2.1 타겟 사용자\n### 2.2 사용자 페르소나\n### 2.3 경쟁사 분석\n\n## 🔧 3. 기능 요구사항\n### 3.1 핵심 기능 (Must Have)\n### 3.2 중요 기능 (Should Have)  \n### 3.3 선택 기능 (Could Have)\n\n## 🏗️ 4. 기술 요구사항\n### 4.1 기술 스택\n### 4.2 아키텍처 설계\n### 4.3 성능 요구사항\n### 4.4 보안 요구사항\n\n## 📅 5. 개발 계획\n### 5.1 마일스톤\n### 5.2 우선순위\n### 5.3 위험 요소\n\n## 📊 6. 비즈니스 고려사항\n### 6.1 예산 및 리소스\n### 6.2 수익 모델\n### 6.3 확장성 계획\n```\n\n## 🔍 Gemini 2.5 Pro Research 활용법\n\n### 1단계: 시장 조사 및 경쟁사 분석\n\n**실제 제가 쓰는 Gemini Research 프롬프트:**\n\n```\n다음 주제에 대한 포괄적인 시장 조사를 진행해줘:\n\n**주제**: 개발자용 API Mock 서버 도구\n\n**조사 범위:**\n1. 🌐 **시장 현황** (2024년 기준)\n   - 전체 시장 규모 및 성장률\n   - 주요 트렌드 및 기술 동향\n   - 사용자 요구사항 변화\n\n2. 🏢 **경쟁사 분석**\n   - 주요 업체 및 제품 (5-10개)\n   - 각 제품의 특징, 장단점\n   - 가격 정책 및 비즈니스 모델\n   - 사용자 리뷰 및 평가\n\n3. 🎯 **기회 영역**\n   - 기존 제품들의 한계점\n   - 미충족 사용자 니즈\n   - 새로운 접근 방식 제안\n\n4. 📊 **데이터 기반 인사이트**\n   - 시장 점유율 분석\n   - 사용자 만족도 지표\n   - 기술적 우위 요소\n\n**결과물 형태**: \n- 체계적인 보고서 (한국어)\n- 주요 인사이트 요약\n- 전략적 제안사항 포함\n\n현재 시간을 고려해서 최신 정보로 조사해줘.\n```\n\n### 2단계: 기술 스택 Research\n\n```\n다음 프로젝트에 최적화된 기술 스택을 연구해서 제안해줘:\n\n**프로젝트**: 개발자용 API Mock 서버\n**요구사항**:\n- 🚀 빠른 프로토타이핑 지원\n- 📱 다양한 클라이언트 환경 지원\n- 🔧 쉬운 설정 및 관리\n- 📊 실시간 모니터링\n- 🔒 기본적인 보안 기능\n\n**고려사항**:\n- 개발팀 3-5명 규모\n- 6개월 내 MVP 출시 목표\n- 오픈소스 우선 고려\n- 클라우드 배포 가능\n\n**연구 범위**:\n1. **Backend Framework** 비교분석\n   - Node.js (Express, Fastify, Koa)\n   - Python (FastAPI, Django, Flask)\n   - Go (Gin, Echo, Fiber)\n   - 각각의 성능, 생산성, 생태계 비교\n\n2. **Database** 선택 기준\n   - 관계형 vs NoSQL\n   - 개발 편의성 vs 성능\n   - 스케일링 고려사항\n\n3. **Frontend** (관리 대시보드용)\n   - React vs Vue vs Svelte\n   - 최신 트렌드 반영\n\n4. **DevOps & Infrastructure**\n   - 컨테이너화 전략\n   - CI/CD 파이프라인\n   - 모니터링 도구\n\n**결과물**: \n- 기술별 상세 비교표\n- 추천 조합 3가지\n- 선택 이유 및 근거\n- 마이그레이션 계획\n```\n\n### 3단계: 사용자 페르소나 Research\n\n```\n다음 프로덕트의 사용자 페르소나를 연구해서 작성해줘:\n\n**프로덕트**: 개발자용 API Mock 서버 도구\n\n**연구 방법**:\n1. 📊 기존 사용자 조사 데이터 분석\n2. 🔍 개발자 커뮤니티 니즈 파악\n3. 📈 사용 패턴 및 워크플로우 분석\n\n**작성할 페르소나** (3-4개):\n각 페르소나마다 다음 정보 포함:\n- 👤 기본 정보 (이름, 나이, 직책, 경력)\n- 🎯 목표 및 동기\n- 😰 페인 포인트 및 좌절 요소\n- 🛠️ 현재 사용 도구 및 워크플로우\n- 📱 기술 친숙도 및 선호도\n- 💬 실제 인용구 (가상이지만 현실적으로)\n\n**페르소나 카테고리 예상**:\n1. **신입 개발자** (1-3년차)\n2. **시니어 개발자** (5년차+)\n3. **팀 리드/아키텍트** (관리 + 기술)\n4. **QA 엔지니어** (테스팅 중심)\n\n**활용 목적**:\n- 기능 우선순위 결정\n- UI/UX 설계 방향\n- 마케팅 메시지 타겟팅\n- 제품 로드맵 수립\n\n현실적이고 구체적인 페르소나를 만들어줘.\n```\n\n## 🚀 실전 PRD 작성 사례\n\n### 실제 프로젝트: \"Service Interface Mock\"\n\n**제가 최근에 Gemini Research로 작성한 실제 PRD를 공유해드릴게요:**\n\n```markdown\n# 📋 PRD: Service Interface Mock\n\n## 🎯 1. 프로젝트 개요\n\n### 1.1 목적 및 배경\n**문제**: 마이크로서비스 개발 시 의존성 서비스가 준비되지 않아 개발 지연 발생\n**해결책**: 쉽고 빠르게 API Mock 서버를 구성할 수 있는 도구 개발\n\n### 1.2 핵심 문제 정의\n- ❌ 기존 Mock 도구들은 설정이 복잡함\n- ❌ 실제 API와 유사한 응답 생성이 어려움  \n- ❌ 팀 간 Mock 설정 공유가 번거로움\n- ❌ 동적 응답 생성이 제한적임\n\n### 1.3 성공 지표\n- 📊 설정 시간: 5분 이내 (기존 30분 → 5분)\n- 📈 사용자 만족도: 4.5/5.0 이상\n- 🚀 첫 Mock API 생성: 1분 이내\n- 👥 팀 도입률: 80% 이상\n\n## 👥 2. 사용자 및 시장 분석\n\n### 2.1 타겟 사용자 (Gemini Research 결과)\n1. **프론트엔드 개발자** (40%)\n   - 백엔드 API 대기 중 개발 진행 필요\n   - 다양한 응답 케이스 테스트 요구\n\n2. **백엔드 개발자** (35%)\n   - 의존성 서비스 Mock 필요\n   - 장애 상황 시뮬레이션 요구\n\n3. **QA 엔지니어** (25%)\n   - 다양한 테스트 시나리오 생성\n   - 엣지 케이스 재현 필요\n\n### 2.2 사용자 페르소나 (Research 기반)\n\n**페르소나 1: 김민수 (프론트엔드 개발자, 3년차)**\n- 🎯 **목표**: 백엔드 API 완성 전에 UI 개발 완료\n- 😰 **페인포인트**: \"또 백엔드팀 기다려야 하나...\"\n- 🛠️ **현재 도구**: Postman, JSON Server (설정 복잡)\n- 💬 **인용구**: \"5분만에 Mock 서버 띄울 수 있으면 얼마나 좋을까\"\n\n**페르소나 2: 박서영 (백엔드 개발자, 7년차)**\n- 🎯 **목표**: 의존성 없이 독립적 개발 환경 구축\n- 😰 **페인포인트**: \"외부 API 장애시 테스트가 불가능해\"\n- 🛠️ **현재 도구**: WireMock, Mockoon (설정 복잡, 공유 어려움)\n- 💬 **인용구**: \"팀 전체가 같은 Mock 설정을 쉽게 공유했으면\"\n\n### 2.3 경쟁사 분석 (Gemini Research 결과)\n\n| 제품 | 장점 | 단점 | 가격 |\n|------|------|------|------|\n| **Postman Mock** | 쉬운 사용, 강력한 생태계 | 동적 응답 제한, 고가 | $12/월 |\n| **WireMock** | 강력한 기능, 무료 | 설정 복잡, 학습곡선 | 무료 |\n| **Mockoon** | 직관적 UI | 협업 기능 부족 | 무료/유료 |\n| **JSON Server** | 초간단 설정 | 기능 제한적 | 무료 |\n\n**🎯 기회 영역**: \n- 쉬운 설정 + 강력한 기능 조합\n- 팀 협업 중심 설계\n- 실제 환경과 유사한 응답 생성\n\n## 🔧 3. 기능 요구사항\n\n### 3.1 핵심 기능 (Must Have)\n- 📋 **JSON 설정 기반 Mock 생성**\n  - YAML/JSON 파일로 API 정의\n  - RESTful API 자동 생성\n  - 즉시 실행 가능\n\n- 🔄 **동적 응답 생성**\n  - 요청 파라미터 기반 응답 변경\n  - 랜덤 데이터 생성 (faker.js 활용)\n  - 조건부 응답 로직\n\n- 📊 **실시간 모니터링**\n  - 요청/응답 로깅\n  - 성능 메트릭 수집\n  - 간단한 대시보드\n\n### 3.2 중요 기능 (Should Have)\n- 👥 **팀 협업 기능**\n  - Git 기반 설정 공유\n  - 버전 관리\n  - 팀원별 권한 관리\n\n- 🔒 **보안 기능**\n  - API 키 기반 인증\n  - CORS 설정\n  - Rate Limiting\n\n### 3.3 선택 기능 (Could Have)\n- 🤖 **AI 기반 Mock 생성**\n  - OpenAPI 스펙에서 자동 생성\n  - 실제 API 호출 패턴 학습\n  - 지능형 응답 추천\n\n## 🏗️ 4. 기술 요구사항\n\n### 4.1 기술 스택 (Gemini Research 추천)\n\n**Backend**: Node.js + TypeScript + Express\n- ✅ 빠른 개발 속도\n- ✅ 풍부한 생태계 (faker.js 등)\n- ✅ JSON 처리 최적화\n- ✅ 개발팀 기술 스택과 일치\n\n**Frontend**: React + TypeScript + Vite\n- ✅ 컴포넌트 재사용성\n- ✅ 개발자 친숙도 높음\n- ✅ 빠른 빌드 및 개발 서버\n\n**Database**: SQLite (개발) + PostgreSQL (운영)\n- ✅ 간단한 배포\n- ✅ 확장성 고려\n- ✅ SQL 쿼리 최적화 가능\n\n### 4.2 아키텍처 설계\n\n```mermaid\ngraph TB\n    subgraph \"Client Layer\"\n        A[React Dashboard]\n        B[CLI Tool]\n    end\n    \n    subgraph \"API Gateway\"\n        C[Express Server]\n    end\n    \n    subgraph \"Core Services\"\n        D[Mock Engine]\n        E[Config Manager]\n        F[Monitor Service]\n    end\n    \n    subgraph \"Data Layer\"  \n        G[SQLite/PostgreSQL]\n        H[File System]\n    end\n    \n    A --> C\n    B --> C\n    C --> D\n    C --> E\n    C --> F\n    D --> G\n    E --> H\n    F --> G\n```\n\n### 4.3 성능 요구사항\n- ⚡ **응답 시간**: 평균 50ms 이하\n- 🔄 **동시 연결**: 1000개 이상\n- 📈 **처리량**: 10,000 req/sec\n- 💾 **메모리 사용량**: 512MB 이하\n\n### 4.4 보안 요구사항\n- 🔐 JWT 기반 인증\n- 🛡️ Input validation 및 sanitization\n- 🚫 SQL Injection 방지\n- 📝 접근 로그 기록\n\n## 📅 5. 개발 계획\n\n### 5.1 마일스톤\n\n**Phase 1: MVP (4주)**\n- ✅ 기본 Mock 서버 구현\n- ✅ JSON 설정 파일 파싱\n- ✅ 간단한 대시보드\n- ✅ CLI 도구 기본 기능\n\n**Phase 2: 협업 기능 (4주)**\n- 🔄 Git 기반 설정 관리\n- 👥 사용자 인증 시스템\n- 📊 모니터링 대시보드 강화\n- 📱 반응형 웹 UI\n\n**Phase 3: 고급 기능 (4주)**\n- 🤖 AI 기반 Mock 생성\n- 🔒 고급 보안 기능\n- 📈 성능 최적화\n- 🧪 자동화된 테스트\n\n### 5.2 우선순위 (MoSCoW)\n- **Must**: 기본 Mock 기능, 설정 관리\n- **Should**: 모니터링, 팀 협업\n- **Could**: AI 기능, 고급 보안\n- **Won't**: 복잡한 분석 기능\n\n### 5.3 위험 요소\n- 🚨 **기술적 위험**: Node.js 성능 한계\n  - **대응**: 성능 테스트 및 최적화\n- ⏰ **일정 위험**: 외부 의존성 지연\n  - **대응**: 핵심 기능 우선 개발\n- 👥 **리소스 위험**: 개발자 리소스 부족\n  - **대응**: 외부 도움 또는 기능 축소\n\n## 📊 6. 비즈니스 고려사항\n\n### 6.1 예산 및 리소스\n- 💰 **개발 비용**: 개발자 3명 × 3개월\n- 🖥️ **인프라 비용**: AWS/Vercel 월 $100\n- 🛠️ **도구 비용**: 개발 도구 라이선스 월 $200\n\n### 6.2 수익 모델 (선택사항)\n- 🆓 **오픈소스**: 기본 기능 무료 제공\n- 💼 **엔터프라이즈**: 고급 기능 유료 ($50/월)\n- ☁️ **클라우드**: 호스팅 서비스 ($20/월)\n\n### 6.3 확장성 계획\n- 🌐 **멀티 언어**: Python, Go SDK 제공\n- 🔌 **플러그인**: 써드파티 통합 지원\n- 📱 **모바일**: React Native 앱 고려\n```\n\n## 🛠️ PRD 기반 AI 활용 전략\n\n### AI에게 PRD 전달하는 방법\n\n**완성된 PRD가 있으면** AI들의 성능이 극적으로 향상됩니다:\n\n#### Cursor Pro에서 활용\n\n```\n@PRD_ServiceInterfaceMock.md\n\n위 PRD를 기반으로 src/services/mockEngine.ts를 구현해줘.\n\n특히 다음 요구사항을 정확히 반영해서:\n1. TypeScript 타입 안정성 (PRD 4.1 기술 요구사항)\n2. 동적 응답 생성 (PRD 3.1 핵심 기능)  \n3. 성능 요구사항 50ms 이하 (PRD 4.3)\n4. 에러 핸들링 표준 (프로젝트 convention 참고)\n\n구현할 주요 메서드:\n- generateMockResponse()\n- parseConfigFile()\n- validateRequest()\n```\n\n#### Claude Code에서 활용\n\n```\n첨부된 PRD 문서를 기반으로 전체 아키텍처를 설계해줘.\n\n**PRD 정보**:\n[PRD 내용 전체 복사-붙여넣기]\n\n**요청사항**:\n1. 📋 상세 아키텍처 다이어그램 (Mermaid)\n2. 🔧 핵심 모듈별 구현 가이드\n3. 🧪 테스트 전략 및 계획\n4. 📝 API 스펙 정의 (OpenAPI 3.0)\n5. 🚀 배포 및 인프라 계획\n\nPRD의 기술 요구사항과 성능 지표를 정확히 반영해서 설계해줘.\n```\n\n#### Gemini CLI에서 활용\n\n```bash\ngemini \"\n다음 PRD를 기반으로 개발자용 README.md를 생성해줘:\n\n$(cat PRD_ServiceInterfaceMock.md)\n\n**생성할 README 구조**:\n1. 🎯 프로젝트 소개 (PRD 1.1 기반)\n2. ⚡ Quick Start (5분 내 실행 가능)\n3. 📋 주요 기능 (PRD 3.1-3.3 기반)\n4. 🏗️ 아키텍처 개요 (PRD 4.2 기반)  \n5. 🔧 설치 및 설정\n6. 📚 API 문서 링크\n7. 🤝 기여 가이드\n\n**톤**: 개발자 친화적, 실용적\n**길이**: 읽기 시간 5분 이내\n\"\n```\n\n## 🔄 PRD 지속적 개선 방법\n\n### 1. 주기적 업데이트\n\n**월간 PRD 리뷰 체크리스트:**\n```markdown\n## PRD 리뷰 체크리스트 (월간)\n\n### 📊 사용자 피드백 반영\n- [ ] 실제 사용자 인터뷰 결과 반영\n- [ ] 사용 패턴 분석 결과 업데이트\n- [ ] 페인포인트 변화 확인\n\n### 🏗️ 기술 요구사항 검토  \n- [ ] 새로운 기술 트렌드 반영\n- [ ] 성능 지표 현실성 검토\n- [ ] 보안 요구사항 강화\n\n### 📅 개발 계획 조정\n- [ ] 실제 개발 진행률 vs 계획 비교\n- [ ] 우선순위 재조정 필요성 검토\n- [ ] 위험 요소 업데이트\n\n### 💼 비즈니스 관점 업데이트\n- [ ] 시장 상황 변화 반영\n- [ ] 경쟁사 동향 분석 업데이트\n- [ ] 수익 모델 검증 결과 반영\n```\n\n### 2. AI Research 자동화\n\n**월간 Research 자동화 스크립트:**\n```bash\n#!/bin/bash\n# monthly-prd-research.sh\n\necho \"📊 월간 PRD Research 시작...\"\n\n# 1. 경쟁사 동향 조사\necho \"🔍 경쟁사 분석 중...\"\ngemini \"\nMock Server 도구 시장의 최신 동향을 조사해줘 (최근 1개월):\n1. 새로운 제품 출시\n2. 기존 제품 업데이트  \n3. 가격 정책 변화\n4. 사용자 반응 및 리뷰\n5. 새로운 기술 적용 사례\n\n기존 PRD와 비교해서 업데이트가 필요한 부분을 제안해줘.\n\" > research/competitor-analysis-$(date +%Y%m).md\n\n# 2. 기술 트렌드 조사\necho \"🚀 기술 트렌드 분석 중...\"\ngemini \"\n개발자 도구 및 Mock Server 관련 기술 트렌드 조사:\n1. 새로운 프레임워크 및 라이브러리\n2. 성능 최적화 기법\n3. 보안 강화 방안\n4. AI/ML 적용 사례\n5. 개발자 경험(DX) 개선 사례\n\n현재 PRD의 기술 스택과 비교 분석해줘.\n\" > research/tech-trends-$(date +%Y%m).md\n\n# 3. 사용자 니즈 변화 조사\necho \"👥 사용자 니즈 분석 중...\"\ngemini \"\n개발자 커뮤니티에서 Mock Server 관련 논의 동향:\n1. Stack Overflow 질문 트렌드\n2. GitHub Issues 분석\n3. Reddit/HackerNews 논의\n4. 개발자 설문조사 결과\n\n새로운 페인포인트나 요구사항을 식별해줘.\n\" > research/user-needs-$(date +%Y%m).md\n\necho \"✅ Research 완료. research/ 폴더 확인.\"\n```\n\n## 📈 PRD 효과 측정\n\n### Before & After 비교\n\n**PRD 도입 전 (AI 활용):**\n```\n🕐 프로젝트 시작 시간: 2-3일\n😵 AI 대화 횟수: 평균 50회 이상\n🔄 요구사항 변경: 주간 5-10회\n😤 팀 혼란도: 높음 (방향성 불일치)\n📉 AI 결과물 품질: 60-70점\n```\n\n**PRD 도입 후:**\n```\n🚀 프로젝트 시작 시간: 반나절\n✨ AI 대화 횟수: 평균 10-15회\n🎯 요구사항 변경: 주간 1-2회\n😊 팀 혼란도: 낮음 (명확한 방향성)\n📈 AI 결과물 품질: 85-90점\n```\n\n### 실제 측정 지표\n\n**제가 추적하는 PRD 효과 지표:**\n```markdown\n## PRD 효과 측정 지표\n\n### 🚀 개발 효율성\n- ⏱️ **초기 설정 시간**: 3일 → 0.5일 (85% 단축)\n- 🔄 **반복 작업 감소**: 주간 재작업 5회 → 1회\n- 📋 **명확한 요구사항**: 모호한 요청 80% → 20%\n\n### 🤖 AI 활용 효율성  \n- 💬 **평균 대화 횟수**: 50회 → 15회 (70% 감소)\n- ✅ **첫 응답 만족도**: 40% → 80% (100% 향상)\n- 🎯 **원하는 결과 도출**: 3-5회 → 1-2회\n\n### 👥 팀 협업 향상\n- 📞 **불필요한 미팅**: 주간 5회 → 2회\n- 📝 **문서 공유**: 팀원별 개별 → 단일 PRD 참조\n- 🎯 **방향성 일치도**: 60% → 95%\n```\n\n## 🎯 PRD 작성 실전 팁\n\n### 1. Gemini Research 최적화 팁\n\n**더 좋은 Research 결과를 얻는 방법:**\n\n```\n✅ 구체적인 시간 범위 명시\n\"최근 6개월간의 트렌드를 분석해줘\"\n\n✅ 정량적 지표 요청\n\"시장 점유율, 사용자 수, 가격 비교를 포함해줘\"\n\n✅ 소스 다양화 요청  \n\"업계 보고서, 사용자 리뷰, 개발자 커뮤니티 의견을 종합해줘\"\n\n✅ 액션 아이템 요청\n\"분석 결과를 바탕으로 우리가 취해야 할 액션을 제안해줘\"\n```\n\n### 2. PRD 작성 시 주의사항\n\n**❌ 피해야 할 실수들:**\n```\n❌ 너무 추상적인 목표: \"사용자 경험 개선\"\n✅ 구체적인 지표: \"API 응답 시간 50ms 이하\"\n\n❌ 모호한 사용자 정의: \"개발자들\"  \n✅ 명확한 페르소나: \"3년차 프론트엔드 개발자\"\n\n❌ 기술 우선 사고: \"React를 써야 해\"\n✅ 요구사항 우선: \"빠른 UI 업데이트가 필요해\"\n```\n\n### 3. AI별 최적화 PRD 활용\n\n**각 AI 도구별로 PRD를 다르게 활용:**\n\n```\n🔵 Cursor Pro: \n- PRD 파일을 프로젝트 루트에 위치\n- @PRD.md 참조로 컨텍스트 제공\n- 기술 요구사항 섹션 중점 활용\n\n🟣 Claude Code:\n- 전체 PRD 복사해서 한 번에 제공  \n- 아키텍처 설계시 전체 맥락 활용\n- 비즈니스 로직 구현시 요구사항 참조\n\n🔴 Gemini CLI:\n- PRD 기반 문서 자동 생성에 활용\n- 사용자 가이드, API 문서 생성\n- 마케팅 콘텐츠 제작시 활용\n\n🟢 ChatGPT:\n- PRD 기반 커뮤니케이션 문서 작성\n- 사용자 대상 설명 자료 제작\n- 팀 내부 발표 자료 준비\n```\n\n## 🔚 시리즈 완결하며...\n\n이렇게 **AI 멀티플렉싱 워크플로우 시리즈**가 완결되었습니다!\n\n5편에 걸쳐 다룬 내용을 정리하면:\n1. **[1편](./2025-07-24-ai-workflow-productivity.md)**: AI 스택 소개와 전체 개요\n2. **[2편](./2025-07-25-ai-workflow-common-methods.md)**: 공통 워크플로우 방법론  \n3. **[3편](./2025-07-26-ai-workspace-management.md)**: 워크스페이스 운영 실전기\n4. **[4편](./2025-07-27-ai-prompt-strategies.md)**: 도구별 프롬프트 전략\n5. **[5편](./2025-07-28-prd-methodology.md)**: PRD 방법론으로 프로젝트 시작하기\n\n## 💡 핵심 메시지\n\n결국 가장 중요한 건 **AI를 단순한 도구로 쓰지 말고**, **체계적인 워크플로우 안에서 전략적으로 활용**하는 것입니다.\n\n- 🎯 **명확한 목표** (PRD로 정의)\n- 🔄 **체계적인 과정** (6단계 워크플로우)  \n- 🛠️ **적절한 도구** (각 상황에 맞는 AI 선택)\n- 📊 **지속적인 개선** (측정과 피드백)\n\n## 🙏 마무리 인사\n\n**월 100달러짜리 AI 스택**으로 시작한 이야기가 여기까지 왔네요. \n\n이 시리즈가 여러분의 AI 활용에 조금이라도 도움이 되었기를 바랍니다. 그리고 여러분만의 독특한 워크플로우나 노하우가 있다면 꼭 댓글로 공유해주세요!\n\n**Claude Code usage가 다시 찰 때까지** 쓴 글이 이렇게 시리즈가 되었다는 게 신기하네요 ㅋㅋㅋ\n\n앞으로도 AI 도구들이 계속 발전할 텐데, 그때마다 새로운 워크플로우와 노하우들을 계속 공유해보겠습니다!\n\n---\n\n*🎉 **시리즈 완결 기념으로** 제가 실제 사용하는 PRD 템플릿과 자동화 스크립트들을 [GitHub Repo](https://github.com/jayleekr)에 공개할 예정입니다. 관심 있으시면 팔로우해주세요!*\n\n*이 시리즈의 모든 글들:*\n- *[1편: AI 스택 소개와 전체 개요](./2025-07-24-ai-workflow-productivity.md)*\n- *[2편: 공통 워크플로우 방법론](./2025-07-25-ai-workflow-common-methods.md)*\n- *[3편: 워크스페이스 운영 실전기](./2025-07-26-ai-workspace-management.md)*\n- *[4편: 도구별 프롬프트 전략](./2025-07-27-ai-prompt-strategies.md)*\n- *[5편: PRD 방법론으로 새 프로젝트 시작하기](./2025-07-28-prd-methodology.md)* ✅","categories":["Tech","AI"],"tags":["AI","Productivity","PRD","Project","Planning","Gemini","Research"],"pubDate":"2025-07-28T00:00:00.000Z","url":"/blog/2025-07-28-prd-methodology/"},{"id":"2025-07-26-ai-workspace-management","title":"AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (3편) - 워크스페이스 운영 실전기","description":"5~8개 Cursor 인스턴스를 효율적으로 관리하는 노하우와 Remote SSH vs Native 환경 구성법","content":"![AI Workspace Management](/assets/img/ai-workflow.jpg)\n\n# AI 멀티플렉싱 워크플로우: ADHD급 생산성 향상기 (3편)\n## 워크스페이스 운영 실전기\n\n안녕하세요! [1편](./2025-07-24-ai-workflow-productivity.md)에서 AI 스택을, [2편](./2025-07-25-ai-workflow-common-methods.md)에서 공통 워크플로우를 다뤘다면, 이번 3편에서는 **실제 워크스페이스 운영 실전기**를 상세히 공유해보겠습니다.\n\n제가 현재 **5~8개의 Cursor 인스턴스**를 동시에 돌리면서 각각의 프로젝트를 어떻게 관리하는지, Remote SSH와 Native 환경을 어떻게 구성하는지 노하우를 전수해드릴게요!\n\n## 🏗️ 현재 워크스페이스 구성 현황\n\n### 실시간 스냅샷 (지금 이 순간 ㅋㅋ)\n\n```\n🖥️  MacBook Pro M2 Max (64GB RAM) - 메인 머신\n├── 📡 Remote SSH 기반 (4-5개)\n│   ├── 🔴 Production Code Server (AWS EC2)\n│   ├── 🟡 Build Server 1 (Docker 환경)\n│   ├── 🟠 Build Server 2 (QA 환경)  \n│   ├── 🔵 Test Environment (Staging)\n│   └── 🟣 Client Demo Server (필요시)\n│\n└── 💻 Native 기반 (3-4개)\n    ├── 📝 Blog/Documentation \n    ├── 🎓 강의자료 (CPS 3기)\n    ├── 🧪 실험용 프로젝트\n    └── 📋 Config/Scripts 관리\n```\n\n**진짜 지금도 8개가 떠있어요 ㅋㅋ** 윈도우 관리가 진짜 ADHD 수준...\n\n## 🌐 Remote SSH vs Native: 언제 뭘 쓸까?\n\n### Remote SSH를 쓰는 경우\n\n**✅ 이런 상황에서 Remote SSH:**\n- **무거운 빌드**가 필요한 프로젝트 (Docker 빌드, 큰 패키지)\n- **팀 협업**이 중요한 Production 코드\n- **DB나 외부 서비스 연동**이 복잡한 경우\n- **보안상 민감한** 프로젝트\n- **지속적으로 실행**되어야 하는 서비스들\n\n**장점:**\n- 💪 **높은 성능**: 서버 스펙을 마음껏 활용\n- 🔄 **연속성**: 연결이 끊어져도 작업 지속\n- 👥 **팀 협업**: 동일한 환경에서 작업\n- 🛡️ **보안**: 민감한 코드가 로컬에 없음\n\n**단점:**\n- 🐌 **네트워크 의존성**: 인터넷 없으면 GG\n- 💸 **비용**: 서버 운영비\n- ⚙️ **초기 설정 복잡**\n\n### Native를 쓰는 경우\n\n**✅ 이런 상황에서 Native:**\n- **빠른 프로토타이핑**이 필요한 경우\n- **개인 프로젝트**나 실험적 코드\n- **문서 작성**이나 **블로그 포스팅**\n- **강의 자료** 준비\n- **Config 파일**들 관리\n\n**장점:**\n- ⚡ **빠른 반응속도**: 로컬이라 지연 없음\n- 📡 **오프라인 가능**: 인터넷 없어도 작업\n- 🎮 **직관적**: 바로바로 확인 가능\n\n**단점:**\n- 🔋 **배터리 소모**: 로컬 리소스 사용\n- 🚫 **협업 제한**: 혼자만 접근 가능\n\n## 🛠️ Remote SSH 환경 구성 실전\n\n### 1. 서버 준비 및 기본 설정\n\n**제가 쓰는 서버 스펙들:**\n```bash\n# Production Code Server (AWS EC2 t3.xlarge)\n- CPU: 4 vCPU\n- RAM: 16GB  \n- Storage: 100GB SSD\n- 용도: 메인 개발, Production 배포\n\n# Build Server (AWS EC2 c5.2xlarge)  \n- CPU: 8 vCPU\n- RAM: 16GB\n- Storage: 200GB SSD\n- 용도: Docker 빌드, CI/CD 파이프라인\n```\n\n**기본 서버 설정 스크립트:**\n```bash\n#!/bin/bash\n# setup-dev-server.sh\n\n# 기본 패키지 업데이트\nsudo apt update && sudo apt upgrade -y\n\n# 개발 도구 설치\nsudo apt install -y git curl wget vim htop tree\n\n# Node.js 설치 (최신 LTS)\ncurl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# Docker 설치\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\nsudo usermod -aG docker $USER\n\n# Git 설정\ngit config --global user.name \"Jay Lee\"\ngit config --global user.email \"your@email.com\"\ngit config --global init.defaultBranch main\n\n# SSH 키 생성 (GitHub용)\nssh-keygen -t ed25519 -C \"your@email.com\" -f ~/.ssh/id_ed25519 -N \"\"\n\necho \"설정 완료! 재로그인 후 Docker 사용 가능\"\n```\n\n### 2. Cursor에서 Remote SSH 연결 설정\n\n**VS Code/Cursor SSH Config (~/.ssh/config):**\n```bash\n# Production Server\nHost prod-server\n    HostName your-prod-server-ip\n    User ubuntu\n    IdentityFile ~/.ssh/your-prod-key.pem\n    ServerAliveInterval 60\n    ServerAliveCountMax 3\n\n# Build Server  \nHost build-server\n    HostName your-build-server-ip\n    User ubuntu\n    IdentityFile ~/.ssh/your-build-key.pem\n    ServerAliveInterval 60\n    ServerAliveCountMax 3\n\n# Test Environment\nHost test-server\n    HostName your-test-server-ip\n    User ubuntu\n    IdentityFile ~/.ssh/your-test-key.pem\n    ServerAliveInterval 60\n```\n\n**Cursor에서 연결하기:**\n1. `Cmd + Shift + P` → \"Remote-SSH: Connect to Host\"\n2. 위에서 설정한 Host 선택\n3. 새 Cursor 창이 Remote 환경으로 열림\n4. 해당 프로젝트 폴더 열기\n\n### 3. Remote 환경에서 AI 도구 활용법\n\n**Remote에서 특히 유용한 패턴들:**\n\n```bash\n# 1. 프로젝트 Context 파일 준비\n📁 ~/projects/service-interface-mock/\n├── .ai-context/\n│   ├── server-config.md      # 서버 환경 정보\n│   ├── deployment.md         # 배포 관련 정보\n│   └── remote-workflow.md    # Remote 작업 가이드\n└── ...\n```\n\n**server-config.md 예시:**\n```markdown\n# 서버 환경 정보\n\n## 서버 스펙\n- Instance: AWS EC2 t3.xlarge\n- OS: Ubuntu 22.04 LTS\n- RAM: 16GB, CPU: 4 vCPU\n\n## 설치된 도구들\n- Node.js 20.x LTS\n- Docker & Docker Compose\n- Git 2.40+\n- PM2 (프로세스 관리)\n\n## 주요 경로\n- 프로젝트: ~/projects/\n- 로그: ~/logs/\n- Config: ~/config/\n\n## 네트워크 설정  \n- HTTP: 8080\n- HTTPS: 8443\n- DB: 5432 (PostgreSQL)\n```\n\n## 💻 Native 환경 구성 실전\n\n### 1. 로컬 개발환경 최적화\n\n**제가 쓰는 로컬 도구들:**\n```bash\n# Homebrew로 설치한 핵심 도구들\nbrew install git node bun\nbrew install --cask cursor\nbrew install htop tree jq\n\n# 글로벌 npm 패키지들\nnpm install -g typescript ts-node nodemon\nnpm install -g @astrojs/cli  # 블로그용\n```\n\n### 2. Git Repository 구조화\n\n**모든 워크스페이스는 Git Repo 기반:**\n```bash\n📁 ~/CodeWorkspace/\n├── 🏢 company-projects/\n│   ├── service-interface-mock/    # 메인 프로덕트\n│   ├── client-demo-app/          # 클라이언트 데모\n│   └── internal-tools/           # 내부 도구들\n│\n├── 📝 content-creation/\n│   ├── jayleekr.github.io/       # 블로그\n│   ├── cps-lectures/             # 강의 자료\n│   └── tech-writing/             # 기술 글쓰기\n│\n├── 🧪 experiments/\n│   ├── ai-experiments/           # AI 실험\n│   ├── new-tech-trials/          # 새 기술 테스트\n│   └── quick-prototypes/         # 빠른 프로토타입\n│\n└── ⚙️ dotfiles-and-configs/\n    ├── cursor-settings/          # Cursor 설정\n    ├── ai-context-templates/     # AI Context 템플릿\n    └── automation-scripts/       # 자동화 스크립트\n```\n\n### 3. Native에서 AI 활용 특화 팁\n\n**문서 작업시 폴더 구조:**\n```bash\n📁 jayleekr.github.io/\n├── .ai-context/\n│   ├── blog-style-guide.md      # 블로그 톤 가이드\n│   ├── content-templates.md     # 컨텐츠 템플릿\n│   └── seo-guidelines.md        # SEO 가이드라인\n├── src/content/blog/\n└── ...\n```\n\n**blog-style-guide.md 예시:**\n```markdown\n# 블로그 글쓰기 스타일 가이드\n\n## 톤 앤 매너\n- 친근하고 대화체적인 어조\n- 이모티콘과 감탄사 자연스럽게 사용 (ㅋㅋ, ㅠㅠ 등)\n- 경험 중심의 실용적 접근\n- 솔직하고 진솔한 표현\n\n## 구조\n- 문제-원인-해결책의 구조적 접근\n- 코드와 설명의 균형\n- Before/After 비교 선호\n\n## 독자층\n- 실무 개발자\n- AI 도구 활용에 관심 있는 사람들\n- 생산성 향상을 원하는 엔지니어들\n```\n\n## 🔄 워크스페이스 전환 및 관리 팁\n\n### 1. 빠른 전환을 위한 단축키 설정\n\n**macOS 기본 설정:**\n```bash\n# Mission Control 설정\n- Desktop 별로 Cursor 인스턴스 배치\n- Ctrl + 1~8로 빠른 전환\n- Cmd + Tab으로 앱 간 전환\n\n# Cursor 내부에서\n- Cmd + ` : 같은 앱의 윈도우간 전환\n- Cmd + W : 탭 닫기\n- Cmd + T : 새 탭\n```\n\n### 2. 컨텍스트 스위칭 최소화 전략\n\n**프로젝트별 독립성 유지:**\n```bash\n# 각 워크스페이스마다\n├── README.md                 # 프로젝트 개요\n├── .ai-context/             # AI 컨텍스트 문서들\n├── .cursor/                 # Cursor 설정\n├── docs/                    # 프로젝트 문서\n└── scripts/                 # 자동화 스크립트\n```\n\n**시작할 때 체크리스트:**\n1. **어떤 프로젝트**를 작업할 건지 명확히\n2. **해당 워크스페이스**만 열기\n3. **AI Context 문서** 먼저 확인\n4. **오늘의 목표** 명확히 설정\n\n### 3. 메모리 관리 및 성능 최적화\n\n**MacBook 64GB RAM이지만 그래도...**\n\n**메모리 사용량 모니터링:**\n```bash\n# Activity Monitor로 확인\n- Cursor Helper 프로세스들 주시\n- 필요없는 Extension 비활성화\n- 사용하지 않는 Remote 연결 종료\n```\n\n**성능 최적화 팁:**\n- **무거운 작업**은 Remote에서 (Docker 빌드 등)\n- **가벼운 작업**은 Native에서 (문서 작성 등)\n- **주기적으로** 사용하지 않는 워크스페이스 정리\n\n## 🔧 자동화 스크립트로 효율성 극대화\n\n### 1. 워크스페이스 빠른 시작 스크립트\n\n**start-workspace.sh:**\n```bash\n#!/bin/bash\n\nPROJECT_NAME=$1\n\nif [ -z \"$PROJECT_NAME\" ]; then\n    echo \"사용법: ./start-workspace.sh <프로젝트명>\"\n    echo \"예시: ./start-workspace.sh service-interface-mock\"\n    exit 1\nfi\n\n# 프로젝트 경로\nPROJECT_PATH=\"~/CodeWorkspace/$PROJECT_NAME\"\n\n# 디렉토리 존재 확인\nif [ ! -d \"$PROJECT_PATH\" ]; then\n    echo \"프로젝트가 존재하지 않습니다: $PROJECT_PATH\"\n    exit 1\nfi\n\n# Cursor로 프로젝트 열기\ncursor \"$PROJECT_PATH\"\n\n# AI Context 파일이 있다면 미리 준비\nif [ -f \"$PROJECT_PATH/.ai-context/project-config.md\" ]; then\n    echo \"AI Context 준비됨: $PROJECT_NAME\"\n    # 필요하면 클립보드에 복사\n    # cat \"$PROJECT_PATH/.ai-context/project-config.md\" | pbcopy\nfi\n\necho \"워크스페이스 시작됨: $PROJECT_NAME\"\n```\n\n### 2. Remote 서버 상태 체크 스크립트\n\n**check-servers.sh:**\n```bash\n#!/bin/bash\n\nSERVERS=(\"prod-server\" \"build-server\" \"test-server\")\n\necho \"🔍 서버 상태 체크 중...\"\n\nfor server in \"${SERVERS[@]}\"; do\n    echo -n \"📡 $server: \"\n    \n    if ssh -o ConnectTimeout=5 -o BatchMode=yes $server 'exit' 2>/dev/null; then\n        echo \"✅ 연결 가능\"\n        \n        # 기본 상태 정보\n        echo \"   - $(ssh $server 'uptime | cut -d\",\" -f1')\"\n        echo \"   - $(ssh $server 'df -h / | tail -1 | awk \"{print \\\"Disk: \\\" \\$5 \\\" used\\\"}\"')\"\n    else\n        echo \"❌ 연결 불가\"\n    fi\ndone\n```\n\n### 3. AI Context 업데이트 자동화\n\n**update-ai-context.sh:**\n```bash\n#!/bin/bash\n\n# 현재 작업 디렉토리 확인\nif [ ! -d \".ai-context\" ]; then\n    echo \"AI Context 디렉토리가 없습니다.\"\n    exit 1\nfi\n\n# Git 상태 확인하여 current-tasks.md 업데이트\ngit status --porcelain > .ai-context/current-git-status.txt\n\n# 최근 커밋 내용을 current-tasks.md에 추가\necho \"## 최근 작업 ($(date '+%Y-%m-%d'))\" >> .ai-context/current-tasks.md\ngit log --oneline -5 >> .ai-context/current-tasks.md\necho \"\" >> .ai-context/current-tasks.md\n\necho \"AI Context 업데이트 완료\"\n```\n\n## 📊 실제 사용 패턴 분석\n\n### 하루 워크플로우 예시\n\n**오전 (09:00-12:00):**\n```\n1. 📧 check-servers.sh 실행으로 상태 체크\n2. 🔴 Production Server 연결 → 긴급 이슈 체크\n3. 💻 Native에서 블로그 포스팅 (지금 이 글 ㅋㅋ)\n4. 🎓 강의 자료 업데이트\n```\n\n**오후 (13:00-18:00):**\n```\n1. 🟡 Build Server → CI/CD 파이프라인 개선\n2. 🔵 Test Environment → 새 기능 테스트\n3. 💻 Native → 실험적 프로토타입 작업\n4. 📝 Documentation 업데이트\n```\n\n**저녁 (19:00-22:00):**\n```\n1. 🧪 실험용 프로젝트 (AI 관련)\n2. 📋 Config 파일들 정리\n3. 💻 개인 프로젝트 (블로그, 강의)\n```\n\n### 월간 워크스페이스 사용량 분석\n\n```\n📊 Cursor 인스턴스 사용 빈도 (지난 한달):\n\n1. 🔴 Production Code (60%) - 가장 많이 사용\n2. 📝 Blog/Documentation (20%) - 꾸준히 사용  \n3. 🟡 Build Server (15%) - 필요할 때만\n4. 🧪 실험용 (5%) - 주말이나 저녁에\n\n💡 인사이트:\n- Remote SSH가 75%, Native가 25% 비율\n- Production 관련 작업이 대부분\n- 문서화 작업도 상당한 비중\n```\n\n## 🔚 마무리하며...\n\n이렇게 **5~8개의 워크스페이스를 동시에 관리**하는 건 처음에는 정말 복잡했는데, 지금은 이게 없으면 일이 안 될 정도로 익숙해졌어요.\n\n핵심은 **각 워크스페이스의 독립성을 유지**하면서도, **빠른 전환**이 가능하도록 체계를 갖추는 것입니다.\n\n다음 4편에서는 **도구별 프롬프트 전략**을 다뤄보겠습니다. Cursor Pro 고급 활용법, Claude Code 3시간 제한 극복법, Gemini CLI 문서화 자동화 등 실전 노하우를 공유할 예정입니다!\n\n여러분은 워크스페이스를 어떻게 관리하고 계신가요? Remote vs Native 어떤 비율로 사용하시는지도 궁금해요!\n\n---\n\n*이 시리즈의 다른 글들:*\n- *[1편: AI 스택 소개와 전체 개요](./2025-07-24-ai-workflow-productivity.md)*\n- *[2편: 공통 워크플로우 방법론](./2025-07-25-ai-workflow-common-methods.md)*\n- *4편: 도구별 프롬프트 전략 (곧 공개)*","categories":["Tech","AI"],"tags":["AI","Productivity","Workspace","Cursor","SSH","DevOps"],"pubDate":"2025-07-26T00:00:00.000Z","url":"/blog/2025-07-26-ai-workspace-management/"},{"id":"building-scalable-microservices-with-kubernetes","title":"실전 마이크로서비스 아키텍처: Kubernetes로 확장 가능한 시스템 구축하기","description":"자율주행 차량 플랫폼 개발 경험을 바탕으로 한 실용적인 마이크로서비스 설계와 운영 가이드","content":"# 마이크로서비스 여행기: 삽질과 깨달음의 1년\n\n안녕하세요! 오늘은 지난 1년간 제가 겪었던 마이크로서비스 도입 여정을 솔직하게 풀어보려고 해요. 성공담보다는 실패담이 더 많을 수도 있는데, 그래도 누군가에게는 도움이 될 것 같아서 용기를 내어 써봅니다.\n\n## 시작은 이랬어요: \"뭔가 한계에 부딪혔다\"\n\n2년 전쯤 우리 팀이 만든 차량 데이터 플랫폼이 점점 커지면서 정말 답답한 상황들이 생기기 시작했어요:\n\n- **배포 한 번 하는데 30분**: 커피 마시고 와도 아직 배포 중이더라고요 😅\n- **한 부분 오류나면 전체 다운**: 새벽에 장애 전화 받는 게 일상이 되었어요\n- **다른 팀과 일정 맞추기**: \"저희 팀 개발 끝나면 배포해주세요\"라는 말을 너무 자주 들었죠\n\n처음엔 \"이 정도면 괜찮지 않나?\"라고 생각했는데, 팀이 커지고 서비스가 복잡해지면서 한계를 확실히 느끼게 되었어요. 그래서 마이크로서비스로 전환하기로 결정했습니다.\n\n## 첫 번째 큰 실수: 잘못된 서비스 분할\n\n### 기술적으로 나누면 안 된다는걸 뒤늦게 깨달았어요\n\n마이크로서비스 도입을 결정하고 가장 먼저 한 일이 \"어떻게 서비스를 나눌까?\"였는데, 처음엔 정말 단순하게 생각했어요:\n\n**처음에 이렇게 나눴어요** (지금 생각해보면 정말 잘못된 방법):\n```\n❌ database-service (DB 관련 모든 것)\n❌ api-service (API 관련 모든 것)\n❌ auth-service (인증 관련)  \n❌ notification-service (알림 관련)\n```\n\n몇 개월 써보니까 정말 답답하더라고요. 하나의 기능을 추가하려면 여러 서비스를 동시에 수정해야 하고, 결국 배포도 함께 해야 하는 상황이 반복되었어요.\n\n**그래서 다시 이렇게 바꿨어요** (비즈니스 중심으로):\n```\n✅ vehicle-management (차량 관리의 모든 것)\n✅ trip-analytics (여행 분석 관련)\n✅ user-profiles (사용자 프로필)\n✅ billing-payments (결제 관련)\n```\n\n이렇게 바꾸고 나니까 각 팀이 독립적으로 개발할 수 있게 되었어요. 정말 큰 차이였습니다!\n\n#### 실제 도메인 분할 사례\n\n```typescript\n// 자율주행 플랫폼의 도메인 분할\ninterface DomainBoundaries {\n  vehicleFleet: {\n    responsibilities: ['차량 등록', '상태 모니터링', '펌웨어 관리'];\n    dataOwnership: ['vehicles', 'sensors', 'diagnostics'];\n    apis: ['/vehicles', '/fleet/status', '/diagnostics'];\n  };\n  \n  tripManagement: {\n    responsibilities: ['여행 생성', '경로 최적화', '실시간 추적'];\n    dataOwnership: ['trips', 'routes', 'locations'];\n    apis: ['/trips', '/routes', '/tracking'];\n  };\n  \n  userExperience: {\n    responsibilities: ['사용자 인터페이스', '알림', '피드백'];\n    dataOwnership: ['users', 'preferences', 'feedback'];\n    apis: ['/users', '/notifications', '/feedback'];\n  };\n}\n```\n\n### 📊 데이터 일관성 전략\n\n#### 이벤트 소싱 패턴 구현\n\n```typescript\n// 이벤트 기반 데이터 동기화\ninterface DomainEvent {\n  eventId: string;\n  aggregateId: string;\n  eventType: string;\n  timestamp: Date;\n  version: number;\n  data: any;\n}\n\nclass VehicleEventHandler {\n  async handleTripCompleted(event: DomainEvent) {\n    const { tripId, vehicleId, mileage, fuelConsumption } = event.data;\n    \n    // 1. Vehicle Service: 차량 상태 업데이트\n    await this.vehicleService.updateMileage(vehicleId, mileage);\n    \n    // 2. Analytics Service: 주행 데이터 저장\n    await this.analyticsService.recordTripData({\n      tripId, vehicleId, mileage, fuelConsumption\n    });\n    \n    // 3. Billing Service: 요금 계산 이벤트 발행  \n    await this.eventPublisher.publish('billing.calculate', {\n      tripId, mileage, userId: event.data.userId\n    });\n  }\n}\n```\n\n#### SAGA 패턴으로 분산 트랜잭션 관리\n\n```yaml\n# trip-booking-saga.yml\nsaga:\n  name: \"TripBookingSaga\"\n  steps:\n    - service: \"user-service\"\n      action: \"reserve-credits\"\n      compensate: \"release-credits\"\n      \n    - service: \"vehicle-service\"  \n      action: \"reserve-vehicle\"\n      compensate: \"release-vehicle\"\n      \n    - service: \"trip-service\"\n      action: \"create-trip\"\n      compensate: \"cancel-trip\"\n      \n    - service: \"notification-service\"\n      action: \"send-confirmation\"\n      compensate: \"send-cancellation\"\n```\n\n## Kubernetes 클러스터 구성\n\n### 🏗️ 인프라 아키�ecture\n\n```yaml\n# cluster-architecture.yml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-config\ndata:\n  # 프로덕션 클러스터 구성\n  nodes: |\n    master-nodes: 3 (HA 구성)\n    worker-nodes: 12 (auto-scaling)\n    \n  resources:\n    cpu: \"48 cores per node\"\n    memory: \"192GB per node\"\n    storage: \"2TB NVMe SSD\"\n    \n  networking:\n    cni: \"Calico\"\n    service-mesh: \"Istio\"\n    ingress: \"NGINX + Cert-Manager\"\n```\n\n### 📦 서비스별 배포 설정\n\n#### 1. 고가용성 서비스 (Vehicle Management)\n\n```yaml\n# vehicle-service-deployment.yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vehicle-service\n  labels:\n    app: vehicle-service\n    version: v2.1.3\nspec:\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: vehicle-service\n  template:\n    metadata:\n      labels:\n        app: vehicle-service\n        version: v2.1.3\n    spec:\n      containers:\n      - name: vehicle-service\n        image: myregistry/vehicle-service:v2.1.3\n        ports:\n        - containerPort: 8080\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n        - name: REDIS_URL\n          valueFrom:\n            configMapKeyRef:\n              name: cache-config\n              key: redis-url\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vehicle-service\nspec:\n  selector:\n    app: vehicle-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: ClusterIP\n```\n\n#### 2. HPA (Horizontal Pod Autoscaler) 설정\n\n```yaml\n# vehicle-service-hpa.yml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vehicle-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vehicle-service\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"100\"\n```\n\n### 🔄 CI/CD 파이프라인\n\n#### GitOps 기반 배포 자동화\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy to Kubernetes\non:\n  push:\n    branches: [main]\n    \njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Run Tests\n      run: |\n        npm test\n        npm run test:integration\n        \n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Security Scan\n      run: |\n        docker run --rm -v $(pwd):/app \\\n          aquasec/trivy fs /app\n          \n  deploy:\n    needs: [test, security-scan]\n    runs-on: ubuntu-latest\n    steps:\n    - name: Deploy to Staging\n      run: |\n        kubectl apply -f k8s/staging/\n        kubectl wait --for=condition=ready pod \\\n          -l app=vehicle-service --timeout=300s\n          \n    - name: Run E2E Tests\n      run: |\n        npm run test:e2e:staging\n        \n    - name: Deploy to Production\n      if: success()\n      run: |\n        kubectl apply -f k8s/production/\n        kubectl rollout status deployment/vehicle-service\n```\n\n## 서비스 메시(Service Mesh) 구현\n\n### 🕸️ Istio로 트래픽 관리\n\n#### 카나리 배포 설정\n\n```yaml\n# canary-deployment.yml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: vehicle-service-canary\nspec:\n  hosts:\n  - vehicle-service\n  http:\n  - match:\n    - headers:\n        canary:\n          exact: \"true\"\n    route:\n    - destination:\n        host: vehicle-service\n        subset: v2\n  - route:\n    - destination:\n        host: vehicle-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: vehicle-service\n        subset: v2\n      weight: 10\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: vehicle-service-destination\nspec:\n  host: vehicle-service\n  subsets:\n  - name: v1\n    labels:\n      version: v2.1.2\n  - name: v2\n    labels:\n      version: v2.1.3\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_CONN\n```\n\n#### 회로 차단기(Circuit Breaker) 패턴\n\n```yaml\n# circuit-breaker.yml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: trip-service-circuit-breaker\nspec:\n  host: trip-service\n  trafficPolicy:\n    outlierDetection:\n      consecutiveErrors: 3\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests: 50\n        maxRequestsPerConnection: 10\n        maxRetries: 3\n        connectTimeout: 30s\n```\n\n## 모니터링과 관찰성(Observability)\n\n### 📊 통합 모니터링 스택\n\n```yaml\n# monitoring-stack.yml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: monitoring-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n    scrape_configs:\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n        \n  grafana-dashboard.json: |\n    {\n      \"dashboard\": {\n        \"title\": \"Microservices Overview\",\n        \"panels\": [\n          {\n            \"title\": \"Request Rate\",\n            \"type\": \"graph\",\n            \"targets\": [\n              {\n                \"expr\": \"rate(http_requests_total[5m])\",\n                \"legendFormat\": \"{{service}}\"\n              }\n            ]\n          },\n          {\n            \"title\": \"Error Rate\",\n            \"type\": \"singlestat\",\n            \"targets\": [\n              {\n                \"expr\": \"rate(http_requests_total{status=~'5..'}[5m]) / rate(http_requests_total[5m])\",\n                \"legendFormat\": \"Error Rate %\"\n              }\n            ]\n          }\n        ]\n      }\n    }\n```\n\n### 🔍 분산 추적(Distributed Tracing)\n\n```typescript\n// OpenTelemetry 계측 예제\nimport { trace, SpanStatusCode } from '@opentelemetry/api';\nimport { NodeSDK } from '@opentelemetry/auto-instrumentations-node';\n\nclass TripService {\n  private tracer = trace.getTracer('trip-service');\n  \n  async createTrip(tripData: CreateTripRequest): Promise<Trip> {\n    const span = this.tracer.startSpan('create-trip');\n    \n    try {\n      span.setAttributes({\n        'trip.user_id': tripData.userId,\n        'trip.vehicle_id': tripData.vehicleId,\n        'trip.route_length': tripData.routeLength\n      });\n      \n      // 1. 사용자 크레딧 확인\n      const creditSpan = this.tracer.startSpan('check-user-credits', { parent: span });\n      const hasCredits = await this.userService.checkCredits(tripData.userId);\n      creditSpan.end();\n      \n      if (!hasCredits) {\n        span.recordException(new Error('Insufficient credits'));\n        span.setStatus({ code: SpanStatusCode.ERROR });\n        throw new InsufficientCreditsError();\n      }\n      \n      // 2. 차량 예약\n      const vehicleSpan = this.tracer.startSpan('reserve-vehicle', { parent: span });\n      await this.vehicleService.reserveVehicle(tripData.vehicleId);\n      vehicleSpan.end();\n      \n      // 3. 여행 생성\n      const trip = await this.tripRepository.create(tripData);\n      \n      span.setAttributes({\n        'trip.id': trip.id,\n        'trip.status': trip.status\n      });\n      \n      span.setStatus({ code: SpanStatusCode.OK });\n      return trip;\n      \n    } catch (error) {\n      span.recordException(error);\n      span.setStatus({ code: SpanStatusCode.ERROR });\n      throw error;\n    } finally {\n      span.end();\n    }\n  }\n}\n```\n\n## 성능 최적화 전략\n\n### ⚡ 캐싱 전략\n\n#### 다계층 캐싱 구현\n\n```typescript\n// 스마트 캐싱 시스템\ninterface CacheStrategy {\n  l1: 'in-memory'; // 애플리케이션 레벨 (100ms TTL)\n  l2: 'redis';     // 클러스터 레벨 (5min TTL) \n  l3: 'database';  // 영구 저장소\n}\n\nclass VehicleLocationService {\n  private memoryCache = new Map();\n  private redisClient: Redis;\n  \n  async getVehicleLocation(vehicleId: string): Promise<Location> {\n    // L1: 메모리 캐시 확인\n    const memCached = this.memoryCache.get(vehicleId);\n    if (memCached && this.isValid(memCached, 100)) {\n      return memCached.data;\n    }\n    \n    // L2: Redis 캐시 확인  \n    const redisCached = await this.redisClient.get(`location:${vehicleId}`);\n    if (redisCached) {\n      const data = JSON.parse(redisCached);\n      this.memoryCache.set(vehicleId, { data, timestamp: Date.now() });\n      return data;\n    }\n    \n    // L3: 데이터베이스 조회\n    const location = await this.locationRepository.findByVehicleId(vehicleId);\n    \n    // 캐시에 저장 (상위 레벨로)\n    await this.redisClient.setex(`location:${vehicleId}`, 300, JSON.stringify(location));\n    this.memoryCache.set(vehicleId, { data: location, timestamp: Date.now() });\n    \n    return location;\n  }\n}\n```\n\n#### 데이터베이스 최적화\n\n```sql\n-- 인덱스 최적화 예제\n-- 복합 인덱스로 조회 성능 개선\nCREATE INDEX CONCURRENTLY idx_trips_user_status_date \nON trips(user_id, status, created_at DESC);\n\n-- 파티셔닝으로 대용량 데이터 관리\nCREATE TABLE trips_2024_q1 PARTITION OF trips \nFOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\n-- 읽기 전용 복제본 활용\n-- application.yml\nspring:\n  datasource:\n    master:\n      url: jdbc:postgresql://master-db:5432/trips\n    slave:\n      url: jdbc:postgresql://readonly-db:5432/trips\n```\n\n### 📈 오토스케일링 최적화\n\n#### 사용자 정의 메트릭 기반 스케일링\n\n```yaml\n# custom-metrics-hpa.yml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vehicle-service-custom-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vehicle-service\n  minReplicas: 3\n  maxReplicas: 50\n  metrics:\n  # 큐 길이 기반 스케일링\n  - type: External\n    external:\n      metric:\n        name: rabbitmq_queue_messages\n        selector:\n          matchLabels:\n            queue: \"vehicle-updates\"\n      target:\n        type: AverageValue\n        averageValue: \"30\"\n  # 응답 시간 기반 스케일링        \n  - type: Pods\n    pods:\n      metric:\n        name: http_request_duration_95percentile\n      target:\n        type: AverageValue\n        averageValue: \"500m\"  # 500ms\n```\n\n## 장애 대응과 복구 전략\n\n### 🚨 장애 시나리오별 대응\n\n#### 1. 서비스 간 통신 장애\n\n```typescript\n// Retry 정책과 Circuit Breaker\nimport { CircuitBreaker } from 'opossum';\n\nclass VehicleApiClient {\n  private circuitBreaker: CircuitBreaker;\n  \n  constructor() {\n    const options = {\n      timeout: 3000,\n      errorThresholdPercentage: 50,\n      resetTimeout: 30000,\n      rollingCountTimeout: 10000,\n      rollingCountBuckets: 10\n    };\n    \n    this.circuitBreaker = new CircuitBreaker(this.callVehicleAPI.bind(this), options);\n    \n    // 폴백 전략\n    this.circuitBreaker.fallback(() => {\n      return this.getCachedVehicleData() || this.getDefaultVehicleData();\n    });\n  }\n  \n  async getVehicleData(vehicleId: string): Promise<VehicleData> {\n    try {\n      return await this.circuitBreaker.fire(vehicleId);\n    } catch (error) {\n      // 로깅 및 메트릭 수집\n      this.logger.error('Vehicle API call failed', { vehicleId, error });\n      this.metrics.increment('vehicle_api_failure');\n      throw error;\n    }\n  }\n}\n```\n\n#### 2. 데이터베이스 장애\n\n```yaml\n# postgresql-ha.yml - 고가용성 DB 설정\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgres-cluster\nspec:\n  instances: 3\n  \n  postgresql:\n    parameters:\n      max_connections: \"200\"\n      shared_buffers: \"256MB\"\n      effective_cache_size: \"1GB\"\n      \n  bootstrap:\n    initdb:\n      database: trips_db\n      secret:\n        name: postgres-credentials\n        \n  backup:\n    retention: \"30d\"\n    barmanObjectStore:\n      s3Credentials:\n        accessKeyId:\n          name: s3-credentials\n          key: ACCESS_KEY_ID\n        secretAccessKey:\n          name: s3-credentials\n          key: SECRET_ACCESS_KEY\n      wal:\n        retention: \"5d\"\n        \n  monitoring:\n    enabled: true\n```\n\n### 🔄 무중단 배포 전략\n\n#### Blue-Green 배포\n\n```bash\n#!/bin/bash\n# blue-green-deploy.sh\n\n# 현재 활성 환경 확인\nCURRENT=$(kubectl get service vehicle-service -o jsonpath='{.spec.selector.version}')\nNEW_VERSION=\"v2.1.4\"\n\nif [ \"$CURRENT\" = \"blue\" ]; then\n    NEW_ENV=\"green\"\n    OLD_ENV=\"blue\"  \nelse\n    NEW_ENV=\"blue\"\n    OLD_ENV=\"green\"\nfi\n\necho \"Deploying $NEW_VERSION to $NEW_ENV environment...\"\n\n# 1. 새 버전을 비활성 환경에 배포\nkubectl set image deployment/vehicle-service-$NEW_ENV \\\n  vehicle-service=myregistry/vehicle-service:$NEW_VERSION\n\n# 2. 배포 완료 대기\nkubectl rollout status deployment/vehicle-service-$NEW_ENV --timeout=600s\n\n# 3. 헬스 체크\nkubectl wait --for=condition=ready pod \\\n  -l app=vehicle-service,version=$NEW_ENV --timeout=300s\n\n# 4. 스모크 테스트\ncurl -f http://vehicle-service-$NEW_ENV/health || exit 1\n\n# 5. 트래픽 전환\nkubectl patch service vehicle-service \\\n  -p '{\"spec\":{\"selector\":{\"version\":\"'$NEW_ENV'\"}}}'\n\necho \"Traffic switched to $NEW_ENV environment\"\n\n# 6. 5분 후 이전 환경 정리 (선택적)\nsleep 300\nkubectl scale deployment vehicle-service-$OLD_ENV --replicas=0\n```\n\n## 운영 경험에서 배운 교훈\n\n### 💡 성공 요인들\n\n#### 1. 팀 구조와 조직 정렬\n\n```mermaid\ngraph TD\n    A[Product Team] --> B[Domain Team 1: Vehicle]\n    A --> C[Domain Team 2: Trip]\n    A --> D[Domain Team 3: User]\n    \n    B --> E[Backend Developer]\n    B --> F[DevOps Engineer]  \n    B --> G[QA Engineer]\n    \n    H[Platform Team] --> I[Infrastructure]\n    H --> J[Monitoring]\n    H --> K[Security]\n```\n\n**Conway's Law 활용**: 조직 구조가 아키텍처를 결정한다는 것을 인정하고 의도적으로 설계\n\n#### 2. 단계적 마이그레이션\n\n```typescript\n// Strangler Fig 패턴으로 점진적 전환\nclass LegacyTripService {\n  async createTrip(tripData: TripData): Promise<Trip> {\n    // Feature Flag로 새/구 시스템 분기\n    if (this.featureFlag.isEnabled('NEW_TRIP_SERVICE', tripData.userId)) {\n      return this.newTripService.createTrip(tripData);\n    } else {\n      return this.legacyCreateTrip(tripData);\n    }\n  }\n  \n  private async legacyCreateTrip(tripData: TripData): Promise<Trip> {\n    // 기존 모놀리식 로직\n  }\n}\n```\n\n#### 3. 관찰성 우선 개발\n\n```typescript\n// 메트릭 수집을 코드 작성 시점부터 고려\nclass PaymentService {\n  async processPayment(payment: Payment): Promise<PaymentResult> {\n    const timer = this.metrics.startTimer('payment_processing_duration');\n    \n    try {\n      this.metrics.increment('payment_attempts_total', {\n        payment_method: payment.method,\n        amount_range: this.getAmountRange(payment.amount)\n      });\n      \n      const result = await this.paymentGateway.charge(payment);\n      \n      this.metrics.increment('payment_success_total');\n      return result;\n      \n    } catch (error) {\n      this.metrics.increment('payment_failure_total', {\n        error_type: error.constructor.name\n      });\n      throw error;\n    } finally {\n      timer.end();\n    }\n  }\n}\n```\n\n### 🚨 실패에서 배운 교훈\n\n#### 1. 너무 작은 서비스의 함정\n\n**문제**: 과도한 네트워크 호출과 복잡한 오케스트레이션\n\n```typescript\n// ❌ 너무 세분화된 서비스\ninterface MicroServices {\n  userIdService: 'generates user IDs';\n  userNameService: 'manages user names'; \n  userEmailService: 'handles user emails';\n  userPhoneService: 'manages phone numbers';\n}\n\n// ✅ 적절한 크기의 서비스\ninterface RightSizedServices {\n  userManagementService: 'complete user lifecycle';\n  authenticationService: 'login, logout, tokens';\n  profileService: 'user preferences, settings';\n}\n```\n\n#### 2. 분산 모놀리스의 위험\n\n분산되어 있지만 여전히 강하게 결합된 시스템:\n\n- 서비스 간 동기 호출 체인\n- 공유 데이터베이스\n- 동시 배포 필요성\n\n**해결책**: 이벤트 기반 아키텍처와 CQRS 패턴 도입\n\n#### 3. 테스트 전략의 중요성\n\n```typescript\n// 컨트랙트 테스트로 서비스 간 호환성 보장\ndescribe('Vehicle Service Contract', () => {\n  it('should return vehicle data in expected format', async () => {\n    const pact = new Pact({\n      consumer: 'trip-service',\n      provider: 'vehicle-service'\n    });\n    \n    await pact\n      .given('vehicle exists')\n      .uponReceiving('a request for vehicle data')\n      .withRequest({\n        method: 'GET',\n        path: '/vehicles/123'\n      })\n      .willRespondWith({\n        status: 200,\n        headers: { 'Content-Type': 'application/json' },\n        body: {\n          id: like('123'),\n          status: like('available'),\n          location: {\n            lat: like(37.5665),\n            lng: like(126.9780)\n          }\n        }\n      });\n      \n    const vehicle = await vehicleClient.getVehicle('123');\n    expect(vehicle).toHaveProperty('id');\n    expect(vehicle).toHaveProperty('status');\n  });\n});\n```\n\n## 성과 측정과 지속적 개선\n\n### 📊 핵심 지표 (KPIs)\n\n#### 기술적 지표\n\n| 지표 | 목표 | 현재 | 개선도 |\n|------|------|------|--------|\n| 배포 빈도 | 일 5회 | 일 8회 | ✅ +60% |\n| 배포 리드타임 | 30분 | 8분 | ✅ -73% |\n| MTTR (평균 복구 시간) | 1시간 | 15분 | ✅ -75% |\n| 변경 실패율 | <5% | 2.3% | ✅ -54% |\n\n#### 비즈니스 지표\n\n- **서비스 가용성**: 99.97% → 99.99%\n- **응답 시간**: P95 500ms → 150ms\n- **동시 사용자**: 1만명 → 10만명 처리 가능\n\n### 🔄 지속적 개선 프로세스\n\n```yaml\n# 월간 회고 프로세스\nretrospective:\n  what_went_well:\n    - \"카나리 배포로 장애 최소화\"\n    - \"모니터링 대시보드로 빠른 문제 파악\"\n    \n  what_needs_improvement:\n    - \"크로스팀 의존성 해결\"\n    - \"테스트 자동화 범위 확대\"\n    \n  action_items:\n    - name: \"이벤트 기반 통신 확대\"\n      owner: \"architecture-team\"\n      due_date: \"2025-02-28\"\n    - name: \"E2E 테스트 커버리지 80% 달성\"  \n      owner: \"qa-team\"\n      due_date: \"2025-02-15\"\n```\n\n## 앞으로의 계획\n\n### 🚀 다음 단계 로드맵\n\n#### Q1 2025: Platform Engineering 강화\n- **Developer Portal** 구축 (Backstage 기반)\n- **셀프서비스 인프라** 도입\n- **개발자 생산성 메트릭** 수집\n\n#### Q2 2025: AI/ML 통합\n- **예측적 오토스케일링** (머신러닝 기반)\n- **이상 탐지** 자동화\n- **성능 최적화** AI 어시스턴트\n\n#### Q3-Q4 2025: 글로벌 확장\n- **멀티 리전** 아키텍처\n- **지리적 데이터 분산**\n- **엣지 컴퓨팅** 도입\n\n## 마무리: 실무 적용 가이드\n\n### ✅ 단계별 체크리스트\n\n#### 설계 단계\n- [ ] 도메인 경계 명확히 정의\n- [ ] 데이터 소유권 분할\n- [ ] 통신 패턴 결정 (동기/비동기)\n- [ ] 장애 시나리오 계획\n\n#### 개발 단계  \n- [ ] API 버전 관리 전략\n- [ ] 로깅/메트릭 표준화\n- [ ] 컨트랙트 테스트 작성\n- [ ] 보안 정책 적용\n\n#### 배포 단계\n- [ ] CI/CD 파이프라인 구축\n- [ ] 카나리/블루-그린 배포\n- [ ] 모니터링 대시보드 설정\n- [ ] 알림 규칙 정의\n\n#### 운영 단계\n- [ ] SLA/SLO 정의\n- [ ] 장애 대응 플레이북\n- [ ] 정기적 재해 복구 훈련\n- [ ] 성능 튜닝 및 최적화\n\n### 정말로 중요한 것들 (제가 깨달은 것들)\n\n1년 동안 삽질하면서 깨달은 가장 중요한 것들을 정리해봤어요:\n\n1. **한 번에 다 바꾸려 하지 마세요**: 저희도 처음엔 욕심을 부렸다가 정말 고생했어요. 하나씩 천천히 바꿔가는 게 훨씬 안전해요\n2. **모니터링부터 하세요**: 뭔가 문제가 생겼을 때 원인을 찾을 수 없으면 정말 막막해져요. 로그와 메트릭 수집은 필수입니다\n3. **팀 구조도 같이 바뀌어야 해요**: 조직 구조와 시스템 구조가 닮아간다는 Conway's Law가 정말 맞더라고요\n4. **자동화에 투자하세요**: 서비스가 많아지면 수동으로는 절대 관리할 수 없어요\n\n### 솔직한 고백\n\n마이크로서비스가 모든 문제를 해결해주는 마법의 해답은 아니에요. 오히려 새로운 복잡성을 가져다주기도 하고, 때로는 모놀리식이 더 나은 선택일 수도 있어요.\n\n하지만 제대로만 도입하면 정말 개발팀의 생산성과 시스템 안정성이 크게 올라갈 수 있어요. 저희 팀도 힘들었지만 지금은 정말 만족하고 있거든요.\n\n혹시 마이크로서비스 도입을 고민하고 계신다면, 언제든지 질문 주세요. 제가 겪었던 삽질들을 미리 알려드릴 수 있을 것 같아요! 😊\n\n---\n\n**다음에는 이런 이야기들을 나눠볼게요:**\n- Kubernetes 운영하면서 배운 성능 최적화 팁들 (다음주 계획)\n- 이벤트 기반 아키텍처, 정말 필요한 건지에 대한 고민 (2월쯤)\n\n**더 이야기하고 싶으시면:**\n- LinkedIn에서 연락주세요: [linkedin.com/in/jaylee](https://linkedin.com/in/jaylee)\n- GitHub에서 만나요: [github.com/jayleekr](https://github.com/jayleekr)","categories":["Tech","Architecture"],"tags":["microservices","kubernetes","architecture","devops","scalability","docker"],"pubDate":"2025-01-18T00:00:00.000Z","url":"/blog/building-scalable-microservices-with-kubernetes/"},{"id":"2025-tech-trends-for-developers","title":"2025년 개발자가 주목해야 할 기술 트렌드","description":"소프트웨어 엔지니어 관점에서 분석한 2025년 핵심 기술 트렌드와 실무 적용 전략","content":"# 2025년, 우리가 함께 주목해야 할 기술 이야기\n\n안녕하세요! 새해가 시작되면서 많은 분들이 \"올해는 어떤 기술을 배워야 할까?\"라는 고민을 하고 계실 것 같아요. 저도 매년 이맘때면 같은 고민을 하게 되는데, 올해는 특히 기술 변화의 속도가 빨라서 더욱 신중해지고 있습니다.\n\n지난 한 해 동안 자율주행 프로젝트를 진행하면서 정말 많은 변화를 직접 경험했어요. AI 도구들이 단순한 코드 완성을 넘어서 진짜 '개발 파트너'가 되어가는 모습을 보면서, \"아, 이제 정말 새로운 시대가 왔구나\"라는 생각이 들었습니다.\n\n오늘은 제가 현장에서 느낀 변화들과 함께, 2025년에 우리가 주목해야 할 기술 트렌드들을 정리해보려고 해요. 너무 딱딱하지 않게, 실무에서 바로 써먹을 수 있는 이야기들로 풀어보겠습니다.\n\n## 이런 이야기들을 나눠볼게요\n- 🤖 AI가 정말로 우리 개발을 어떻게 바꾸고 있는지\n- ☁️ 클라우드가 이제 선택이 아닌 필수가 된 이유  \n- 🌐 웹이 앱과 진짜로 경계가 사라지고 있다는 이야기\n- 🔒 보안이 왜 이렇게 중요해졌는지\n- ⚡ 개발자 경험이 생산성에 미치는 진짜 영향\n- 📋 실제로 어떻게 학습 계획을 세우면 좋을지\n\n## 🤖 AI, 이제 정말 내 개발 파트너가 되었어요\n\n### 코드 짜주는 도구에서 함께 고민하는 동료로\n\n솔직히 말하면, 작년 초만 해도 \"AI가 코드를 써준다고? 그게 정말 쓸만할까?\" 싶었어요. 하지만 지금은 정말 다릅니다. GitHub Copilot 없이 개발하는 상상을 할 수 없을 정도가 되었거든요.\n\n가장 놀라웠던 건, AI가 단순히 반복적인 코드만 생성하는 게 아니라는 점이에요. 복잡한 시스템 설계에 대해서도 꽤 괜찮은 제안을 해주더라고요.\n\n#### 최근에 정말 도움이 되었던 경험\n\n얼마 전에 마이크로서비스 아키텍처를 설계하면서 이벤트 소싱 패턴을 적용해야 했어요. 혼자 고민하다가 AI에게 \"이런 요구사항이 있는데 어떻게 설계하면 좋을까?\"라고 물어봤는데, 정말 깔끔한 구조를 제안해주더라고요:\n\n```typescript\n// AI가 제안한 이벤트 소싱 패턴\ninterface DomainEvent {\n  eventId: string;\n  aggregateId: string;\n  eventType: string;\n  timestamp: Date;\n  version: number;\n  data: any;\n}\n\nclass EventStore {\n  async saveEvents(streamId: string, events: DomainEvent[], expectedVersion: number): Promise<void> {\n    // AI가 제안한 낙관적 동시성 제어 로직\n    const currentVersion = await this.getStreamVersion(streamId);\n    if (currentVersion !== expectedVersion) {\n      throw new ConcurrencyError('Stream version mismatch');\n    }\n    \n    await this.persistEvents(streamId, events);\n  }\n}\n```\n\n물론 아직 완벽하지는 않아요. 가끔 엉뚱한 코드를 제안하기도 하고, 보안 이슈를 놓치는 경우도 있어서 여전히 꼼꼼히 리뷰해야 합니다. 하지만 전체적으로는 정말 개발 생산성이 많이 올라갔어요.\n\n#### 올해는 이런 변화가 더 있을 것 같아요\n- **코드 리뷰도 AI가**: 이제 PR 올리면 AI가 먼저 한 번 체크해주는 시대가 올 것 같아요\n- **테스트도 자동으로**: 내가 놓친 엣지 케이스까지 찾아서 테스트를 만들어주는 도구들이 나오고 있어요\n- **문서화는 당연히**: 코드 바뀔 때마다 문서도 같이 업데이트되는 게 일반화될 듯해요\n\n### 지금부터 어떻게 준비하면 좋을까요?\n\n정말 솔직하게 말씀드리면, 이제 AI 도구 사용법을 모르면 뒤처질 수밖에 없을 것 같아요. 하지만 부담 갖지 마세요! 단계별로 천천히 접근하면 됩니다:\n\n1. **우선**: GitHub Copilot이나 Cursor 같은 도구를 일상에서 써보세요\n2. **그 다음**: \"어떻게 질문하면 더 좋은 답을 얻을까?\" 고민해보세요 (프롬프트 엔지니어링이라고 하죠)\n3. **나중에**: 팀에서 AI를 활용한 코드 리뷰 프로세스를 만들어보세요\n\n## ☁️ 클라우드, 이제 정말 피할 수 없는 선택이 되었어요\n\n### 서버실에서 엣지까지, 경계가 사라지고 있어요\n\n요즘 개발하다 보면 정말 느끼는 게, \"온프레미스로 뭔가 하기가 점점 어려워진다\"는 거예요. 특히 글로벌 서비스를 생각하면 더욱 그렇고요.\n\n최근에 실시간 데이터 처리 시스템을 만들면서 정말 실감했어요. 전 세계 사용자들이 빠른 응답을 원하는데, 기존 CDN만으로는 한계가 있더라고요. 그래서 엣지 컴퓨팅을 도입했는데, 결과가 정말 놀라웠어요.\n\n#### 실제로 적용해본 구조는 이런 식이었어요\n\n```yaml\n# Kubernetes 엣지 배포 설정\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edge-processor\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: edge-processor\n  template:\n    metadata:\n      labels:\n        app: edge-processor\n    spec:\n      containers:\n      - name: processor\n        image: myapp/edge-processor:v1.2.3\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n        env:\n        - name: REGION\n          value: \"asia-northeast1\"\n```\n\n#### 주목할 기술들\n\n1. **WebAssembly (WASM)**: 엣지에서의 고성능 코드 실행\n2. **Kubernetes at Edge**: K3s, MicroK8s를 활용한 경량 컨테이너 관리  \n3. **eBPF**: 커널 레벨 프로그래밍으로 네트워크/보안 최적화\n\n### 📊 성능 비교 결과\n\n| 배포 방식 | 응답 시간 | 비용 (월) | 확장성 |\n|-----------|-----------|-----------|--------|\n| 전통적 서버 | 200ms | $500 | 수동 |\n| 서버리스 | 150ms | $300 | 자동 |\n| 엣지 컴퓨팅 | 50ms | $400 | 지능형 |\n\n## 웹 플랫폼의 진화\n\n### 🌐 Web Platform API의 새로운 가능성\n\n2025년 웹 플랫폼은 네이티브 앱과의 경계가 더욱 모호해집니다. **PWA 2.0**과 **WebGPU** 등 새로운 Web API들이 웹 애플리케이션의 가능성을 크게 확장하고 있습니다.\n\n#### 주목할 새로운 API들\n\n```javascript\n// WebGPU를 활용한 고성능 연산\nconst adapter = await navigator.gpu.requestAdapter();\nconst device = await adapter.requestDevice();\n\nconst computeShader = device.createShaderModule({\n  code: `\n    @compute @workgroup_size(64)\n    fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n      // GPU에서 실행되는 병렬 연산\n      let index = global_id.x;\n      output[index] = input[index] * 2.0;\n    }\n  `\n});\n\n// File System Access API로 로컬 파일 직접 조작\nconst fileHandle = await window.showSaveFilePicker();\nconst writable = await fileHandle.createWritable();\nawait writable.write(data);\nawait writable.close();\n```\n\n#### 실무 적용 예시\n최근 웹 기반 데이터 시각화 도구를 개발하면서 WebGPU를 활용해 **10배 빠른** 렌더링 성능을 달성했습니다:\n\n- **기존**: Canvas 2D API로 1만개 노드 렌더링 시 500ms\n- **개선**: WebGPU로 동일 작업 50ms 내 완료\n\n## 보안과 프라이버시 강화\n\n### 🔒 제로 트러스트 아키텍처 확산\n\n2025년 보안 패러다임은 **\"절대 신뢰하지 않고, 항상 검증하라\"**는 제로 트러스트 원칙이 핵심입니다. 특히 원격 근무 확산과 클라우드 마이그레이션으로 인해 전통적인 경계 보안 모델의 한계가 드러나고 있습니다.\n\n#### 실제 구현 사례\n\n```typescript\n// JWT 토큰 기반 제로 트러스트 인증\ninterface SecurityContext {\n  userId: string;\n  permissions: Permission[];\n  deviceFingerprint: string;\n  locationVerified: boolean;\n  mfaCompleted: boolean;\n}\n\nclass ZeroTrustMiddleware {\n  async validateRequest(req: Request): Promise<SecurityContext> {\n    // 1. 토큰 검증\n    const token = this.extractToken(req);\n    const payload = await this.verifyJWT(token);\n    \n    // 2. 디바이스 지문 검증\n    const deviceId = this.getDeviceFingerprint(req);\n    if (!await this.isKnownDevice(payload.userId, deviceId)) {\n      throw new SecurityError('Unknown device');\n    }\n    \n    // 3. 위치 기반 검증\n    const location = this.getClientLocation(req);\n    if (!await this.validateLocation(payload.userId, location)) {\n      await this.requestAdditionalAuth(payload.userId);\n    }\n    \n    return {\n      userId: payload.userId,\n      permissions: await this.getUserPermissions(payload.userId),\n      deviceFingerprint: deviceId,\n      locationVerified: true,\n      mfaCompleted: payload.mfa === true\n    };\n  }\n}\n```\n\n#### 개인정보보호 강화 기술\n\n1. **Differential Privacy**: 통계적 노이즈로 개인정보 보호\n2. **Homomorphic Encryption**: 암호화된 상태에서 연산 수행\n3. **Secure Multi-party Computation**: 데이터를 노출하지 않고 협력 연산\n\n## 개발자 경험(DX) 혁신\n\n### ⚡ 개발 생산성의 패러다임 전환\n\n2025년 개발자 도구는 **속도**보다는 **품질과 안정성**에 초점을 맞춥니다. 빠른 개발만큼이나 유지보수와 확장성이 중요해진 것입니다.\n\n#### 차세대 개발 환경\n\n```json\n// 새로운 개발 워크플로우 설정\n{\n  \"devContainer\": {\n    \"image\": \"mcr.microsoft.com/devcontainers/typescript-node:18\",\n    \"features\": {\n      \"ghcr.io/devcontainers/features/github-cli:1\": {},\n      \"ghcr.io/devcontainers/features/docker-outside-of-docker:1\": {}\n    },\n    \"postCreateCommand\": \"npm install && npm run setup-dev\",\n    \"customizations\": {\n      \"vscode\": {\n        \"extensions\": [\n          \"ms-vscode.vscode-typescript-next\",\n          \"bradlc.vscode-tailwindcss\",\n          \"esbenp.prettier-vscode\"\n        ]\n      }\n    }\n  }\n}\n```\n\n#### 주목할 도구들\n\n1. **Dev Containers**: 일관된 개발 환경 보장\n2. **Bun**: 올인원 JavaScript 런타임 및 패키지 매니저\n3. **Biome**: 빠른 린팅/포매팅 도구\n4. **Turborepo**: 모노레포 빌드 최적화\n\n## 실무 적용 전략\n\n### 🎯 단계별 학습 로드맵\n\n#### 1단계: 즉시 적용 (1-2개월)\n- [ ] AI 코딩 도구 일상화 (GitHub Copilot, Cursor)\n- [ ] 클라우드 서비스 기본 활용 (AWS Lambda, Vercel Edge)\n- [ ] 모던 개발 도구 도입 (Bun, Biome)\n\n#### 2단계: 기반 구축 (3-6개월)  \n- [ ] 컨테이너/쿠버네티스 실무 경험\n- [ ] 보안 모범 사례 적용\n- [ ] 성능 모니터링 시스템 구축\n\n#### 3단계: 전문성 심화 (6-12개월)\n- [ ] 엣지 컴퓨팅 아키텍처 설계\n- [ ] WebAssembly 활용 프로젝트\n- [ ] 제로 트러스트 보안 모델 구현\n\n### 🚨 피해야 할 함정들\n\n1. **기술 스택 과다**: 새 기술에 현혹되어 기존 시스템 복잡도 증가\n2. **성능 맹신**: 벤치마크 결과만 믿고 실제 사용 환경 무시  \n3. **보안 후순위**: 개발 속도 우선으로 보안 요소 미뤄두기\n\n### 📊 기술별 우선순위 매트릭스\n\n| 기술 영역 | 학습 난이도 | 즉시 활용성 | 장기 가치 | 추천도 |\n|-----------|------------|------------|----------|--------|\n| AI 코딩 도구 | 낮음 | 높음 | 높음 | ⭐⭐⭐⭐⭐ |\n| 클라우드 네이티브 | 중간 | 중간 | 높음 | ⭐⭐⭐⭐ |\n| WebGPU/WASM | 높음 | 낮음 | 높음 | ⭐⭐⭐ |\n| 제로 트러스트 | 높음 | 중간 | 높음 | ⭐⭐⭐⭐ |\n| 엣지 컴퓨팅 | 중간 | 중간 | 중간 | ⭐⭐⭐ |\n\n## 함께 성장해요, 2025년!\n\n### 제가 느낀 가장 중요한 것들\n\n이 모든 기술 변화를 지켜보면서 느낀 건, 결국 **사람**이 중심이라는 거예요:\n\n- **AI는 친구**: 대체재가 아니라 더 좋은 코드를 만들어주는 파트너로 생각해요\n- **클라우드는 기본기**: 이제 선택사항이 아니라 기본 소양이 되었어요\n- **보안은 습관**: 나중에 추가하는 게 아니라 처음부터 생각하는 습관을 길러야 해요\n- **천천히 꾸준히**: 모든 걸 한 번에 배우려 하지 말고, 하나씩 차근차근 해봐요\n\n### 앞으로 어떻게 해볼까요?\n\n저는 이런 식으로 계획을 세워봤어요. 혹시 참고가 되실지:\n\n1. **3개월 단위로**: 너무 먼 미래는 예측하기 어려우니, 짧은 주기로 목표를 세워요\n2. **작은 프로젝트부터**: 새로운 기술은 일단 토이 프로젝트로 맛보기\n3. **동료들과 함께**: 혼자 하면 금방 지치니까, 스터디나 사이드 프로젝트를 같이 해요\n\n### 마지막으로 하고 싶은 말\n\n2025년이 시작된 지 얼마 안 되었는데 벌써 많은 변화가 일어나고 있어요. 가끔 \"이 모든 걸 다 따라가야 하나?\" 싶어서 막막할 때도 있지만, 천천히 하나씩 해나가면 분명히 할 수 있을 거예요.\n\n무엇보다, 기술은 도구일 뿐이고 우리가 해결하고 싶은 문제가 무엇인지가 더 중요하다고 생각해요. 새로운 도구들을 현명하게 활용해서, 더 좋은 서비스를 만들고 더 많은 사람들에게 도움이 되는 그런 개발자가 되면 좋겠어요.\n\n올해도 함께 성장해봐요! 궁금한 게 있으시거나 경험을 공유하고 싶으시면 언제든 댓글이나 메일로 연락 주세요. 🚀\n\n---\n\n**관련 글**:\n- [시니어 개발자가 되기 위한 5가지 핵심 역량](/blog/senior-developer-competencies) (다음 포스트)\n- [마이크로서비스 아키텍처 도입 가이드](/blog/microservices-guide) (3월 예정)\n\n**참고 자료**:\n- [Stack Overflow Developer Survey 2024](https://survey.stackoverflow.co/2024/)\n- [GitHub State of the Octoverse 2024](https://github.blog/2024-11-06-the-state-of-the-octoverse-2024/)\n- [CNCF Annual Survey 2024](https://www.cncf.io/reports/cncf-annual-survey-2024/)","categories":["Tech","Trends"],"tags":["2025","technology","trends","development","AI","cloud","web3"],"pubDate":"2025-01-22T00:00:00.000Z","url":"/blog/2025-tech-trends-for-developers/"},{"id":"en/2019-04-17-toy-project-youtube-downloader","title":"Toy Project #1","description":"","content":"### [What is J-YoutubeDL?]\n\nIt's a Windows program that lets you copy and paste YouTube links (URLs) to download videos and music in your desired quality from the supported formats of that YouTube link.\n\n### [Why did I make it?]\n\nI was using web-based link input services but wanted to download in different qualities, so I made my own.\n\n### [How to install & run?]\n\n[**Installation File Link**](https://github.com/jayleekr/YoutubeDownloaderWPF/releases/tag/1.0.1904)\n\n![Desktop View](/assets/img/post1/github.png)\n\nThe link above has everything from source code to installation files and .Net40.\n\nThe dotnetfx40 program doesn't need to be installed if your Windows PC already has the framework, and it even automatically downloads via web connection during installation. It's uploaded just in case, so those who need it can download it. (**dotNetFx40_Client_x86_x64.exe**)\n\nDownload Release1.0.1904.zip, extract it, and run setup.exe to see the screen below.\n\n![Desktop View](/assets/img/post1/setup.png)\n\nSet the installation location as you prefer and just keep clicking next to finish the installation.\n\nAfter installation, the following icon will be created on your desktop and start programs.\n\n![Desktop View](/assets/img/post1/icon.png)\n\nRunning the program shows the screen below.\n![Desktop View](/assets/img/post1/main.png)\n\n### [How to use?]\n\nUsage is simple. <br/>\n<br/>\nGo to www.youtube.com, copy the URL of your desired video to clipboard, then paste it into the program's URL field. This will generate a list of extractable video and audio formats and quality options for that video.<br/>\n![Desktop View](/assets/img/post1/howtogeturl.png)\n\n<br/>\n<br/>\n<br/>\n\nSelect your desired settings and click the GO! button to create the file in the \"Save Location\".\n![Desktop View](/assets/img/post1/workwell1.png)\n![Desktop View](/assets/img/post1/workwell2.png)\n\nNote that the default \"Save Location\" is the program's **installation directory**.\n\nI recommend changing it to your desired location.\n\nSince it only includes very basic features, please leave your desired features in the **comments** and I'll implement and add them when I'm bored.\n\nAdios then","categories":["Collaboration","ToyProjects"],"tags":["ToyProjects","C#","Windows","Youtube","Downloader","유튜브","다운로더","유튜버"],"pubDate":"2019-04-17T03:00:00.000Z","url":"/blog/en/2019-04-17-toy-project-youtube-downloader/"},{"id":"en/2020-04-05-cross-development-fundamentals","title":"Cross Development Fundamentals","description":"","content":"## What is Cross Development?\n\nCross Development means developing applications where the **development environment** you're currently working in is different from the **target environment** where the developed application will actually run.\n\nFor example, you could use Visual Studio on Windows 10 to create applications that run on Windows 10, but with the help of frameworks, you can also create applications that run on iOS, Android, etc. at the OS level. Xamarin, UWP, Flutter are examples of such frameworks.\n\nWhat does it mean for **environments** to be different?\n\nSomeone might think, *\"Whether it's Windows, Mac, or Android, coding is all the same, and the application-level functionality we want is all the same, so what's different?\"*\n\nBut when you look under the hood, there are libraries that the application needs to function, and there need to be devices, kernels, operating systems, and architectures in place to run those libraries before the application can actually work.\n\nWe sometimes call the system-level work that handles these things primarily the framework layer.\n\n- JAVA's **JavaRuntimeEnvironment**\n- Windows' **.NETFramework**\n- macOS's Rosetta 2\n\nAnd integrated development environments (IDEs) like Visual Studio and Android Studio make it easy for developers to develop by connecting compilers, linkers, debuggers, etc. with frameworks.\n\nSomeone might say, *\"Well, I'm going to develop using interpreter languages like Python, JavaScript, Ruby, SQL that don't need compilation, so I don't need to do Cross Development, right?\"*\n\nIf the ***libraries*** that your interpreter language needs don't ***support*** the environment you want to run in, then applications written in that language become useless.\n\nThis is why we need to thoroughly consider environments to test and guarantee application performance and functionality.\n\n## Cross Compiling for Embedded Systems\n\n![Desktop View](/assets/img/00-CrossDevelopment/1.png)\nRef: Linux Foundation 2020 Conference   \n\nEmbedded systems are one of the fields where Cross Development techniques must be used efficiently.\n\nWhile embedded devices have excellent performance today, just a few years ago, it was almost impossible to dream of developing directly on those devices or compiling on them.\n\nThat's why Cross build system-related Open Source Projects like OpenEmbedded, Buildroot, Yocto Project, and Bitbake have evolved.","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-04T15:00:00.000Z","url":"/blog/en/2020-04-05-cross-development-fundamentals/"},{"id":"en/2020-04-06-understanding-toolchains","title":"Understanding Toolchains for Cross Development","description":"","content":"## What is a Toolchain?\n\nA Toolchain is literally a collection of tools for Cross Development.\n\n![Desktop View](/assets/img/01-Toolchain/1.png)\nRef: Linux Foundation 2020 Conference   \n\nAs shown in the diagram above, the GNU Toolchain is broadly divided into compiler, binutils, C library, and GDB debugger.\n\n- Compiler: A tool for compiling from the host development environment to the target architecture\n- binutils: Assemblers, linkers, etc. for controlling compiled objects\n- C library: POSIX and Linux kernel interfaces\n- GDB: GNU debugger\n\n## Getting a Toolchain\n\n![Desktop View](/assets/img/01-Toolchain/2.png)\nRef: Linux Foundation 2020 Conference   \n\nAs shown in the diagram above, toolchains are usually provided by chip manufacturers. Alternatively, you can download them from famous third parties like Linaro, or get them conveniently using build systems like the Yocto Project.\n\n## Toolchain Prefix\n\n![Desktop View](/assets/img/01-Toolchain/3.png)\nRef: Linux Foundation 2020 Conference   \n\nLet's look at the toolchain naming rules through the diagram above.\n\nThey're separated by \"-\" and named in the order \"architecture-vendor-kernel-OS\".\n\n## Toolchain Prefix for ARM Toolchain\n\n![Desktop View](/assets/img/01-Toolchain/4.png)\nRef: Linux Foundation 2020 Conference   \n\nI mentioned above that toolchain naming ends with the OS. Additionally, the ABI (Application Binary Interface) gets added to this.\n\nThis tells us the rules about how the target OS exchanges binary data between applications.\n\n- Data types and alignment methods\n- How to exchange registers for arguments and results during function calls\n- How to make system calls\n- How to initialize program code start and data\n- How to exchange files (ELF, etc.)\n\nAs shown in the diagram above, old (or obsolete) ABIs omit the suffix after the OS. This was used during the era when 32-bit ARM architecture was supported.\n\nStarting from EABI (Embedded ABI), it was created to support 64-bit architecture in embedded environments (ARM, PowerPC, MIPS, etc.).\n\nIf you just use eabi, it's softfloat, but if you add hf to make it eabihf, it becomes hardfloat. The differences are as follows:\n\n- softfloat\n    - Doesn't create FP instructions; GCC prepares them as functions in libraries at compile time\n- hardfloat\n    - Emulates FP instructions\n\n    > A CPU like the ARM can do calculations. In most programs most of these calculations are of the \"whole numbers\" (integer) type, as these are very simple to do electronically.\n    Some programs also do \"floating point\" calculations, and these programs also are expected to work on CPU's that do not have the hardware to do floating point calculations. In such cases these calculations are automatically routed, by the operating system, to a library of calculation routines that do these calculations using integer calculus. For example a simple division like 2/3 is done with hundreds of integer calculations. This is called \"software floating point calculus\", or short \"soft float\"\n    But the ARM chip has a CPU that can also do floating point calculations directly in hardware!\n    This happens very much faster, as a floating point calculation in hardware is almost as fast as a integer calculation. hardware floating point calculus is shortened to \"hard float\".\n    In the past the R-PI's operating systems did not \"know\" the R-PI's CPU could do floating point calculus, so all floating point calculations were done using the software library. With the latest OS's they became \"aware\" of the hard float capability of the PI and began using it, which means a very big speed increase for programs using a lot of floating point calculations.\n\n## Toolchain Sysroot\n\n![Desktop View](/assets/img/01-Toolchain/5.png)\nRef: Linux Foundation 2020 Conference   \n\nSysroot refers to the rootfs that will actually be present in the target environment.\n\n![Desktop View](/assets/img/01-Toolchain/6.png)\nLinux rootfs\n\n> The root file system (named rootfs in our sample error message) is the most basic component of Linux. A root file system contains everything needed to support a full Linux system. It contains all the applications, configurations, devices, data, and more. Without the root file system, your Linux system cannot run.\n\n![Desktop View](/assets/img/01-Toolchain/7.png)\nRef: Linux Foundation 2020 Conference   \n\nYou don't need everything in that target environment - just some parts for cross-development.\n\nFor example, libraries and header files.\n\n### What Toolchain Contains\n\n![Desktop View](/assets/img/01-Toolchain/8.png)\nRef: Linux Foundation 2020 Conference   \n\n## Example\n\nLet's assume we're developing with Raspberry Pi as the target.\n\n![Desktop View](/assets/img/01-Toolchain/9.png)\nRef: Linux Foundation 2020 Conference   \n\n> These days, they're called SBCs (Single Board Computers) and have performance that wouldn't be embarrassing to call computers even if we go back just 10 years. But embedded devices are still embedded devices - they don't match our host computer performance, and the goal of the device isn't to replace PCs, right? I'm getting a bit off-topic here. Haha;;\n\nThe official toolchains for the Raspberry Pi 3B model are the following 6:\n\n- arm-bcm2708hardfp-linux-gnueabi\n- arm-bcm2708-linux-gnueabi\n- arm-linux-gnueabihf\n- arm-rpi-4.9.3-linux-gnueabihf\n- gcc-linaro-arm-linux-gnueabihf-raspbian\n- gcc-linaro-arm-linux-gnueabihf-raspbian-x64\n\nDo you see the common patterns in these names?\n\nThis sequence is usually called a Triple.\n\nThese days, \\<arch>-\\<vendor>-\\<kernel>-\\<os> is the basic rule, and they seem to modify it a bit here and there. (I need to research this more.)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-05T15:00:00.000Z","url":"/blog/en/2020-04-06-understanding-toolchains/"},{"id":"en/2020-09-11-creating-github-blog","title":"Creating a Blog with GitHub Pages","description":"","content":"## How GitHub Pages Works\n\nGitHub provides a web page service linked to your account username. Pretty neat, right?\n\nFor example, my GitHub User ID is `jayleekr`. If I create a new blog repository at https://github.com/jayleekr/jayleekr.github.io, then GitHub automatically creates a website at https://jayleekr.github.io/ that's connected to that repository.\n\nThis means GitHub (on the remote side) automatically generates a backend web server for you following this naming convention. It's like getting free hosting! 🎉\n\nLet's build the front-end!\n\n## Front-end Framework\n\nThere are tons of front-end frameworks out there, but to get up and running quickly, I'll use the most popular method: **Jekyll**.\n\nJekyll was created by Tom Preston-Werner (GitHub co-founder) using Ruby-on-Rails, and it's an MIT-licensed static site generator.\n\n*(Makes sense that the GitHub co-founder's tool integrates seamlessly with GitHub, right?)*\n\n## Step 1: Create Repository\n\n1. Go to GitHub and create a new repository\n2. **Important**: Name it `[your-username].github.io`\n   - For example: `jayleekr.github.io`\n3. Make it public (required for free GitHub Pages)\n4. Initialize with a README\n\n## Step 2: Choose a Jekyll Theme\n\nInstead of building from scratch, let's use a pre-made theme:\n\n### Option A: Fork a Theme\n1. Browse [Jekyll Themes](http://jekyllthemes.org/)\n2. Find one you like\n3. Fork it to your account\n4. Rename the repository to `[username].github.io`\n\n### Option B: Start from Scratch\n```bash\n# Clone your repository\ngit clone https://github.com/[username]/[username].github.io.git\ncd [username].github.io\n\n# Create a basic Jekyll site\necho \"Hello World\" > index.html\n\n# Commit and push\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main\n```\n\n## Step 3: Local Development Setup\n\nTo test locally before publishing:\n\n### Install Jekyll\n```bash\n# On macOS (using Homebrew)\nbrew install ruby\ngem install jekyll bundler\n\n# On Ubuntu/Debian\nsudo apt-get install ruby-dev\ngem install jekyll bundler\n\n# On Windows\n# Download Ruby installer from rubyinstaller.org\ngem install jekyll bundler\n```\n\n### Run Locally\n```bash\n# In your repository directory\njekyll serve\n\n# or if you have a Gemfile\nbundle exec jekyll serve\n```\n\nVisit `http://localhost:4000` to see your site!\n\n## Step 4: Customize Your Blog\n\n### Basic Configuration (_config.yml)\n```yaml\ntitle: Your Blog Title\ndescription: A cool description of your blog\nbaseurl: \"\"\nurl: \"https://[username].github.io\"\n\n# Build settings\nmarkdown: kramdown\nhighlighter: rouge\ntheme: minima\n\n# Social links\ngithub_username: your-github-username\ntwitter_username: your-twitter-username\n```\n\n### Create Your First Post\nCreate a file: `_posts/2020-09-11-my-first-post.md`\n\n```markdown\n---\nlayout: post\ntitle:  \"My First Blog Post!\"\ndate:   2020-09-11 14:10:00 +0900\ncategories: general\n---\n\n# Hello World!\n\nThis is my first blog post on GitHub Pages. \nPretty exciting stuff! 🚀\n\n## What I'll Write About\n\n- Technology stuff\n- Coding adventures\n- Random thoughts\n\nStay tuned for more content!\n```\n\n### Add Pages\nCreate `about.md`:\n```markdown\n---\nlayout: page\ntitle: About\npermalink: /about/\n---\n\nHi there! I'm [Your Name], and this is my blog where I write about...\n```\n\n## Step 5: Deploy\n\nThe beauty of GitHub Pages is that deployment is automatic:\n\n```bash\ngit add .\ngit commit -m \"Add first post and about page\"\ngit push origin main\n```\n\nWithin a few minutes, your changes will be live at `https://[username].github.io`!\n\n## Pro Tips\n\n### 1. Use a Custom Domain (Optional)\nCreate a `CNAME` file with your domain:\n```\nmyblog.com\n```\n\nThen configure your DNS settings to point to GitHub Pages.\n\n### 2. SEO Optimization\nAdd to your `_config.yml`:\n```yaml\nplugins:\n  - jekyll-sitemap\n  - jekyll-seo-tag\n```\n\n### 3. Comments with Disqus\nAdd to your `_config.yml`:\n```yaml\ndisqus:\n  shortname: your-disqus-shortname\n```\n\n### 4. Google Analytics\n```yaml\ngoogle_analytics: UA-XXXXXXXX-X\n```\n\n## Troubleshooting\n\n### Build Failures\n- Check your `_config.yml` syntax\n- Make sure all posts have proper front matter\n- Verify image paths are correct\n\n### Local vs Production Differences\n- Use `bundle exec jekyll serve` locally\n- Check that your `baseurl` is set correctly\n- Ensure all links use `site.baseurl` prefix\n\n### Slow Build Times\n- Minimize plugins\n- Optimize images\n- Use `incremental: true` in development\n\n## Advanced Features\n\n### Custom Themes\n```bash\n# Add to Gemfile\ngem \"theme-name\"\n\n# Install\nbundle install\n\n# Update _config.yml\ntheme: theme-name\n```\n\n### Collections\nPerfect for portfolios or project showcases:\n```yaml\ncollections:\n  projects:\n    output: true\n    permalink: /:collection/:name/\n```\n\n## What's Next?\n\nNow that you have a blog:\n1. **Write consistently** - Even short posts are valuable\n2. **Engage with community** - Comment on other blogs\n3. **Share your knowledge** - Teach what you learn\n4. **Customize gradually** - Improve the design over time\n\n*Remember: The best blog is one that exists and gets updated regularly! 🚀*","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Jekyll"],"pubDate":"2020-09-11T05:10:00.000Z","url":"/blog/en/2020-09-11-creating-github-blog/"},{"id":"en/2020-10-05-syncing-fork-with-upstream-repository","title":"Syncing Your Fork with the Upstream Repository","description":"","content":"## TLDR\n\nMy repository (fork): https://github.com/jayleekr/adas-study-group.github.io\n\nGroup repository: https://github.com/ADAS-study-group/adas-study-group.github.io\n\n### 0. Clone your forked repository locally\n\n~~If you've already done this, you can skip it, obviously!~~\n\n``` sh\n$ git clone https://github.com/jayleekr/adas-study-group.github.io.git forked_repo\n$ cd forked_repo\n```\n\n### 1. Set up remote configuration for the upstream repository\n\n``` sh\n$ git remote add upstream https://github.com/ADAS-study-group/adas-study-group.github.io.git\n\n# Let's verify\n$ git remote -v\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (fetch)\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (push)\nupstream        https://github.com/ADAS-study-group/adas-study-group.github.io.git (fetch)\nupstream        https://github.com/ADAS-study-group/adas-study-group.github.io.git (push)\n```\n\n### 2. Merge the upstream repository into your fork\n\nFetch first:\n\n``` sh\n$ git fetch upstream\nremote: Enumerating objects: 172, done.\nremote: Counting objects: 100% (172/172), done.\nremote: Compressing objects: 100% (56/56), done.\nremote: Total 148 (delta 67), reused 135 (delta 57), pack-reused 0\nReceiving objects: 100% (148/148), 1.33 MiB | 1.30 MiB/s, done.\nResolving deltas: 100% (67/67), completed with 4 local objects.\nFrom https://github.com/ADAS-study-group/adas-study-group.github.io\n * [new branch]      gh-pages   -> upstream/gh-pages\n * [new branch]      master     -> upstream/master\n``` \n\nNow let's merge:\n\n``` sh\n$ git merge upstream/master\n```\n\nPush to your fork:\n\n``` sh\n$ git push\nCounting objects: 7, done.\nDelta compression using up to 24 threads.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (7/7), 1.40 KiB | 1.40 MiB/s, done.\nTotal 7 (delta 4), reused 0 (delta 0)\nremote: Resolving deltas: 100% (4/4), completed with 4 local objects.\nTo https://github.com/jayleekr/adas-study-group.github.io.git\n   203f2c0..d7f0922  master -> master\n```\n\nAnd that's it! Your fork is now synced with the upstream repository. This is super useful when you're contributing to open source projects and need to keep your fork up to date with the latest changes.\n\n## Reference\n\n1. https://hyunjun19.github.io/2018/03/09/github-fork-syncing/","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Git"],"pubDate":"2020-10-04T15:00:00.000Z","url":"/blog/en/2020-10-05-syncing-fork-with-upstream-repository/"},{"id":"en/2020-10-19-about-automotive-grade-linux","title":"About AGL (Automotive Grade Linux)","description":"","content":"## 1. Introduction\n\nAGL (Automotive Grade Linux) is an open source OS (Operating System) suitable for automotive environments, created through collaboration between various vehicle manufacturing and parts companies from Europe, Japan, Korea, and other regions. It's rapidly developing under the guidance of the Linux Foundation with Toyota's strong funding power.\n\nThere are many participating member companies, but Japanese companies, led by Toyota, are very actively pursuing this project.\n[https://www.automotivelinux.org/about/members/](https://www.automotivelinux.org/about/members/)\n\n![Desktop View](/assets/img/post/2020-10-19/1.png)\n\nToyota was actually showing rather passive activity on AGL, almost like just a financial sponsor, but starting from Q3 2020, they suddenly began pursuing it quite aggressively.\nAs a side note, Toyota, a regular guest of Harvard Business School education and the ultimate bureaucratic organization, has recently been attempting to change to a performance-based system and trying to transform through various SW policies including AGL.\n(According to rumors, they were stimulated by Tesla...)\n\nRecently, BMW and VW Group's research centers are also actively participating, and it feels like they're doing [strategic alliances](https://namu.wiki/w/%ED%95%A9%EC%A2%85%EC%97%B0%ED%9A%A1) as a kind of complement set to counter Tesla...\n\nFor these reasons, I felt that having a basic study of AGL should be essential, so I decided to write this article as both a summary and reference.\n\nLet's get started, shall we?\n\n## 2. Prelude\n\nTo understand AGL, you need to know a bit about [Tizen OS](https://namu.wiki/w/%ED%83%80%EC%9D%B4%EC%A0%A0).\nTizen OS is a mobile environment OS started by Samsung and Intel.\nThey ambitiously started it to prevent the monopoly of Android and iOS, but were overwhelmingly defeated by their powerful developer-friendly policies and marketing strategies, eventually disappearing into the back pages of history in the mobile market.\n(Samsung stopped developing Tizen phones as of 2018)\n\nBut AGL is reviving this dead OS by starting a base project with [Tizen IVI](https://wiki.tizen.org/IVI) as a reference.\n\n## 3. The Competition\n\n![Desktop View](/assets/img/post/2020-10-19/genevi.png)\n\nA representative competitor is the GENIVI Open Source Software Project from [GENIVI Alliance](https://en.wikipedia.org/wiki/GENIVI_Alliance).\n(I plan to cover this in detail in a future article)\n\nGENIVI started at an earlier point in 2009. This project is also led by the Linux Foundation.\n\nParticipating members include BMW Group, Delphi, GM, Intel, Magneti-Marelli, PSA Peugeot Citroen, Visteon, Wind River System, etc.\nNot only automotive OEMs but also companies like Wind River, famous for NASA rocket OS (VxWorks), participate, making it one of the representative players in the anti-Google camp in the smart car market.\n\n## 4. Goals\n\nAs I mentioned above, AGL targets the IVI (In-Vehicle Infotainment) market.\nThis is the same for GENIVI. The common point of these two OSes is that they're non-profit organizations.\nWhether due to market logic or not, their development speed was significantly slower compared to Android or iOS.\nHowever, after events like the 2020 Huawei incident where Google's Android OS usage license itself was blocked, and with recent escalation to international disputes, \nthe development speed of these OSes is accelerating.\n\nLooking at the direction, it seems to prioritize following up on features currently supported in mobile.\n\nBut there's still a very long way to go.\n\nLet's look at the current indicators and direction through AGL's 2020 roadmap materials. [Link](https://wiki.automotivelinux.org/agl-roadmap)\n\n## 5. 2020 Roadmap (February Update)\n\n- Yocto 2.6(thud) → 3.0(zeus) platform update\n- Support for graphical API features for various platforms\n- Solutions for smartphone connectivity like Android Auto, Apple CarPlay\n- Roadmap for continuous updates\n- Enhanced QA and testing (CI/CD) coverage\n- C/C++ skeleton code and Json auto-generation API tool development\n- Proposing a model for controlling vehicles through smartphones using cloud systems\n- Native MQTT protocol support (communication with minimal power and data)\n- Security/Application Framework\n  - Adding firewall functionality related to application framework\n  - Security enhancement through security manager functionality\n  - Providing framework that supports plugins, security, protocols and is easy to maintain\n  - Support for legacy and third-party applications like Java Client, LXC/systemd\n  - Built-in functionality for connection persistence (keep alive, reconnection)\n  - Javascript and python binding support\n- Boot and Power related\n  - Boot manager supporting boot time reduction, boot mode selection, monitoring functionality\n  - RAM Sleep support & low temperature operation\n- AGL minimization system functionality support for ECUs requiring minimal functionality\n- Real Time API functionality support to guarantee QoS\n\n### 5.1 App FW & Security EG(Expert Group)\n\n- Establish mechanism for developers to sign and load applications\n- App Launcher for web apps and code management strategy for HTML5 downloadable on the fly\n- App framework API and strategy to stop non-privileged apps in background (e.g., SIGTERM)\n- Provide app framework communication binder that can return from Sleep mode\n- Application lifecycle manager\n  - Recognize background apps from home screen (music, phone, unread messages, etc.)\n  - Ability to turn off each through manager\n  - Need to define and implement SM(State Management)\n- Provide hardware abstraction layer (HAL) API for app registration and packaging\n- Modularization of main application framework that can manage keys, maintain, and build on multi-platform basis\n  - Separation of application framework and keys (code level)\n  - Enable device developers to easily change keys\n  - Provide library for easy key management and binding\n\n- Web Apps/HTML5\n  - Support Chromium Webview Upstream API required by WAM (Web App Manager)\n  - Improve communication connection between WAM and Chromium\n  - Make WAM upstream and WebOSE Chromium work independently\n  - Integration of new Window Manager and WAM\n  - Web apps\n    - Integrate new security model\n    - Enhanced application lifecycle manager\n    - Provide HTML5 demo platform containerized\n    - Provide demo web app library\n    - Provide additional demo applications\n\n### 5.2 Graphical EG\n\n**Seems to have mostly achieved roadmap by end of 2019**\n\n- Complete Window Manager and Homescreen development\n  - Homescreen API/service\n    - QT, HTML5 completion stage\n  - Provide high-level API for Japanese and English virtual keyboard\n  - Support popup functionality (virtual keyboard, warnings, etc.)\n  - Support multi-display resolution related functionality\n    - Portrait vs Landscape\n    - Scalable display size\n- Multi-page, folder, slider and other off-screen app management\n- Enhanced dual screen functionality\n- Hardware Plane management: rear camera, smartphone connection, etc.\n- Interactive user response functionality: screen shake, beep, etc.\n- Wayland/Weston Upstream support\n- xdg protocol\n- QT change review: HTML5, GTK+, etc.\n- High Level Audio API support\n- Change Bluetooth Audio to Blues/Alas\n- Voice recognition and text-to-speech service\n- Policy management between microphone input and media player output\n\n### 5.3 Connectivity EG\n\n- Vehicle Signal management\n  - CAN message initialization based on last vehicle settings\n  - Encryption of streaming and Blu-ray content\n  - Enhanced CANoe ↔ AGL message translation functionality support\n- Enhanced Bluetooth functionality\n  - Prepare support for proprietary chip stacks based on work done by other projects supporting AGL\n  - Low power functionality support\n- Enhanced Wifi\n  - AP mode support\n- Enhanced phone functionality\n- Network binding\n  - Provide concurrency functionality for telematics services requiring simultaneous phone calls\n\n### 5.4 V2X EG\n\n- Waiting for Kickoff\n\n### 5.5 Virtualization EG\n\n- AGL virtualization support as Host\n  - Add Open source Hypervisors functionality supporting both ARM and Intel\n  - Decide which Hypervisor to select at compile time\n    - Finally(?) KVM support (available on Renesas RCar-M3)\n    - XEN and Jailhouse support\n- AGL virtualization support as Guest\n  - Officially operate as guest OS on ARM and Intel CPUs\n  - XEN, KVM, Jailhouse support\n  - Provide Guide Document\n- AGL's graphical virtual machine manager application\n  - Create tool to control guest OS from AGL GUI\n  - Provide guest OS start/stop, USB attachment functionality\n- VMs/AGL profile communication\n  - Define/design/develop common API for communication and interaction between different VMs and AGL\n\n### 5.6 Navi EG\n\n- AGL navigation API development completed\n- Documentation completed\n- Reference app provided\n\n## 6. 5-Star Projects\n\n### 6.1 Resource Control Project\n\n[Jira Link](https://jira.automotivelinux.org/browse/SPEC-138?filter=10409)\n\nAGL should provide a mechanism for allocating CPU sets to a series of tasks (kernel's cpuset). Based on policy-based decisions, the policy manager should allocate appropriate CPU subsets to target processes or process groups using \"resource control\" at the kernel layer. (Generally cgroups/cpuset are used).\n\n- Background:\nHardware may have multiple CPUs and the system may run multiple tasks/APPs. The system can benefit from careful processor placement to reduce scheduling and contention (cache, etc.)\n\n## 7. 4-Star Projects\n\n### 7.1 Smart Device Link (SDL)\n\n[Jira Link](https://jira.automotivelinux.org/browse/SPEC-133?filter=10410)\n\nPort Smart Device Link to AGL.\n\n- Background:\nSmart Device Link is being used by Ford in next-generation vehicles and is being marketed industry-wide in partnership with Toyota.\n\n## 8. Conclusion\n\nBy summarizing the roadmap as above, we could infer the parts that have progressed so far and the future direction.\n\nAPIs for device usage like wifi, bluetooth, and audio have matured to some extent, and the navigation API, which is most important in automotive environments, seems to be in completion stage.\nBy putting a lot of effort into supporting web apps, it seems they can guarantee cross-platform compatibility and provide flexible APIs to developers.\n\nAdditionally, they're considering many APIs to ensure security at the framework level and make development easier for developers.\n\nWe could see that they still don't support many features that Smart Phones support.\nActually, to support features like Cloud, projects in Connectivity or V2X EG need to progress and mature quickly.\n\nGraphical API has achieved the roadmap in many areas, but they're also considering transition from the current QT to something else.","categories":["TechSavvy","OperatingSystems"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2020-10-18T15:00:00.000Z","url":"/blog/en/2020-10-19-about-automotive-grade-linux/"},{"id":"en/2020-09-20-how-to-contribute-to-repo","title":"So... How Do I Actually Contribute to a Repository?","description":"","content":"## TLDR\n### 1. Fork the repo to your account\n\n![Desktop View](/assets/img/post/2020-09-20/fork.png)\n\n### 2. Work on your own copy and write your content\n\n![Desktop View](/assets/img/post/2020-09-20/my_posting.png)\n\n### 3. Build it locally\n\n```sh\n$ bash tools/build.sh\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/_config.yml\n          Cleaner: Removing /home/jayleekr/00_Projects/08_ADAS_main/_site...\n          Cleaner: Nothing to do for /home/jayleekr/00_Projects/08_ADAS_main/.jekyll-metadata.\n          Cleaner: Nothing to do for /home/jayleekr/00_Projects/08_ADAS_main/.jekyll-cache.\n          Cleaner: Nothing to do for .sass-cache.\n$ cd /home/jayleekr/00_Projects/08_ADAS_main/.container\n[INFO] Succeed! 5 category-pages created.\n[INFO] Succeed! 6 tag-pages created.\n```\n\n### 4. Test it locally\n\n```sh\n$ bash tools/test.sh\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/_config.yml\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/_config.yml\n            Source: /home/jayleekr/00_Projects/08_ADAS_main\n       Destination: /home/jayleekr/00_Projects/08_ADAS_main/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n                    done in 2.183 seconds.\n Auto-regeneration: enabled for '/home/jayleekr/00_Projects/08_ADAS_main'\n    Server address: http://127.0.0.1:4000/\n  Server running... press ctrl-c to stop.\n```\n\n### 5. Check how it looks\n\nOpen your browser and go to `http://127.0.0.1:4000/` - make sure everything looks good!\n\n### 6. Commit and push your changes\n\n```sh\n$ git add .\n$ git commit -m \"Add my awesome contribution\"\n$ git push origin main\n```\n\n### 7. Create a Pull Request\n\nGo back to the original repository and create a pull request from your fork. Write a nice description of what you've added or changed!\n\n## The Long Version (For Those Who Want Details)\n\nHey there! So you found a cool open source project and want to contribute? Awesome! Here's the step-by-step breakdown of how to actually do it without breaking anything (been there, done that 😅).\n\n### Why Fork Instead of Clone?\n\nYou might be wondering - why not just clone the repository directly? Well, unless you're already a maintainer, you probably don't have write access to the original repo. Forking creates your own copy that you can mess around with to your heart's content.\n\n### The Sacred Build-Test-Commit Cycle\n\nOne thing I learned the hard way: **always build and test locally before pushing**. I can't tell you how many times I've seen pull requests that break the build because someone forgot this step. Don't be that person! \n\nThe build process does a few important things:\n- Generates category and tag pages automatically\n- Validates your markdown syntax\n- Optimizes images and assets\n- Makes sure all links work\n\n### Pro Tips from the Trenches\n\n1. **Write descriptive commit messages**: \"Fix stuff\" is not helpful. \"Fix broken link in Docker tutorial\" is much better.\n\n2. **Keep your fork up to date**: Before starting new work, always sync with the upstream repository:\n   ```sh\n   $ git remote add upstream https://github.com/original-owner/repo-name.git\n   $ git fetch upstream\n   $ git merge upstream/main\n   ```\n\n3. **Test on different screen sizes**: Your contribution should look good on both desktop and mobile.\n\n4. **Follow the project's style**: Look at existing content and try to match the tone and formatting.\n\n### Common Gotchas\n\n- **Image paths**: Make sure your images are in the right directory and the paths are correct\n- **Front matter**: Jekyll is picky about the YAML front matter at the top of posts\n- **Build dependencies**: Make sure you have all the required gems installed\n\n### What Makes a Good Contribution?\n\nThe maintainers will love you if your contribution:\n- Solves a real problem or adds genuine value\n- Follows the existing conventions\n- Includes proper documentation\n- Doesn't break anything\n- Has been tested locally\n\nRemember, open source is all about collaboration and making things better together. Don't be afraid to start small - even fixing typos is a valuable contribution!\n\n*Happy contributing! 🚀*","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-09-20T05:10:00.000Z","url":"/blog/en/2020-09-20-how-to-contribute-to-repo/"},{"id":"en/2020-10-08-adding-gpg-signature-to-github","title":"Adding GPG Signature to GitHub - A Complete Guide","description":"","content":"## About GPG\n\nHey there! GPG stands for GNU Privacy Guard, and you might also hear it called PGP. :D\n\nIt's a powerful encryption program that comes bundled with Ubuntu by default - pretty handy, right?\n\nSince GitHub is an open-source platform, anyone can clone and access public repositories. But when you're working with private repositories or collaborating on sensitive projects, having someone peek at your data is definitely not cool.\n\nThat's why GitHub strongly recommends (and sometimes requires) using GPG signatures when making pull requests. It's like adding a digital signature to prove that the commit really came from you!\n\nLet me walk you through how to set up GPG signatures step by step. I'll keep it brief here, but check out the reference links at the bottom for more detailed info.\n\n## 1. Generating a GPG Key\n\n``` sh\n$ gpg --full-generate-key\ngpg (GnuPG) 2.2.4; Copyright (C) 2017 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\nYour selection? \n```\n\nI went with the RSA encryption method (the default option).\n\n``` sh\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (3072) 4096\nRequested keysize is 4096 bits\n```\n\nGitHub requires at least 4096 bits, so let's go with 4096!\n\n``` sh\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      <n>  = key expires in n days\n      <n>w = key expires in n weeks\n      <n>m = key expires in n months\n      <n>y = key expires in n years\nKey is valid for? (0) 0\n```\n\nI was feeling lazy, so I set it to never expire. Just enter 0!\n\n``` sh\nGnuPG needs to construct a user ID to identify your key.\n\nReal name: Jay Lee\nEmail address: jayleekr0125@gmail.com\nComment: lol\nYou selected this USER-ID:\n    \"Jay Lee (lol) <jayleekr0125@gmail.com>\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\n```\n\nFill in your name, email, and other info, then proceed to generate your GPG key.\n\nOh, and don't forget your password! 😅\n\n## 2. Checking Your GPG Key\n\n``` sh\n$ gpg --list-secret-keys --keyid-format LONG\ngpg: checking the trustdb\ngpg: marginals needed: 3  completes needed: 1  trust model: pgp\ngpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u\n/home/jayleekr/.gnupg/pubring.kbx\n---------------------------------\nsec   rsa4096/037ED189F6F42EF3 2020-10-08 [SC]\n      A95244C509A02D9F0790CFB0037ED189F6F42EF3\nuid                 [ultimate] Jay Lee (lol) <jayleekr0125@gmail.com>\nssb   rsa4096/38DDEF7B4E6758A4 2020-10-08 [E]\n```\n\nHere's my GPG key information! My GPG Key ID is `037ED189F6F42EF3`.\n\nThe public key encryption file is located at `/home/jayleekr/.gnupg/pubring.kbx` (I backed this up just like SSH keys, haha).\n\nNow let's generate the public key using this ID!\n\n## 3. Generating the Public Key\n\nTo generate the public key, use your ID like this:\n\n``` sh\n$ gpg --armor --export 037ED189F6F42EF3\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n...\n...\n-----END PGP PUBLIC KEY BLOCK-----\n```\n\nYou'll get output from BEGIN to END like above. Just copy this entire block to your GitHub personal settings, and you're done!\n\n## 4. Adding the GPG Public Key to GitHub\n\n![Desktop View](/assets/img/post/2020-10-08/userbar-account-settings.png)\n\n![Desktop View](/assets/img/post/2020-10-08/settings-sidebar-ssh-keys.png)\n\n![Desktop View](/assets/img/post/2020-10-08/gpg-key-paste.png)\n\n## 5. Telling Git About Your GPG Key\n\nWe confirmed our GPG ID earlier:\n\n``` sh\n$ gpg --list-secret-keys --keyid-format LONG\ngpg: checking the trustdb\ngpg: marginals needed: 3  completes needed: 1  trust model: pgp\ngpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u\n/home/jayleekr/.gnupg/pubring.kbx\n---------------------------------\nsec   rsa4096/037ED189F6F42EF3 2020-10-08 [SC]\n      A95244C509A02D9F0790CFB0037ED189F6F42EF3\nuid                 [ultimate] Jay Lee (lol) <jayleekr0125@gmail.com>\nssb   rsa4096/38DDEF7B4E6758A4 2020-10-08 [E]\n```\n\nNow let's tell git about this key:\n\n``` sh\n$ git config --global user.signingkey 037ED189F6F42EF3\n```\n\nAnd let's tell our terminal (tty) about it too!\n\n```sh\n$ test -r ~/.bash_profile && echo 'export GPG_TTY=$(tty)' >> ~/.bash_profile\n$ echo 'export GPG_TTY=$(tty)' >> ~/.profile\n```\n\n## 6. Committing with GPG Signature\n\nIt's simple - just add the `-S` option!\n\n``` sh\n$ git commit -S -m \"#IssueNumber\"\n```\n\nNow enter the password you set when creating the GPG key, and your commit will be complete with a signature!\n\n## Reference\n\n1. https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/about-commit-signature-verification","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Security"],"pubDate":"2020-10-06T15:00:00.000Z","url":"/blog/en/2020-10-08-adding-gpg-signature-to-github/"},{"id":"en/2020-11-10-github-blog-faq","title":"GitHub Blog FAQ - Common Questions and Solutions","description":"","content":"## 1. Enabling Google Search\n\n### 1.1 Activate sitemap\n\nModify your `_config.yml`:\n``` yml\nplugins: ['jekyll-paginate', 'jekyll-sitemap', 'jekyll-include-cache', 'jekyll-gist']\n```\n\n### 1.2 Add robots.txt\n\nCreate a `robots.txt` file in your root directory:\n``` txt\nUser-agent: *\nAllow:/\n\nSitemap: https://jayleekr.github.io/sitemap.xml\n```\n\n### 1.3 Register sitemap to Google Search Console\n\n1. Go to https://www.google.com/webmasters\n2. Click **SEARCH CONSOLE**\n3. Add your site and verify ownership\n4. Submit your sitemap URL\n\n## 2. Jekyll Related Issues\n\n### 2.1 Installing dependencies for local blog testing\n\nIf you want to test your Jekyll blog locally, you'll need these dependencies:\n\n``` sh\n$ sudo apt-get install -y gem \n$ sudo gem install jekyll bundler tzinfo tzinfo-data minima \n```\n\n### 2.2 Installing a Theme\n\nLet's install the minimal-mistakes theme as an example:\n\n1. First, add the minimal-mistakes theme to your Gemfile:\n\n``` sh\ngem \"minimal-mistakes-jekyll\"\n```\n\n2. Install the bundle:\n``` sh\n$ bundle install \n```\n\n3. Add the theme to your `_config.yml` file:\n``` yml\ntheme: minimal-mistakes-jekyll\n```\n\n### 2.3 Running Jekyll\n\n``` sh\n$ jekyll serve\nConfiguration file: /home/jayleekr/workspace/00_codes/07_blog/_config.yml\n            Source: /home/jayleekr/workspace/00_codes/07_blog\n       Destination: /home/jayleekr/workspace/00_codes/07_blog/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.823 seconds.\n Auto-regeneration: enabled for '/home/jayleekr/workspace/00_codes/07_blog'\n    Server address: http://127.0.0.1:4000\n  Server running... press ctrl-c to stop.\n```\n\nYour blog should now be accessible at `http://127.0.0.1:4000`!\n\n### 2.4 Jekyll Version Compatibility Issues\n\nThis happens when your project version doesn't match your installed Jekyll version.\n\nYou can resolve this by managing gem dependencies properly:\n\n``` sh\n$ sudo gem install bundler\n$ bundle install\n$ bundle exec jekyll serve\nConfiguration file: /home/jayleekr/workspace/00_codes/07_blog/_config.yml\n            Source: /home/jayleekr/workspace/00_codes/07_blog\n       Destination: /home/jayleekr/workspace/00_codes/07_blog/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.823 seconds.\n Auto-regeneration: enabled for '/home/jayleekr/workspace/00_codes/07_blog'\n    Server address: http://127.0.0.1:4000\n  Server running... press ctrl-c to stop.\n```\n\nUsing `bundle exec` ensures you're running Jekyll with the exact gem versions specified in your Gemfile.lock, avoiding compatibility issues.\n\n## Pro Tips\n\n- Always use `bundle exec jekyll serve` instead of just `jekyll serve` to avoid version conflicts\n- Keep your gems updated with `bundle update`\n- Test your blog locally before pushing to GitHub Pages\n- Use GitHub Pages supported plugins to avoid build failures\n\nHope this helps with your GitHub blog setup! 🚀","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","Jekyll","Blog"],"pubDate":"2020-11-09T15:00:00.000Z","url":"/blog/en/2020-11-10-github-blog-faq/"},{"id":"en/2020-11-16-yocto-build-system-basics","title":"Understanding the Yocto Build System","description":"","content":"## 1. Introduction\n\nIn this document, I'll organize the key concepts I studied while working with the Yocto Project's build system for building and deploying embedded Linux systems.\n\nThe Yocto Project proudly displays its \"Mega Manual\" on the homepage - and trust me, it's mega for a reason! Given the sheer volume of documentation, trying to master everything doesn't make much sense at my level. So I'll focus on the important parts and explain them with helpful diagrams.\n\nI'll kindly provide you with the link here, feel free to dive in → [mega-manual](https://www.yoctoproject.org/docs/current/mega-manual/mega-manual.html)\n\nThis contains a lot of my subjective analysis through googling, so if you want more objective analysis, please refer to the link above.\n\n## 2. A Fun Look at History\n\nThe Yocto Project has its roots in the **OpenEmbedded Project**.\n\n<center>[Image Description] Symbol of the veterans [Source: Google Image Search]</center>\n![Desktop View](/assets/img/post/2020-11-16/oe.png)\n\nThe OpenEmbedded Project is said to have started when **Sharp** (familiar to me through electronic dictionaries) released their ROM images under **Open Source license**.\n\n(Those were the good old days...)\n\n<center>[Image Description] ~~90s kids might not know this~~ [Source: Google Image Search]</center>\n![Desktop View](/assets/img/post/2020-11-16/sharp.jpeg)\n\nBased on this, the OpenZaurus Project was launched in 2002. (Zaurus was a PDA)\n\n<center>[Image Description] OpenZaurus and Sharp's Zaurus PDA [Source: Google Image Search]</center>\n\n![Desktop View](/assets/img/post/2020-11-16/Openzaurus-logo.png)\n![Desktop View](/assets/img/post/2020-11-16/Sharp_Zaurus.jpg)\n\nOver time, it evolved into the OpenEmbedded Project, which adopted Debian-based package management and build methods (Ubuntu is also Debian-based, which we're familiar with).\n\nOpenEmbedded consists of **Bitbake** - a build tool written in shell and Python scripts - and **metadata (recipes)** that specify what to build.\n\n(Bitbake also grew too large and was split into a separate project in 2004, like make)\n\nThe OpenEmbedded Project gained huge popularity among developers worldwide, supporting nearly 10,000 recipes and hundreds of machines. However, this growth made management increasingly difficult, leading to various improvement attempts.\n\nOne of these was the Poky Linux Project, started in 2003 by the embedded startup OpenedHand. This project selected only hundreds of essential recipes and gained rapid popularity by supporting virtual environment builds through QEMU and SDK builds. ~~(OpenedHand was acquired by Intel in 2008. No matter how hard I looked, I couldn't find the acquisition price...)~~\n\n<center>[Image Description] Intel's startup OpenedHand acquisition announcement. They look happy for some reason.</center>\n![Desktop View](/assets/img/post/2020-11-16/openedhand.png)\n![Desktop View](/assets/img/post/2020-11-16/oh.jpg)\n\nTime passed, and in 2010, the Linux Foundation WG announced the **Yocto Project**. The core of the project was creating embedded Linux distributions based on Poky Linux.\n\nFrom 2011, the overgrown OpenEmbedded Project was separated from **Poky Linux** as **OpenEmbedded-Core (OE-Core)**. (The previous OpenEmbedded is called **OE-Classic**)\n\nOE-Core focuses on supporting major architectures like ARM and x86, QEMU support, and even includes Sato-based GUI testing tools that run on X windows.\n\n<center>[Source: Yocto Project Homepage]</center>\n![Desktop View](/assets/img/post/2020-11-16/yoctolayer.png)\n\nThrough this long process of reducing size and splitting, it achieved the current Layered Architecture shown above, and by adopting a Pull Model instead of the previous Push model, it eliminated the possibility of project divergence.\n\n## 3. Cross-build Overview\n\nNow that you understand the history, readers who have done simple embedded development or cross-building will find Chapter 3 easy to follow.\n\nAs mentioned in the history, the Yocto Project uses Bitbake as a build tool based on Poky Linux, with metadata (recipes) as specifications, and Bitbake is the core functionality of the OpenEmbedded project.\n\nSo we need to focus on analyzing this Bitbake.\n\nBut...\n\nIt's too much to know everything about Bitbake with its long history... Let's briefly understand the important functions needed for building through simplified diagrams.\n\n### 3.1 User Configuration\n\nFirst things first - setting up the build environment.\n\nAnyone who has done cross-building knows this is the most annoying and time-consuming part, but also the most important area.\n\nLet's look at the diagram below:\n\n![Desktop View](/assets/img/post/2020-11-16/bitbake1.png)\n<center>[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]</center>\n\nThe Poky Project provides scripts to automate this part. That's the oe-init-build-env script.\n\nWhen you run the script, a Build Directory is created where all build-related work takes place.\n\nThe conf directory in the created Build directory allows **User Configuration** to be modified, which can also be changed via bitbake command line.\n\nThe information you can modify includes which of the vast metadata (recipes) to use in the build system, target machine settings, download paths for packages needed for building, cache paths, etc.\n\n### 3.2 Source Preparation\n\n![Desktop View](/assets/img/post/2020-11-16/source-fetching.png)\n[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nOnce the foundation is laid, we need to bring our own software and attach it to the Yocto Project structure.\n\nThe recipe functions that handle this part are do_fetch and do_unpack. These two functions create a Working Directory within the Build Directory and copy the actual source code there.\n\nThis structure is designed to support various architectures and OSes from the same source.\n\n### 3.3 Configuration & Compilation & Staging\n\n![Desktop View](/assets/img/post/2020-11-16/configuration-compile-autoreconf.png)\n\n[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nAfter bringing the original software source, the next process is compilation and installation.\n\nThis process will be more familiar if you know build tools like CMake or Autotool.\n\nFirst, the **do_prepare_recipe_sysroot** function places two sysroots for cross-building in the Working Directory (target **sysroot** and **sysroot-native**).\n\nThrough **do_configure**, it extracts the build configuration files needed for compilation from the **original source (S)** to the **Build Directory (B)** (equivalent to the general cmake process).\n\nThe **do_compile** process proceeds with compilation in the **Build Directory** (equivalent to the general make process).\n\nThe **do_install** process places these compiled files in the installation **target destination (D)**.\n\n### 3.4 Package Classification\n\n![Desktop View](/assets/img/post/2020-11-16/analysis-for-package-splitting.png)\n\n[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nOnce source compilation and installation are complete, it's time to decide how to package and distribute them.\n\nThe Yocto Project supports three types of distribution formats: **rpm**, **deb**, and **ipk**.\n\nThe **do_package, do_packagedata** functions split and classify the files in the installation destination D for packaging.\n\nSplitting packages can be understood by comparing it to how when we install python as an apt package in Ubuntu, it exists in various forms like python, python-dev, python-3.6, etc.\n\n### 3.5 Image Generation\n\n![Desktop View](/assets/img/post/2020-11-16/image-generation.png)\n\n[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nOnce packages are well-made, we can now use Bitbake to put them into the image's rootfs (Root file system).\n\nFirst, the do_rootfs function creates the image's Rootfs with the packages created in the above process installed.\n\nThere are several important variables in this process that are very important, so let's go over them:\n\n* IMAGE_INSTALL: Lists packages from our created package collection (Package Feeds area) to include in the image\n* PACKAGE_EXCLUDE: Lists what should not be installed\n* PACKAGE_CLASSES: Selects the package type to use (rpm, deb, ipk)\n* PACKAGE_INSTALL: Final package list to be installed in the image\n\nAfter completing package installation, you can do some post-processing work.\n\nIn post-processing, manifest files are created and specific scripts are run, mostly for testing purposes.\n\nManifest files are used for test automation not only in virtual environments like Qemu supported by Poky but also in actual target environments. (For details, see testimage*.bbclass or testsdk.class)\n\nThis would be useful for building integration tests or even system tests in the V-cycle for deployment.\n\nOnce post-processing is complete, we're finally ready to create the image to upload to the target.\n\nThe do_image function handles this role, providing do_image_* internally to handle different file systems (ext4, fat32, etc.) differently.\n\n### 3.6 SDK Generation\n\n<center>[Description: DevOps territory???..???? Is it all of it?] [Source: FastCampus]</center>\n![Desktop View](/assets/img/post/2020-11-16/devops.png)\n\nMost people think this should just naturally work, and indeed, most of the processes mentioned above are in the DevOps domain.\n\nFor feature developers, going through image generation for cross-compilation is a huge waste of time and resources, so DevOps should create an environment where they can easily compile in SDK form.\n\nBuilding this is very tedious but essential from a productivity standpoint. That's why, as mentioned in the history, the Poky Linux project also considered this and provided it as a feature.\n\n![Desktop View](/assets/img/post/2020-11-16/sdk-generation.png)\n\n[Source: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nLet's assume packaging went well in the above processes.\n\nIf you use the Yocto Project, you can easily create SDKs from these packages using do_populate_sdk or do_populate_sdk_ext.\n\nSDK installation files are typically created as /build/tmp/deploy/*.sh as shown in the diagram above.\n\nWith just that file, developers can easily set up a cross-build environment.\n\n![Desktop View](/assets/img/post/2020-11-16/cross-development-toolchains.png)\n\n## 4. Conclusion\n\nAs of the end of 2020, automotive system platforms are a hot potato that not only automotive OEMs but also global IT giants, electronics, and semiconductor companies are rushing to grab.\n\nConsidering that not only vehicles but also the IoT world has Linux-based kernels, ecosystems that provide integrated build environments based on open source projects like Yocto will become even more popular.\n\nIn fact, looking at LinkedIn, companies like VW Group, BMW Group, Hyundai Motors Group, Toyota Group, as well as IT companies including automotive semiconductor chip manufacturers, Amazon, and Facebook are hiring senior-level Build Architects or Build Engineers who are familiar with the Yocto Project.\n\nSince the Yocto project covers everything from kernel to application, you need to be proficient in at least 3 computer languages, requiring an enormous amount of study.\n\nI also found it quite challenging while studying the Yocto build system, and I hope this article helps those who are studying from scratch like me. With that, I conclude this article.","categories":["TechSavvy","Yocto"],"tags":["TechSavvy","ProgrammingLanguage","Yocto"],"pubDate":"2020-11-16T01:00:00.000Z","url":"/blog/en/2020-11-16-yocto-build-system-basics/"},{"id":"en/2020-11-17-bash-eval","title":"Bash eval - Dynamic Command Execution","description":"","content":"## What is eval?\n\nThe `eval` command takes string arguments and executes them as commands. It's super useful when you need to manipulate commands at the string level before executing them.\n\nThink of it as building commands dynamically and then running them!\n\n## Basic Usage\n\n```bash\n# Simple example\ncommand=\"ls -la\"\neval $command\n\n# Same as running: ls -la\n```\n\n## Practical Examples\n\n### 1. Dynamic Variable Names\n```bash\n# Create variables dynamically\nfor i in {1..3}; do\n    eval \"var$i=value$i\"\ndone\n\n# Now you have var1=value1, var2=value2, var3=value3\n\n# Access them dynamically\nfor i in {1..3}; do\n    eval \"echo \\$var$i\"\ndone\n```\n\n### 2. Building Complex Commands\n```bash\n# Build a command string\nsearch_term=\"*.txt\"\noptions=\"-name\"\naction=\"-exec cat {} \\;\"\n\nfind_command=\"find . $options '$search_term' $action\"\neval $find_command\n```\n\n### 3. Configuration Processing\n```bash\n# Process config-like strings\nconfig=\"USER=john PASSWORD=secret123 HOST=localhost\"\neval $config\n\necho \"Connecting to $HOST as $USER\"\n```\n\n## When to Use eval\n\n✅ **Good use cases:**\n- Dynamic variable creation\n- Building commands from user input (with proper validation!)\n- Processing configuration strings\n- Metaprogramming in shell scripts\n\n❌ **Avoid when:**\n- You can use arrays or other safer alternatives\n- Processing untrusted input without validation\n- Simple variable assignments (use direct assignment instead)\n\n## Security Warning\n\n⚠️ **BE CAREFUL!** eval executes whatever you give it, which can be dangerous:\n\n```bash\n# DANGEROUS - never do this with user input\nuser_input=\"rm -rf /\"\neval $user_input  # This would delete everything!\n```\n\nAlways validate and sanitize input before using eval!\n\n## Safer Alternatives\n\nSometimes you don't need eval:\n\n```bash\n# Instead of eval for simple cases\n# Bad:\neval \"ls $options\"\n\n# Good:\nls $options\n\n# For arrays:\n# Instead of dynamic variables, use arrays\ndeclare -a values=(\"value1\" \"value2\" \"value3\")\n```\n\n## Pro Tips\n\n1. **Quote properly**: Always quote variables to prevent word splitting\n2. **Validate input**: Never eval untrusted data\n3. **Consider alternatives**: Arrays, parameter expansion, etc.\n4. **Debug first**: Echo the command before eval to see what you're executing\n\n```bash\n# Debug pattern\ncommand=\"ls -la /tmp\"\necho \"About to execute: $command\"\neval $command\n```\n\n## References\n\n- General: [Bash Beginners Guide](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)\n\n*Use eval wisely - with great power comes great responsibility! 🚀*","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-eval/"},{"id":"en/2020-11-17-bash-get-ip-address","title":"Quick & Dirty: Getting IP Address with Bash","description":"","content":"## The Problem\n\nEver needed to grab your IP address from a bash script? Here's a quick one-liner that's saved me countless times!\n\n## The Solution\n\nIf your ethernet interface is `eth0`:\n\n```sh\n$ IPADDR=$(ifconfig eth0|grep inet|head -1|sed 's/\\:/ /'|awk '{print $2}')\n$ echo $IPADDR\n172.17.0.3\n```\n\n## How It Works\n\nLet's break this down step by step:\n\n1. `ifconfig eth0` - Gets interface information for eth0\n2. `grep inet` - Filters for lines containing \"inet\" \n3. `head -1` - Takes just the first match\n4. `sed 's/\\:/ /'` - Replaces colons with spaces for easier parsing\n5. `awk '{print $2}'` - Grabs the second field (which is our IP)\n\n## Modern Alternative\n\nIf you're on a newer system, you might prefer using `ip` instead of the older `ifconfig`:\n\n```sh\n$ IPADDR=$(ip route get 1 | awk '{print $NF;exit}')\n$ echo $IPADDR\n172.17.0.3\n```\n\nThis approach is more reliable because it gets the IP of the interface that would be used to reach the internet.\n\n## Pro Tips\n\n- Always test your scripts in your specific environment - network interface names can vary!\n- For production scripts, add error checking to make sure the interface exists\n- Consider what happens if the interface is down or doesn't have an IP\n\n## Appendix: References\n\n- General Bash Guide: [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)\n\n*Sometimes the simplest solutions are the best ones! 🚀*","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-get-ip-address/"},{"id":"en/2020-11-17-bash-directory-exists","title":"Bash: How to Check if Directory Exists","description":"","content":"## The Problem\n\nNeed to check if a directory exists before doing something with it? Here's how to do it properly in Bash!\n\n## Basic Directory Check\n\n```bash\n#!/bin/bash\n\ntest_dir_exist(){\n    set -e \n    if [ -e \"/home/jayleekr/workspace/00_codes/05_info_archive\" ]; then\n        echo \"DIR Exist\"\n        exit 1\n    fi\n}\n\ntest_fail(){\n    echo \"test_fail\"\n}\n\ntest_dir_exist\n```\n\nRunning this:\n```bash\n$ ./directory_ex.sh\nDIR Exist\n```\n\n## Better Approaches\n\n### 1. Using -d for Directory-Specific Check\n```bash\n#!/bin/bash\n\ncheck_directory() {\n    local dir_path=\"$1\"\n    \n    if [ -d \"$dir_path\" ]; then\n        echo \"✅ Directory '$dir_path' exists\"\n        return 0\n    else\n        echo \"❌ Directory '$dir_path' does not exist\"\n        return 1\n    fi\n}\n\n# Usage\ncheck_directory \"/home/user/workspace\"\n```\n\n### 2. Create Directory if It Doesn't Exist\n```bash\n#!/bin/bash\n\nensure_directory() {\n    local dir_path=\"$1\"\n    \n    if [ ! -d \"$dir_path\" ]; then\n        echo \"Creating directory: $dir_path\"\n        mkdir -p \"$dir_path\"\n    else\n        echo \"Directory already exists: $dir_path\"\n    fi\n}\n\n# Usage\nensure_directory \"/tmp/my_project/logs\"\n```\n\n### 3. Multiple Directory Checks\n```bash\n#!/bin/bash\n\ncheck_multiple_dirs() {\n    local dirs=(\"$@\")\n    local all_exist=true\n    \n    for dir in \"${dirs[@]}\"; do\n        if [ ! -d \"$dir\" ]; then\n            echo \"❌ Missing: $dir\"\n            all_exist=false\n        else\n            echo \"✅ Found: $dir\"\n        fi\n    done\n    \n    if [ \"$all_exist\" = true ]; then\n        echo \"🎉 All directories exist!\"\n        return 0\n    else\n        echo \"💥 Some directories are missing!\"\n        return 1\n    fi\n}\n\n# Usage\ncheck_multiple_dirs \"/tmp\" \"/home\" \"/var/log\" \"/nonexistent\"\n```\n\n## Different Test Operators\n\nHere are the different ways to test for files and directories:\n\n```bash\n# -e: exists (file or directory)\n[ -e \"/path\" ] && echo \"Something exists at /path\"\n\n# -f: regular file exists\n[ -f \"/path/file.txt\" ] && echo \"File exists\"\n\n# -d: directory exists\n[ -d \"/path/dir\" ] && echo \"Directory exists\"\n\n# -r: readable\n[ -r \"/path\" ] && echo \"Readable\"\n\n# -w: writable\n[ -w \"/path\" ] && echo \"Writable\"\n\n# -x: executable\n[ -x \"/path\" ] && echo \"Executable\"\n```\n\n## Practical Examples\n\n### 1. Backup Script\n```bash\n#!/bin/bash\n\nBACKUP_DIR=\"/backup/$(date +%Y%m%d)\"\n\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Creating backup directory: $BACKUP_DIR\"\n    mkdir -p \"$BACKUP_DIR\"\nfi\n\necho \"Backing up to: $BACKUP_DIR\"\n# Your backup commands here...\n```\n\n### 2. Project Setup\n```bash\n#!/bin/bash\n\nsetup_project() {\n    local project_name=\"$1\"\n    local project_dirs=(\"$project_name/src\" \"$project_name/tests\" \"$project_name/docs\")\n    \n    for dir in \"${project_dirs[@]}\"; do\n        if [ ! -d \"$dir\" ]; then\n            echo \"Creating: $dir\"\n            mkdir -p \"$dir\"\n        fi\n    done\n    \n    echo \"Project structure created for: $project_name\"\n}\n\n# Usage\nsetup_project \"my_awesome_app\"\n```\n\n### 3. Log Rotation Check\n```bash\n#!/bin/bash\n\nLOG_DIR=\"/var/log/myapp\"\nARCHIVE_DIR=\"/var/log/myapp/archive\"\n\nif [ -d \"$LOG_DIR\" ]; then\n    echo \"Log directory exists\"\n    \n    # Create archive directory if needed\n    [ ! -d \"$ARCHIVE_DIR\" ] && mkdir -p \"$ARCHIVE_DIR\"\n    \n    # Rotate logs\n    find \"$LOG_DIR\" -name \"*.log\" -mtime +7 -exec mv {} \"$ARCHIVE_DIR\" \\;\nelse\n    echo \"Log directory not found: $LOG_DIR\"\n    exit 1\nfi\n```\n\n## Pro Tips\n\n1. **Always quote variables**: `[ -d \"$dir\" ]` not `[ -d $dir ]`\n2. **Use -d for directories**: More specific than -e\n3. **Handle spaces in paths**: Quoting protects against spaces\n4. **Exit codes matter**: Return meaningful exit codes for scripting\n5. **Create parent directories**: Use `mkdir -p` to create nested directories\n\n## Common Pitfalls\n\n```bash\n# ❌ Don't do this\nif [ -d $HOME/my dir ]; then  # Breaks with spaces!\n\n# ✅ Do this instead\nif [ -d \"$HOME/my dir\" ]; then  # Properly quoted\n\n# ❌ Don't do this\nif [ -e \"$dir\" ]; then  # Could be a file!\n\n# ✅ Do this for directories\nif [ -d \"$dir\" ]; then  # Specifically checks for directory\n```\n\n## References\n\n- General: [Bash Beginners Guide](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)\n- Test operators: `man test` or `help test`\n\n*Directory checks are fundamental - get them right and your scripts will be much more robust! 🚀*","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-directory-exists/"},{"id":"en/2020-11-17-bash-arrays","title":"Working with Bash Arrays","description":"","content":"## array(list)\n\nIn Bash, you can represent arrays using parentheses.\nThe delimiter inside is the space bar - pretty simple!\n\n``` sh\n$ cat array_ex.sh\n#!/bin/bash\nlists=(\"a\" b \"c\")\necho ${lists[1]}\necho ${lists[0]}\necho ${lists[3]}\necho ${lists[2]}\n$ sh array_ex.sh\nb\na\n\nc\n```\n\nNotice how `lists[3]` returns empty since we only have 3 elements (indices 0, 1, 2). Bash won't complain - it just gives you nothing, which is actually quite handy for conditional checks.\n\nSlicing is also supported, as shown in the example below:\n\n``` sh\nlists=(\"V0.1.0\" \"V1.0.0\")\necho \"[1] : \"${lists[1]}\necho \"[0] : \"${lists[0]}\necho \"[3] : \"${lists[3]}\necho \"[-1] : \"${lists[-1]}\n\nselected=${lists[-1]}\necho \"selected : \"$selected\n``` \n\nThe negative indexing `[-1]` is pretty cool - it gives you the last element, just like in Python! This makes it super easy to grab the most recent version or the last item without having to calculate the array length.\n\nPro tip: When working with arrays in Bash, always remember that they're zero-indexed and space-delimited. If you need comma-separated values, you'll need to handle that separately or use a different approach.\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-arrays/"},{"id":"en/2020-11-17-bash-sed","title":"Bash sed - The Text Processing Legend","description":"","content":"### 6. sed\n\nThere's a legendary saying: \"There might be people who haven't used the sed utility, but there's no one who has used it only once.\"\n\nAnd honestly? That's so true! Once you discover the power of `sed` (stream editor), you'll find yourself reaching for it constantly. It's like that Swiss Army knife of text processing - incredibly versatile and indispensable once you get the hang of it.\n\nWhether you're doing simple find-and-replace operations, deleting lines, inserting text, or more complex pattern matching and substitutions, `sed` handles it all with elegance. The learning curve might be a bit steep at first, but trust me, it's worth every minute you spend mastering it.\n\nThe beauty of `sed` is that it works on streams of text, making it perfect for processing files, pipeline operations, and automating text transformations. Once you start using it in your daily workflow, you'll wonder how you ever lived without it!\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-sed/"},{"id":"en/2020-11-17-bash-set-utility","title":"Bash set Utility Guide","description":"","content":"## set \n\n### set -e\n\nWhen `set -e` is executed within a script, the shell environment will stop executing subsequent commands if any script command fails with an error.\n\nThis is super handy when you want your script to fail fast instead of continuing with potentially broken state. Think of it as your script's safety net!\n\n### exit code\n\nGenerally in Unix systems, 0 means success, and 1~255 are recognized as error codes.\nThe recognized range is 0~255 (8-bit, not 16-bit as originally stated).\nYou can check the result value with `$?`.\n\n``` sh\n$ cat test.sh\necho \"hello\"\nexit 100\n$ sh test.sh\nhello\n$ echo $?\n100\n```\n\nPretty straightforward, right? The script says \"hello\" and then exits with code 100, which you can immediately check with `$?`.\n\n### set -x\n\nWhen `set -x` is executed within a script, the shell environment will show all shell commands in verbose mode.\n\nThis is basically your debugging best friend! It'll print out every command before executing it, so you can see exactly what's happening step by step. Super useful when your script isn't behaving the way you expect.\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-set-utility/"},{"id":"en/2020-11-17-bash-string-comparison","title":"Bash String Comparison with if","description":"","content":"## Bash string comparison using \"if\"\n\n\"==\" and \"!=\" only can be used in case of string comparison.\n\n``` sh\nif [ \"$STRING\" == \"abc\" ];then\n    echo \"STRING is abc!\"\nfi \n\nor\n\nif [ \"$STRING\" = \"abc\" ];then\n    echo \"STRING is abc!\"\nfi \n```\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/en/2020-11-17-bash-string-comparison/"},{"id":"en/2020-11-27-cv-qualifiers-explained","title":"CV-qualifiers Explained - Understanding const and volatile","description":"","content":"## CV-qualifiers\n\n### 0. Preface\n\nLet's talk about CV-qualifiers! `const` is for expressing constants, and `volatile` is for expressing volatile (changeable) types.\n\nIn the STL, `const` and `volatile` together are defined as CV-qualifiers. [Check it out here!](https://en.cppreference.com/w/cpp/language/cv)\n\n### 1. Notation\n\nIn C++, type qualifiers like CV-qualifiers can appear on both the left and right sides of a type.\n\nLet me show you what I mean:\n\n``` cpp\nconst int i = 100;\nint const i = 100;\n```\n\nIf you're coming from other programming languages, this might be confusing because `const` is usually written to the left of the type.\n\nIn C and C++, both expressions above are correct and identical!\n\n> Here's the key rule: `const` modifies from right to left, and only when there's nothing to the left does it modify from left to right.\n\nWhy did they design it this way?\n\n#### Readability Benefits\n\nWhile the examples above were identical, things get different when dealing with pointers:\n\n``` cpp\nconst char *const s = \"aaa\";\nchar const *const s = \"aaa\";\n```\n\nBoth expressions are the same, but notice how we have two `const` keywords mixed in. In the first example, one `const` modifies from left to right, and the other modifies from right to left.\n\nWhen expressions become complex, consistently placing `const` on the right (right-to-left modification) makes for much better readability.\n\nThink of it this way:\n- `char const *const s` reads as \"s is a const pointer to const char\"\n- Reading right to left: \"s is const, pointing to char that is const\"\n\nThis convention becomes especially helpful when you're dealing with more complex pointer declarations. By always putting `const` on the right side of what it modifies, you can read the declaration from right to left in a consistent manner.\n\n### Pro Tips for CV-qualifiers\n\n1. **Be Consistent**: Choose either left-side or right-side `const` and stick with it throughout your codebase\n2. **Right-to-Left Reading**: Many C++ experts prefer right-side `const` because it reads more naturally from right to left\n3. **Team Standards**: Follow your team's coding standards - consistency matters more than personal preference\n\nUnderstanding CV-qualifiers properly will make you a better C++ programmer and help you write more maintainable code! 🎯","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++"],"pubDate":"2020-11-26T15:00:00.000Z","url":"/blog/en/2020-11-27-cv-qualifiers-explained/"},{"id":"en/2020-11-27-understanding-constexpr","title":"Understanding constexpr - Compile-time vs Runtime Evaluation","description":"","content":"## constexpr\n\n### 0. Preface\n\nI'll be honest - `constexpr` used to keep me up at night! It was so confusing that I couldn't stand it anymore, so I decided to write this comprehensive guide.\n\n`constexpr` is supported in Modern C++ (C++11 and above). The specification of `constexpr` has been evolving as the STL versions progress.\n\nAfter looking at various lecture materials and use cases, it seems like everyone uses it differently and understands it differently. While the original author of `constexpr` certainly had a clear intention, it feels quite different from the initial concept now.\n\nHowever, there's one thing everyone commonly asks:\n\n**\"How is it different from const?\"**\n\n`const` is simply constant - once compiled, the data becomes immutable during runtime.\n\n`constexpr` has a similar intention. Programmers use it to ensure that variables or functions are determined at compile time.\n\nBut here's where `constexpr` gets a bit ambiguous: variables or functions declared with `constexpr` can be determined either during compilation OR during runtime! \n[cpp reference c++1x constexpr](https://en.cppreference.com/w/cpp/language/constexpr)\n\nLet's dive into some examples to explore this ambiguity...\n\n### 1. Regular Example\n\n``` cpp\n#include <iostream>\nint fibonacci(int n){\n    if (n >= 2) \n        return fibonacci(n-1) + fibonacci(n-2);\n    else\n        return n;\n}\n\nint main(){\n    std::cout << fibonacci(10) << '\\n';\n}\n```\n\nHere's a simple Fibonacci sequence calculator.\n\nThis code calculates the value at runtime after compilation.\n\n``` sh\n$ /usr/bin/time ./fibonacci \n102334155\n0.56user 0.00system 0:00.56elapsed 100%CPU (0avgtext+0avgdata 3324maxresident)k\n0inputs+0outputs (0major+126minor)pagefaults 0swaps\n```\n\nSince the value is calculated at runtime, it takes quite a bit of time.\n\n### 2. Template Approach\n\nWhat if we want the efficiency of compile-time calculation? Let's try C++ [Template Meta Programming](https://en.wikipedia.org/wiki/Template_metaprogramming):\n\n``` cpp\n#include <iostream>\n\ntemplate <int N>\nstruct fibonacci\n{\n    static int64_t const value = fibonacci<N-1>::value + fibonacci<N-2>::value;\n};\n\ntemplate<>\nstruct fibonacci<0>\n{\n    static int64_t const value = 0;\n};\n\ntemplate<>\nstruct fibonacci<1>\n{\n    static int64_t const value = 1;\n};\n\nint main(){\n    std::cout << fibonacci<40>::value << '\\n';\n}\n```\n\nUsing C++ template techniques, the compiler internally generates code and does the work at compile time.\n\nSo **fibonacci&lt;40&gt;::value** is determined at compile time.\n\n``` sh\n$ /usr/bin/time ./fibonacci_template \n102334155\n0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 3388maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n```\n\nSince the value was calculated at compile time, the execution time is super fast!\n\n### 3. constexpr\n\nSo what happens when we use **constexpr**?\n\n``` cpp\n#include <iostream>\nconstexpr int fibonacci(int n){\n    return n>=2 ? fibonacci(n-1) + fibonacci(n-2): n;\n}\n\ntemplate<int N>\nstruct constN{\n    constN(){ std::cout << N << '\\n';}\n};\n\nint main(){\n    constN<fibonacci(40)> a; // Compile time\n    //std::cout << fibonacci(40) << '\\n';  //Runtime\n}\n```\n\n``` sh\n$ /usr/bin/time ./fibonacci_constexpr \n102334155\n0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 3412maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n```\n\nAgain, fast execution because the value was calculated at compile time!\n\nHere's the interesting part: **constexpr** can be used in both forms.\n\nIf I uncomment the line in the same code above, the value gets determined at runtime and takes much longer:\n\n``` sh\n$ /usr/bin/time ./fibonacci_constexpr \n102334155\n0.55user 0.00system 0:00.55elapsed 99%CPU (0avgtext+0avgdata 3376maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n```\n\n### 4. Conclusion\n\nAs we've seen, `constexpr` can be used at both compile time and runtime.\n\nIf the `constexpr` requirements aren't met during compilation, it automatically falls back to runtime calculation.\n\nUsing `constexpr` gives you the best of both worlds:\n- You can have values determined at compile time for efficiency\n- You can still use debuggers to inspect runtime values at breakpoints\n\n**My conclusion**: `constexpr` is a technique that shows the programmer's intention that a value *could* be determined at compile time, but doesn't guarantee it will be.\n\nIt's like saying \"Hey compiler, if you can figure this out at compile time, please do. If not, that's okay too - just calculate it at runtime.\"\n\nPretty neat, right? 🚀","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++"],"pubDate":"2020-11-26T15:00:00.000Z","url":"/blog/en/2020-11-27-understanding-constexpr/"},{"id":"en/2020-12-31-year-end-retrospective","title":"2020 Year-End Retrospective","description":"","content":"![Desktop View](/assets/img/autron2.jpg)\n\n## 2020 Year-End Retrospective\n\nSomeone once said that assistant managers and managers are when you're most productive and do the most work...\n\nI blinked and suddenly Q4 was here. Whoa!\n\nDuring my 4.5 years of graduate school, I focused on theoretical research. During my 3 years of military service, I did work that demanded maximum execution for users. Now I'm doing platform development that requires both aspects.\n\nLet me talk more specifically about what I do now: Adaptive AUTOSAR platform, commonly called AP middleware, which is a very important component that goes into high-performance computing chips that must be installed in vehicles.\n\nThe main functions include automatically generating structured code so that vehicle application developers can easily develop according to design specifications, providing interfaces for these vehicle applications to be remotely updated via OTA, and providing various interfaces to behave differently according to different vehicle situations - all very important things to prepare for the coming autonomous driving era.\n\nActually, European companies are typically good at designing and developing these software functions with long-term, abstract vision, and they're leading this development.\n\nTraditional European and Japanese vehicle manufacturers have considered it very important and common sense to coexist with the ecosystem for vehicle manufacturing (commonly called n-tier suppliers).\n\nTesla, this mutant, appeared and shook all of this up.\n\nConsumers and the market chose Tesla, and the power of the market was truly terrifying.\n\nThe key is approaching cars from an agile software development methodology perspective.\n\nThey dramatically shortened the cycle of design → development → verification, and made it possible to update these changed or newly added features.\n\nAnyone can say this - it sounds easy.\n\nBut...\n\n## The Reality of Automotive Software\n\nThe reality is much more complex than it sounds. The automotive industry has decades of established processes, safety standards, and regulatory requirements that you can't just ignore because you want to be \"agile.\"\n\n### What I Learned in 2020\n\n**1. Platform Thinking is Everything**\nWorking on Adaptive AUTOSAR taught me that the most important skill in modern software development isn't coding - it's platform thinking. How do you create abstractions that make complex things simple for other developers?\n\n**2. Legacy Systems are Both Blessing and Curse**\nThe automotive industry's established n-tier supplier ecosystem isn't just bureaucracy - it's decades of hard-learned lessons about safety and reliability. But it's also what makes rapid innovation so difficult.\n\n**3. The Tesla Effect is Real**\nEvery automotive meeting in 2020 somehow referenced Tesla. Not always positively, but they've fundamentally changed customer expectations about what a car should be able to do.\n\n### Technical Deep Dive: What is Adaptive AUTOSAR?\n\nFor those not familiar, Adaptive AUTOSAR (AP) is basically the operating system for next-generation automotive computing platforms. Think of it as:\n\n- **Application Framework**: Provides standardized APIs for vehicle applications\n- **Communication Middleware**: Handles service-oriented communication between applications  \n- **OTA Update Manager**: Enables secure over-the-air software updates\n- **Resource Management**: Manages compute, memory, and network resources\n- **Security Framework**: Implements automotive cybersecurity standards\n\nThe challenge is building something that's:\n- Safe enough for automotive (ISO 26262 functional safety)\n- Secure enough for connected vehicles (ISO 21434 cybersecurity)\n- Fast enough for real-time applications\n- Flexible enough for OTA updates\n- Standard enough for ecosystem interoperability\n\n## The European vs Silicon Valley Approach\n\n**European Approach**: Comprehensive standards, long development cycles, extensive validation\n- Pros: Safety, reliability, ecosystem compatibility\n- Cons: Slow to market, expensive, less innovative\n\n**Silicon Valley Approach**: Move fast, break things, iterate quickly\n- Pros: Rapid innovation, customer focus, cost efficiency  \n- Cons: Potential safety issues, regulatory challenges, ecosystem disruption\n\nThe winning approach probably combines both: \"Move fast with safety rails.\"\n\n## Personal Growth in 2020\n\nThis year I learned that being a platform engineer means:\n\n1. **Thinking in Systems**: Every decision affects multiple stakeholders\n2. **Balancing Abstractions**: Too abstract = unusable, too concrete = inflexible\n3. **Managing Complexity**: Hide complexity from users without hiding important details\n4. **Future-Proofing**: Build for today's needs and tomorrow's unknowns\n\n## What 2021 Holds\n\nThe automotive industry is at an inflection point. Software-defined vehicles aren't just a buzzword - they're becoming reality. The question is whether traditional automotive companies can transform fast enough, or whether tech companies will eat their lunch.\n\nMy prediction: We'll see more partnerships between traditional OEMs and tech companies, more in-house software development at car companies, and more automotive engineers learning to think like software engineers.\n\n## Closing Thoughts\n\n2020 taught me that the most interesting problems exist at the intersection of different industries. Automotive + Software + AI + Connectivity creates challenges that pure software companies and pure automotive companies can't solve alone.\n\nThe future belongs to engineers who can bridge these worlds.\n\n*Here's to a 2021 full of interesting technical challenges and maybe fewer Zoom meetings! 🚗💻*","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2020-12-30T15:00:00.000Z","url":"/blog/en/2020-12-31-year-end-retrospective/"},{"id":"en/2021-04-06-understanding-l-r-values-and-move-semantics","title":"Understanding L-Values, R-Values & Move Semantics in Modern C++","description":"","content":"## Understanding L-Values, R-Values & Move Semantics in Modern C++\n\n### 0. Preface\n\nBefore we dive in, let's quickly refresh our understanding of references (&) and pointers (*).\n\n> References (&) store the address of an object located somewhere in memory, just like pointers (*).\n> However, once a reference is initialized, it cannot be changed to reference another object or be set to null.\n\nValues can be classified into two types: l-values and r-values.\nThe definitions of l-values and r-values (left-hand values and right-hand values) come from the C language.\n\n> Left-hand values are expressions that can appear on either the left or right side of an assignment, while right-hand values are expressions that can only appear on the right side of an assignment.\n\n```cpp\nint a = 0;  \na; // l-value\n0; // r-value\nPlayer player;\nplayer; // l-value\nPlayer(); // r-value\n```\n\nThis definition has evolved somewhat differently in C++.\n\n> L-values refer to some memory location and can be referenced with the & operator. R-values are anything that's not an l-value.\n> L-values persist beyond the expression they appear in\n> R-values don't persist beyond the expression they appear in\n> L-values reference named variables (&)\n> R-values reference temporary objects (&&)\n\nHonestly, even this definition is quite ambiguous, which confuses many people.\n\nLet's take a closer look at l-values and r-values step by step...\n\n### 1. L-Values\n\nL-values are relatively straightforward.\n\nHere's what qualifies as an l-value:\n- **Objects that persist beyond a single expression!**\n  - Has an address\n  - Named variables\n  - const variables\n  - Array variables\n  - Bit-fields\n  - Unions\n  - Class members\n  - Function calls that return an l-value reference (&)\n  - String literals\n\nSo how do l-value assignment and referencing work?\n\nLet's look at the code and comments below.\n\n```cpp\n// All are l-values\nint a = 1;                    // global variable a -> l-value\nint& function(){              // l-value function is initialized with the address of returned a\n    a = 3;                    // a-> l-value, can be assigned  \n    return a;\n}\n\nint main()\n{\n    int i = 3;                // i is l-value\n    i = 4;                    // i is l-value, can be assigned\n                              // i -> 4\n    int *ptr = &i;            // i is l-value, & operator can reference it, ptr points to i's address\n                              // *ptr is i's value -> 4\n    ptr = &a;                 // ptr is an int * type pointer, so it can change what it references\n                              // *ptr is a's value -> 1\n    int & r = i;              // r is initialized with l-value i's address, so r is an l-value assigned with l-value\n    r = 5;                    // writes value to the address r points to, changing i's value (both r and i are 5)\n                              // both r and i point to the same address -> 5\n    int c = function();       // l-value reference stores its address only during initialization\n                              // c -> function's return value after internal stack operation, which is changed a value 3\n    function() = 50;          // function is l-value, r-value 50 can be assigned\n                              // a's value changes to 50. \n                              // and ptr points to a's address, so *ptr -> 50 is assigned.\n    int d = a;                // l-value d is assigned a's value 50\n    int *ptr_2 = &function(); // int * type ptr_2 can use &function (function is l-value)\n                              // function's return value after internal stack operation, which is changed a value 3\n                              // * ptr_2 -> changed a's value -> 3\n                              // c's value also changes -> 3\n                              // * ptr's value also changes -> 3\n    return 0;\n}\n```\n\nL-value assignment is relatively clear and easy to understand as shown above.\n\n### 2. R-Values\n\nBut r-values are somewhat harder to define...\n\nHere's what we can say about them:\n\n- Objects that are not l-values... (contrapositive statement haha..)\n- **Temporary values that don't persist beyond the single expression they're used in!**\n  - Objects without addresses\n  - Literals (except string literals)\n  - Function calls that don't return by reference (e.g. int function())\n  - i++ and i-- (but ++i and --i operators are l-values)\n  - Basic arithmetic, logical, comparison expressions (+,-,*,=, &lt;,&gt; etc, can vary with operator overloads)\n  - Enumerations (enum)\n  - Lambdas (the compiler treats lambdas/anonymous functions as temporary)\n\nLet's look at some examples.\n\nR-value example 1:\n\n```cpp\nint num1 = 10;\nint num2 = 15;\n\nif (num1 < num2) // (num1 < num2) is r-value\n{\n    ...\n}\n```\n\nR-value example 2:\n\n```cpp\nint function();         // function doesn't persist after call -> r-value \nfunction();             // r-value\nint i = 0;\ni = function();         // i is l-value assigned with r-value function's return value\nint *ptr = &function(); // ERROR!!! Can't reference address of r-value\n```\n\nThe original C/C++ designers defined r-values as temporary objects, so by default, r-values cannot be assigned to.\n\nBut this changed with the introduction of r-value \"**move semantics**\" in Modern C++ (C++11).\n\n### 3. Move Semantics\n\nConsider the following scenario:\n\n```cpp\n//Math.cpp\nstd::vector<float> Math::ConvertToPercentage(const std::vector<float>& scores){\n    std:vector<float> percentages;\n    for (auto& score : scores){\n        //...\n    }\n    return percentages;\n}\n// main.cpp\n#include \"Math.h\"\nint main(){\n    std::vector<float> scores;\n    //...\n    scores = ConvertToPercentage(scores); \n    //...\n}\n```\n\nWhen `ConvertToPercentage(scores)` is called in main,\nthis function in the Math class creates an r-value that's a temporary value of `std::vector<int>` type called percentages.\nThen it gets assigned to scores by the assignment operator \"=\".\n\nHow would this work in pre-C++11 (before Modern C++)?\n\nWhen percentages' temporary value gets **assigned** to the memory area created during scores' initialization, it gets **copied**.\nThen the temporary r-value disappears as we exit the Math::ConvertToPercentage function stack.\n\n> There's a quite unnecessary process here - the moment when percentages temporary value gets **copied**.\n\nThink about it logically.\n\nIf we simply **swap** the memory area storing the temporary percentages result (r-value) with the memory area created during scores' initialization,\nsince the temporarily created percentages memory area will disappear anyway when we exit the stack, the \"**copying**\" process becomes unnecessary.\n\nThis method of preventing unnecessary **copying** is the core of Modern C++'s r-value references and Move Semantics.\n\n> Actually, modern compilers are pretty smart about this nowadays... Sometimes overusing r-value references can actually slow down the system. 😅\n\n### 4. R-Value References (&&) and std::move\n\nR-value references were first introduced in Modern C++ (C++11).\nThey function similarly to the & operator.\n\nThe example below shows referencing basic types using move semantics:\n\n```cpp\n#include <memory>\n#include <utility>\nfloat CalculateAverage(){\n    float average=3;\n    // ...\n    return average;\n}\n\nint main(){\n    int num = 10;                           // num -> l-value\n    //int && rNum = num;                      // Error!!! num is l-value -> int & lnum is possible\n    int && rNum = std::move(num);           // move allows referencing l-value num\n    num = 30;                               // changing num also changes rNum (because it's a reference value)\n    \n    int && rNum1 = 10;                       // OK, 10 is r-value\n\n    float && rAverage = CalculateAverage();   // OK CalculateAverage is r-value\n    float tmp_1 = std::move(rAverage);       // tmp_1 is l-value, so it's assigned\n    float tmp_2 = rAverage;                  // tmp_2 is l-value, so it's assigned\n    float && tmp_3 = std::move(rAverage);    // tmp_3 is l-value, but it's an r-value reference value\n    rAverage = 1.0f;                         // changing rAverage only changes tmp_3, tmp_1 and tmp_2 are assigned values\n}\n```\n\n## Appendix.A References\n\n1. [https://m.blog.naver.com/yoochansong/222082508401](https://m.blog.naver.com/yoochansong/222082508401)\n2. [https://docs.microsoft.com/ko-kr/cpp/cpp/references-cpp?view=msvc-160&viewFallbackFrom=vs-2019](https://docs.microsoft.com/ko-kr/cpp/cpp/references-cpp?view=msvc-160&viewFallbackFrom=vs-2019)\n3. [https://skstormdummy.tistory.com/entry/%EC%9A%B0%EC%B8%A1-%EA%B0%92-%EC%B0%B8%EC%A1%B0-RValue-Reference](https://skstormdummy.tistory.com/entry/%EC%9A%B0%EC%B8%A1-%EA%B0%92-%EC%B0%B8%EC%A1%B0-RValue-Reference)\n4. [https://modoocode.com/189](https://modoocode.com/189)","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++","l-value","r-value","move"],"pubDate":"2021-04-05T15:00:00.000Z","url":"/blog/en/2021-04-06-understanding-l-r-values-and-move-semantics/"},{"id":"en/2021-04-07-preparing-for-gdb-debugging","title":"Preparing for GDB Debugging","description":"","content":"Once your toolchain is ready, you can consider all the tools prepared.\n\nNow let's make the actual target you need to debug ready for debugging.\n\n## Adjusting Compiler Options\n\n![Desktop View](/assets/img/02-PreparingToDebugWithGDB/1.png)\nRef: Linux Foundation 2020 Conference   \n\nFirst, when compiling C or C++ source code, add the -g option or -gN option as shown in the diagram above to include various information like source code debugging information in the ELF file.\n\nUsing -ggdbN will create an ELF using the gdb format instead of the standard DWARF format.\n\nGenerally, just adding -g will give the ELF a level equivalent to -g2.\n\n![Desktop View](/assets/img/02-PreparingToDebugWithGDB/2.png)\nRef: Linux Foundation 2020 Conference   \n\nNext, you need to adjust the Optimization Options.\n\nWhen optimization is enabled, the compiler analyzes the code logic and might generate different execution code that produces the same results, so you should remove optimization options during debugging.\n\nUse the -O0 option or give -Og for GDB-compatible optimization options.\n\n## Reference\n\n1. [http://www.epnc.co.kr/news/articleView.html?idxno=48128](http://www.epnc.co.kr/news/articleView.html?idxno=48128)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-06T15:00:00.000Z","url":"/blog/en/2021-04-07-preparing-for-gdb-debugging/"},{"id":"en/2021-04-08-remote-debugging-with-gdbserver","title":"Remote Debugging Using GDB Server","description":"","content":"This post covers how to do remote debugging using the debugger tools gdbserver and gdb.\n\nRemote debugging is a method where you run gdbserver on the target where the application is actually running, and debug using gdb and toolchain from a remote host.\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/1.png)\nRef. Linux Foundation Conference 2020\n\nThis method involves running an application on the target with optimizations and debugging symbols removed through gdbserver listening, while the remote host connects to gdbserver using gdb with an application that includes optimizations and debugging symbols through the toolchain for debugging.\n\nWhen the target is an embedded board, disk memory capacity and system memory capacity are very limited, making remote debugging useful in such cases.\n\n# **Breaking into Examples**\n\n- Before the detailed explanation, I'll assume you basically know how to use gdb from the command line to some extent.\n\n## **0. Setting up Environment**\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/2.png)\n\nThe diagram above shows a situation where two VS Code instances each have Docker containers running.\n\n- Host IP: 172.25.125.2\n- Target IP: 172.25.125.3\n\n## **1. Printing Hello World**\n\nThe test scenario we want to test is as follows:\n\n1. Compile ***Helloworld.cpp*** for the target on the host and send it to the target\n    - ***Not Stripped, with debug info, no optimization***\n2. Connect GDBServer and GDB\n3. Real-time debugging of the application on the target from the host via network\n\nThe ***Helloworld.cpp*** test code is as follows:\n\n```cpp\n#include <iostream>\n#include <vector>\n\nint main(){\n    std::vector<int> intVector {1,2,3,4,5,6,7};\n    \n    for (auto i : intVector){\n        std::cout << \"Hello World : \"<< i << std::endl;\n    }\n    return 0;\n}\n```\n\n### 1. Compile ***Helloworld.cpp*** for the target on the host and send it to the target\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/3.png)\n\n### 2. Connect GDBServer and GDB\n\n- ***[TARGET]*** First, let's start GDBServer on the target to put it in listening state\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/4.png)\n\n- *[HOST]* Run GDB and connect to GDBServer over the network\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/5.png)\n\n- *[HOST, TARGET]* Successfully connected screen\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/6.png)\n\n### 4. Real-time debugging of the application on the target from the host via network\n\n- *[HOST]* Set a breakpoint at the main function and check\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/7.png)\n\n- *[HOST]* Execute\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/8.png)\n\n    - The run command is not available in gdb connected remotely\n    - When you proceed line by line with 'n' from the HOST, you can see messages from std::cout appearing on stdout on the TARGET\n\n## **2. Attaching to Running Applications**\n\nBesides starting an application from scratch using GDBServer, you can also attach to applications that are already running just-in-time.\n\nI'll explain using an example that runs in an infinite loop like the one below:\n\n```cpp\n//running_app.cpp\n#include <iostream>\n#include <vector>\n#include <thread>\n#include <chrono>\n\nint main(){\n    using namespace std::chrono_literals;\n    while (true){\n        std::vector<int> intVector {1,2,3,4,5,6,7};\n        for (auto i : intVector){\n            std::cout << \"Hello World : \"<< i << std::endl;\n            std::this_thread::sleep_for(1s);\n        }\n    }\n    \n    return 0;\n}\n```\n\nThe above example will repeatedly output \"Hello World : 1 ~ 7\" to stdout every second.\n\n*[TARGET]* First, let's move the built application above to the target and run it.\n\n- running_app is running with pid=25685 assigned.\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/9.png)\n\n***[TARGET]*** Let's attach gdbserver to running_app\n\n- When listening starts normally, you'll see logs like below\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/10.png)\n\n***[HOST]*** Connect to the TARGET with gdb as follows\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/11.png)\n\n- The TARGET's operation pauses as soon as you connect\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/12.png)\n\n- Looking at the logs, the connection point was when it was executing\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/13.png)\n\n    ```cpp\n    std::this_thread::sleep_for(1s);\n    ```\n\n    - Symbol tracing is impossible because there's no source code for the nanosleep.c side\n\n***[HOST]*** You can proceed with line-by-line debugging using n or s commands\n\n- n command (next)\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/14.png)\n\n- s command (step into)\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/15.png)\n\n***[HOST]*** You can make it resume normal operation through detach\n\n- It resumes normal operation from the last point where you were debugging\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/16.png)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-07T15:00:00.000Z","url":"/blog/en/2021-04-08-remote-debugging-with-gdbserver/"},{"id":"en/2021-04-09-remote-debugging-with-vscode","title":"Remote Debugging with VS Code","description":"","content":"## Preface\n\nThis post is a continuation of the following post:\n\n- [Remote Debugging Using Gdbserver]()\n- I'm also planning to write about VS Code's Remote-Development feature (to be uploaded later)\n\nWe'll continue using the environment that was set up in [Remote Debugging Using Gdbserver]().\n\n## 0. Setting up Environment\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/1.png)\n\nThe image above shows two VS Code instances, each running Docker containers.\n\n- Host IP: 172.25.125.2\n- Target IP: 172.25.125.3\n\nVS Code comes with a built-in Run Extension by default.\n\nThis Run feature uses a specific configuration file called *launch.json* to support various languages, compilers, and debuggers for execution.\n\n## 1. Configuring launch.json\n\nPress F1 to open the Command Palette and search for ***launch.json*** to execute it.\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/2.png)\n\nThe ***launch.json*** file is created inside workspace/.vscode.\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/3.png)\n\n## 2. Debugging Running Application\n\nBy modifying and adding a few things to launch.json, you can easily debug applications running on the **TARGET**.\n\n- Remove prelaunchTask\n    - We assume the ELF has already been built on the HOST and transferred to the TARGET\n- Add miDebuggerServerAddress → Enter the IP address and port of the TARGET's gdbserver\n    - ex) \"miDebuggerServerAddress\": \"172.25.125.3:2001\"\n\nNow navigate to the source code and click on the Run Extension in the left bar.\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/4.png)\n\nClick the play button in the top left to establish a debugging session.\n\n- The GDB Server must be in listening state\n\n    ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/5.png)\n\n- The session is connected, but if you don't set a breakpoint, the TARGET application will continue running\n\n    ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/6.png)\n\n    - Toggle breakpoints by clicking to the left of the line number or using the F9 key\n\n        ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/7.png)\n\n    - Once you set a breakpoint, the TARGET execution will immediately pause, and you can trace variables and the call stack at that line","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-08T15:00:00.000Z","url":"/blog/en/2021-04-09-remote-debugging-with-vscode/"},{"id":"en/2021-04-10-understanding-arm64-architecture","title":"Understanding ARM64 Architecture","description":"","content":"## Definitions\n\n### ARM's definitions when introducing 64-bit support\n\n- **AArch32** – The legacy 32-bit instruction set architecture (ISA) defined by ARM, including Thumb mode execution.\n- **AArch64** – The new 64-bit ISA (instruction set architecture) defined by ARM.\n- **ARMv7** - The specification for \"7th generation\" ARM hardware that also includes support for AArch32. This is the first version of ARM hardware supported by Windows for ARM.\n- **ARMv8** - The specification for \"8th generation\" ARM hardware that includes support for both AArch32 and AArch64.\n\n### Windows definitions\n\n- **ARM** – Refers to AArch32 (32-bit ARM architecture), also known as WoA (Windows on ARM).\n- **ARM32** – Same as ARM above, used in this document for clarity.\n- **ARM64** – Refers to 64-bit ARM architecture (AArch64). There is no WoA64.\n\n### ARM data types\n\n- **Short-Vector** – A data type that can be directly represented as 8-byte or 16-byte elements in a vector. The size is aligned to 8 or 16 bytes, and each element can be 1, 2, 4, or 8 bytes.\n- **HFA (Homogeneous Floating-point Aggregate)** – A data type containing 2-4 identical floating-point members (floats or doubles).\n- **HVA (Homogeneous Short-Vector Aggregate)** – A data type with 2-4 identical Short-Vector members.\n\n## Reference\n\n[1] ARM64 ABI Conventions Overview for C++ Projects on ARM Processors, Microsoft, [https://docs.microsoft.com/ko-kr/cpp/build/arm64-windows-abi-conventions?view=msvc-160](https://docs.microsoft.com/ko-kr/cpp/build/arm64-windows-abi-conventions?view=msvc-160)","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-09T15:00:00.000Z","url":"/blog/en/2021-04-10-understanding-arm64-architecture/"},{"id":"en/2021-04-10-preferred-networks-introduction","title":"Introduction to Preferred Networks","description":"","content":"## About this post\n\nThis article is adapted from a post by Jihyo Lee, formerly of Bain & Company.\n\n## Preferred Networks\n\nAmong the countless AI startups I've seen recently, this is the largest and most ambitious company. It's a Japanese company, so it's somewhat hidden, but it's incredibly impressive to an almost absurd level.\n\nInitially, they created their own AI framework like TensorFlow, then expanded downward to libraries like CuDNN and upward to tools for optimizing AI model hyperparameters. Eventually, they built their own dedicated GPU cluster, and ultimately manufactured their own dedicated AI chips to construct an AI supercomputer cluster. The chip is also remarkable - it's a 2018 chip with FP32 performance of 131 TFLOPS.\n\nFor reference, NVIDIA's latest A100 has FP32 performance of 19 TFLOPS. The implementation using chiplets to attach four dies is also admirable... The cluster built with this became the officially recognized #1 in the global supercomputer Green500 rankings as of 2020. (Thinking about how those who boast about making AI chips in Korea talk big but haven't produced anything properly working, and exactly zero have made proper training systems...)\n\nAfter completely internalizing all the HW and SW stack of this infrastructure, they develop AI applications directly on their own dedicated data center.\n\nAll major Japanese corporations are attached as partners/customers.\n\nFor automobiles, there's Toyota; for robots, FANUC; for consumer goods, KAO; for telecommunications, NTT. @_@ The direction and vision of the business they're pursuing might be understandable, but seeing them actually accomplish all this made me flinch.\n\nBut why isn't such a company well-known?\n\nThis startup is competing with Google at a comparable level (semiconductors - AI data center - AI framework - AI applications...).\n\nJapan shouldn't be underestimated.\n\nWith just the domestic market, they can achieve this scale, so they don't need to be known externally.\n\nAll Korean AI startups combined don't even come close to this scale.\n\nWow... wow... the world is vast and we must be humble... I can't even sleep now..","categories":["DeepThinking","AI"],"tags":["DeepThinking","ToyProjects","TensorFlow","AI","DL","ML","Preferred Networks","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2021-04-09T15:00:00.000Z","url":"/blog/en/2021-04-10-preferred-networks-introduction/"},{"id":"en/2021-04-11-abi-standards-explained","title":"ABI Standards Explained","description":"","content":"## ABI (Application Binary Interface) Standards\n\nThe ABI defines the following rules for how applications should exchange binary data:\n\n- Data types and alignment methods\n- Register exchange methods for function call arguments and results\n- System call invocation methods\n- Program code initialization and data initialization methods\n- File exchange methods (ELF, etc.)\n\n### EABI Standards\n\nEABI (Embedded ABI) deals with ABI for embedded environments. In ARM architecture, the way ABI is used differs based on the Linux version, divided into the following two approaches:\n\n- arm/OABI\n    - ABI method used before kernel v2.6.15 (mainline v2.6.16) (also called Old ABI or legacy ABI)\n    - Used up to glibc 2.3.6\n    - gcc: linux-arm-none-gnu\n- arm/EABI\n    - ARM EABI method used since kernel v2.6.16\n    - Used from glibc v2.3.7 and v2.4\n    - gcc: linux-arm-none-gnueabi\n\n## Differences between arm/OABI and arm/EABI\n\n### Software interrupt calling method\n\n- OABI\n    - swi __NR_SYSCALL_BASE(==0x900000)+1\n- EABI\n    - mov r7, #1 (system call index)\n    - swi 0\n\n### Structure packaging\n\n- OABI\n    - Structures are aligned in 4-byte units\n- EABI\n    - Uses structure size as-is\n\n### Stack argument alignment\n\n- OABI\n    - Stored in 4-byte units when saving to stack\n- EABI\n    - Stored in 8-byte units when saving to stack\n\n### 64-bit type argument alignment\n\n- OABI\n    - Aligned in 4-byte units\n- EABI\n    - Aligned in 8-byte units\n\n### Enum type size\n\n- OABI\n    - 4-byte units\n- EABI\n    - Can be specified as variable\n\n### Number of registers for argument passing\n\n- OABI\n    - 4 registers (r0~r3)\n- EABI\n    - 7 registers (r0~r6)\n\n## Example of differences\n\n### Software interrupt + 64-bit type argument alignment\n\nExample: long sum64(unsigned int start, size_t size); syscall no=100\n\n- arm/OABI\n    - Assign start to r0\n    - Assign size as 64-bit to r1 and r2\n    - swi #(0x900000 + 100)\n- arm/EABI\n    - Assign start to r0\n    - Assign size as 64-bit to r2 and r3\n    - Assign 100 to r7\n    - swi 0","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-10T15:00:00.000Z","url":"/blog/en/2021-04-11-abi-standards-explained/"},{"id":"en/2021-04-14-understanding-posix","title":"Understanding POSIX","description":"","content":"## About POSIX\n\n- General\n    - Portable Operating System Interface\n    - Since 1988~\n    - Started with the idea of unifying at least a minimal area since different operating systems use different APIs, causing development difficulties\n    - Spec: [http://get.posixcertified.ieee.org/](http://get.posixcertified.ieee.org/)\n    - Versions\n        - POSIX.1 (IEEE 1003.1-1988): core services\n        - POSIX.2 (IEEE 1003.2-1992): shell & utility\n        - POSIX.1b (IEEE 1003.1b-1993): realtime related\n        - POSIX.1c (IEEE 1003.1c-1995): Thread related\n    - Released up to POSIX.1 - 2017\n        - Process creation & control, signals, file & directory operations, pipes, C library, I/O port interface and control, process triggers\n\n- POSIX PSE51-compliant API\n    - Defined in IEEE 1003.13\n    - POSIX Realtime and Embedded Application Support (AEP)\n    - PSE51-Minimal Realtime System Profile: small embedded systems, no MMU, no Disk, no terminal\n\n   ![Desktop View](/assets/img/posix_arch.png)","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","POSIX","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-13T15:00:00.000Z","url":"/blog/en/2021-04-14-understanding-posix/"},{"id":"en/2021-04-15-adaptive-autosar-retrospect","title":"Working on Adaptive AUTOSAR R&D","description":"","content":"It's been a while since I wanted to write a long diary-like post, so here we go.\n\nI've been developing Adaptive AUTOSAR standard-based platforms for about a year and a half now.\n\nAdaptive AUTOSAR refers to the standard architecture being established by the AUTOSAR consortium, which defines a common architecture for next-generation automotive applications as an open system.\n\nThe goal of this technical standard architecture is to paint the big picture that enables sustainable and reusable software applications to operate and be developed in automotive environments.\n\nSounds difficult? \n\nOf course it is.\n\nDespite all the debates, this AUTOSAR architecture was essentially created by major automotive companies led by Europe (read: Germany), who love standardization, coming together to raise entry barriers in Europe. (I heard this from my father, who worked as an engineer at Hyundai Motor for 25 years and is a living witness to the industry, so it's quite credible haha)\n\nEuropeans really love standardizing things like this.\n\nThey form consortiums where all participants think intensively together, capturing optimal words and phrases within common interests - this is the standard that Europeans (Germans) love.\n\nI've been researching various standards from multiple standardization organizations since my graduate school days.\n\nBack then, standards felt like laws to me, and I just studied them with a sense of overwhelming weight from the sheer volume.\n\nTime is scary - even an ignorant person like me started asking fundamental questions after encountering various standards continuously.\n\nMaybe because of these deep contemplations, when I encountered AUTOSAR, it didn't just approach me as a collection of text like before. I let out a sigh of relief knowing I hadn't wasted all that time.\n\nThere probably isn't a slower learner than me... Anyway.\n\nStandards actually take a tremendous amount of time to establish. They go through repeated cycles of proposal, discussion, more discussion, establishment, and continuous revision.\n\nIn a way, the weight of standards created through consensus among massive institutions backed by enormous capital over such long periods is tremendous.\n\nAs I mentioned above, Europeans (Germans) really love structuring the world this way and have dominated major companies for nearly a century.\n\nBased on such structured architectures, the automotive industry's OEMs, Tier-1s, Tier-2s, along with management-perspective bureaucracy, have been the characteristic of automotive groups that have dominated for nearly a century: Volkswagen, BMW, Benz, Toyota, GM, etc.\n\nAmong these, Toyota Group's bureaucracy is such a regular case study of successful management systems at Harvard Business School.\n\nThey built a really solid glass ceiling (in modern terms haha)... but someone shattered that glass.\n\nThat someone was Tesla.\n\nThis broad ecosystem of companies built on extensive technical standards...\n\nThe grand plan to see this heaviness as a disadvantage and do everything in one company.\n\nEveryone doubted whether they could pull it off for nearly 15 years, but this company achieved vertical integration.\n\nAlthough it's quite distant from the standardization I've studied, they approached the automotive industry completely differently from a business perspective and ultimately shattered this thick glass ceiling.\n\nI think this shows the culture of Silicon Valley, which values technical execution, and it's completely opposite to what Europe aims for.\n\nAmong the things Tesla broke, I think the automotive market's software ecosystem is the most impressive.\n\nTesla built their innovation based on low-power computer architectures that rapidly developed in the mobile market, open-source operating systems, and open-source applications, earning love from many tech gurus.\n\nThe Adaptive AUTOSAR standard began in earnest around 2017, and in my view, the major automotive groups weren't that urgent back then.\n\nStarting from 2019 and going through 2020-2021, as Tesla received tremendous market valuation and its market cap exceeded the combined value of all automotive groups, each group company has been frantically trying to show innovation in software.\n\nVolkswagen, the world's largest automotive group, openly declared they would hire 6,500 software engineers by 2025 and wouldn't be defeated by Tesla with their eyes wide open. Toyota Group is letting go of some of their proud bureaucracy and offering differential salary compensation like the software industry.\n\nHyundai Motor Group, where I work, is also developing Adaptive AUTOSAR standard-based platforms and applications with advanced teams from Hyundai Motor, Mobis, and Autron coming together.\n\nWhat's interesting is that it started as advanced team work, but now it's become essential work, not just advanced research.\n\nThis is because the various functions and ecosystems provided by Adaptive AUTOSAR standard-based platforms are a series of responses to Tesla's vertical integration.\n\nThis challenging work is really fun.\n\nI don't know if someone like me, who goes slowly, fits in this field where the core is to approach things business-wise based on an enormous range of knowledge and execute with maximum speed...\n\nBut I'm confident that I can go slowly and steadily more than anyone else.","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2021-04-14T15:00:00.000Z","url":"/blog/en/2021-04-15-adaptive-autosar-retrospect/"},{"id":"en/2021-08-22-tesla-ai-day","title":"Tesla AI Day - Thoughts from a Platform Engineer","description":"","content":"# Tesla AI Day Reflections\n\nAs an engineer building platforms myself, this was a pretty fascinating tech talk that really caught my attention.\n\nSeeing how they're doing everything in-house - from semiconductor design to manufacturing processes, plus the matching compilers, development environment, and software stack - makes engineers like me who can only observe this innovation from the outside incredibly curious.\n\nMost of the questions during the Q&A session were about exactly these areas.\n\nFor people who've gotten tired of simply using frameworks and applying new modeling on top to find more efficient AI models, this kind of next-generation platform approach is what they're craving. While ML hasn't been booming for that long, the fact that simply advancing at the high-level software stack isn't sustainable became clear with GPT-3.\n\n![Desktop View](/assets/img/aiday/1.png)\n\nThis tech talk clearly showed that companies like Intel, Tesla, Apple, Google, and Microsoft are changing the world not simply because they have platforms, but because they have the capability to attract the researchers and engineers who can build next-generation platforms.\n\n![Desktop View](/assets/img/aiday/2.png)\n\nAnd these next-generation platforms become the foundation for all software and business worldwide, and we can't escape from that ecosystem... 😅\n\nSince these engineers now operate within the unstoppable flow of open source, if you don't show new vision and new architecture, you'll quickly lose out to competitors. (Actually, this seems to make the open source movement even stronger.)\n\nIt's always a sad reality that Korea doesn't have many people who can see and present such incredible visions.\n\n## The Evolution of Human Thinking\n\nThe cultural evolution from logical thinking to computational thinking that brought huge progress to humanity is now advancing to AI-based thinking, and what drives this is computational thinking.\n\nI really hope Korea has more people who contribute to making the world better based on computational and AI-based thinking.\n\n## What This Means for Platform Engineers\n\nAs someone working on platform development, Tesla AI Day made me think about several key points:\n\n### 1. Vertical Integration is the Future\nTesla isn't just building AI models - they're building the entire stack from silicon to software. This kind of vertical integration allows for optimizations that wouldn't be possible otherwise.\n\n### 2. Hardware-Software Co-design Matters\nThe way they designed their D1 chip specifically for their neural network workloads shows how important it is to design hardware and software together, not separately.\n\n### 3. Scale Changes Everything\nWhen you're processing exabytes of data from millions of vehicles, traditional approaches simply don't work. You need to rethink everything from the ground up.\n\n## The Korean Tech Landscape\n\nIt's somewhat disappointing that Korea doesn't have more companies taking these kinds of bold, visionary approaches. We have incredible engineering talent, but we often seem to be following rather than leading in these next-generation platform innovations.\n\nMaybe it's because of:\n- Risk-averse corporate culture\n- Emphasis on incremental improvements over breakthrough innovation  \n- Lack of patient capital for long-term R&D investments\n- Brain drain to Silicon Valley companies\n\n## What We Can Learn\n\nFor engineers working in Korea (or anywhere outside Silicon Valley), Tesla AI Day offers some lessons:\n\n1. **Think in Systems**: Don't just optimize one component - think about the entire system\n2. **Question Assumptions**: Just because everyone uses a certain approach doesn't mean it's optimal\n3. **Invest in Fundamentals**: Hardware, compilers, and low-level optimizations matter more than ever\n4. **Open Source Everything**: The best way to build platforms is to make them accessible to everyone\n\n## Looking Forward\n\nThe shift from computational thinking to AI-based thinking is happening whether we're ready or not. The question is: will we be building the platforms that enable this shift, or will we just be using platforms built by others?\n\nI hope more Korean engineers and companies start thinking about these fundamental platform challenges. We have the talent - we just need the vision and the courage to pursue it.\n\n*The future belongs to those who can see it coming and build the infrastructure to make it happen! 🚀*","categories":["DeepThinking","AI"],"tags":["DeepThinking","AI","GithubPage","Retrospect","CPU","GPU","SoC","AP","Semiconductor","Tesla"],"pubDate":"2021-08-21T15:00:00.000Z","url":"/blog/en/2021-08-22-tesla-ai-day/"},{"id":"en/2021-12-31-year-retrospective","title":"2021 Year-End Retrospective","description":"","content":"2021 was probably the most dynamic year in my short career history.\n\nMost of the intense memories that settled in my head this year seem connected to Sonatus in one way or another, so it feels like my head was full of nothing but Sonatus this year.\n\nOf course, in parallel, I participated in many other gatherings too. ASG (ADAS Study Group, which I named haha) second year, the speech group at Cloudy University SCP that my life mentor Vice President Lee Dong-hoon created for someone like me who can't speak well, and the Cloudy University industry study where crazy studious people gather - I even embarrassingly led sessions as a mobility expert, but I feel like I only talked about work or work-related things even there.\n\nSonatus - a company that now feels like home, but at the beginning of the year I was desperately struggling to get into.\n\nAfter a dramatic joining, followed by a series of really dynamic events.\n\nIt's only been 6 months, but what I experienced in that short period feels like more than what I studied and worked for 5 years before, and the quality seems different too.\n\nI think life is similar to atomic orbitals, and this feels like a meaningful year where I achieved some kind of quantum leap.\n\nLooking back, whether I was in academia, during my military service, or at a large corporation, no matter how proactively I worked and studied with ownership, there always felt like there was some unreachable glass door.\n\nSo instinctively, I tried to find things that could satisfy those desires - studying various things, meeting different people, desperately trying to find and join online communities where so-called excellent people gather, having lots of conversations.\n\nSonatus resolved all these needs of mine at once.\n\nMy needs weren't just tech-related, but also curiosity about macroscopic perspectives on global social phenomena, curiosity about various people trying to solve these things with emerging technologies, the joy of learning how these are viewed financially in the market, and the stimulation from new relationships with people who live with similar thoughts - this complex yet simple and continuously changing desire is one driving force that keeps me alive and breathing.\n\nThese complex needs of mine all connected when I joined Sonatus, opening new horizons in my head. Now I need to continuously fill myself with new things to settle into a new orbit and wait for the next quantum jump.\n\nNow let me unpack what I felt in 2021.\n\n## Tech\n\nThe macroscopic perspective our boss Jeff has on the changing flow of the connected world. He read the unstoppable rapid flow that no one could resist and found opportunities there.\n\nJeff is a famous ace in Silicon Valley who successfully exited 6 times in the market (not quite Big Tech level, but still).\n\nIn the tech world he sees, papers, patents, and standards are too slow. (Of course, we need to take care of these things later too. But we can't lock the barn door after the horse is stolen, right?)\n\nTo ride this incredibly fast flow, we must use the powerful tool of the titan called Open Source.\n\nWe also need to understand the hardware design and architecture ecosystem changing around open source software due to this mighty decentralized open source ecosystem.\n\nCompanies that can't ride this wave will probably be phased out from the business world.\n\nAlso, if we just simply ride the flow led by American Big Tech, we'll forever remain under Big Tech.\n\nAll business areas that will change human life will change around the decentralized open source ecosystem. Bio, Health Care, Blockchain, Mobility, Quantum Computer - all of this will move around that open source ecosystem. It's an inevitable flow.\n\n## Finance\n\nQuantitative easing has been going on for quite a while, but even more money was released to revive the economy hit by COVID, making an already money-rich market even richer. It would be nice if that money was used according to government scenarios to solve private sector recession, but it largely tilted toward making the wealthy even wealthier.\n\nHowever, if we use this flow well, it could be an opportunity for a quantum jump.\n\nCurrently in the US, all these quantum jumps are led by tech companies, with Silicon Valley startups at the center.\n\nThis change seems to have started from 2010, the beginning of the golden era of VCs started by nouveau riche who successfully created wealth from the dot-com bubble through the mass adoption of smartphones.\n\nAccording to Jeff, even 10 years ago, a startup's successful exit meant being M&A'd by traditional American powerhouse companies like Cisco or Intel, but after FAMANG dominated the world, the exit paradigm completely changed.\n\nSo Jeff's dream for his 7th company, Sonatus, isn't to be M&A'd by large corporations, but to become a decacorn company beyond unicorn. And it's an era where that's possible.\n\nSonatus has so many engineers who have experienced consecutive exits. Sonatus is a company created by decent people among those with such experience, asking around and joining together, centered around Stanford alumni, growing with smart and passionate talent.\n\nI'm also trying to do well riding that wave, and I hope it serves well as an initial investment for my life going forward. Haha\n\n## People\n\nBefore, I thought a lot about whether I should maintain work-life balance, but I've changed a lot. Work = Life, so there's nothing to balance haha.\n\nThinking about it, I wonder if there's a need to think of work as something separate. The people I work with are really good people I want to spend my lifetime with, so I don't want to separate the two.\n\nThese days, work is my life's motivation and joy, so I barely have time to go to mediocre gatherings for networking and drink with strangers.\n\nAnd if I used to simply like giving, now I spend time on people I want to give more to because I'm grateful for what I've received. The time spent exchanging with people who give and take without worrying about who goes first is so happy and enjoyable.\n\nIt's not just working together, but discussing social phenomena, contemplating how to solve them with the tech we know, and sharing when new tech comes out - this kind of life is so enjoyable.\n\nThinking about it, we're always a rapidly scaling company short on people, yet we're incredibly careful when hiring one person.\n\nThere are hardly any difficult people, everyone likes to teach each other, wants to take on even small tasks, and there seem to be only considerate people who think this is contributing to the company.\n\nI feel like I'm a good person too, so I feel good along with them. Haha\n\n## Workout\n\nIf you asked me to pick my good habit, I would pick exercise without hesitation.\n\nI've played basketball for about 20 years, and weight training (which I started for rehabilitation and to play basketball better) for 15 years.\n\nAnd now it even plays a role in relieving my stress and helping brain function.\n\nActually, these things are what I've been doing continuously and are as familiar as eating, but what I'm most proud of this year is teaching exercise to company people.\n\nIt started when the Korean team leader, who had been doing CrossFit for a few years, told us to bring workout clothes on the first day, and now I'm overseeing all the workouts of people who exercise together.\n\nI compress and teach what I researched and experienced physically while wanting to play basketball well, and everyone seems amazed and loves it, which makes me quite proud.\n\nI'm like a semi-resident trainer, teaching workouts and even planning diets if they want... haha\n\nI can teach them the very positive signals that exercise brings to human life firsthand and contribute to their lives, which makes it even better.\n\nNow we can't be separated even if we wanted to~~\n\nI should wrap up the things I want to write more about here.\n\nIn 2022, I hope my tech knowledge becomes more mature, my English improves enough to have soulful conversations, and I write more to improve my writing.\n\nI'll set detailed milestones and definitely achieve them~!\n\nAdieu Dynamic 2021","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2021-12-30T16:00:00.000Z","url":"/blog/en/2021-12-31-year-retrospective/"},{"id":"en/2022-12-26-pre-retrospective","title":"2022 Retrospective","description":"","content":"2022 !!! Wow, it really passed by like lightning - like roasting beans over lightning fire.\n\nI honestly don't know how 2021 and 2022 went by...\nI failed most at blogging in the year I planned to blog the most...\nI'm reflecting on this ㅠㅠ\n\nAs I wrap up the year-end, let me list the blog posts I couldn't complete this year.\n\n(I'll turn some of the list below into blog posts by early next year)\n\n- 2022 Backlog Blog Content\n    - Tech\n        - C++\n            - Thoughts on Efficient Configuration Management for C++ Projects\n            - C++ Memory Protection Techniques\n            - What is io_uring and how does it help us?\n        - Yocto\n            - How to Manage ELF in Yocto\n            - Thoughts on Yocto as a Collaboration System\n            - Thoughts on Efficient Build and Deployment Using Yocto System\n        - Bazel\n            - What is Bazel?\n            - Bazel VS Yocto\n        - Docker\n            - Thoughts on Efficient Embedded Development Environment (With Docker)\n            - Docker Operating Principles Deep Dive\n            - Docker Container from Linux Namespace Perspective\n        - ETC\n            - Deployment Artifacts and Post-Management (Reproducible Build, Debuginfod)\n            - Thoughts on Linux RT Patch (Preemption)\n            - How to Receive GitHub Triggers in VPN-Protected Intranet\n            - Reverse Engineering with HYDRA\n    - Network\n        - Thoughts on Talent Needed at Different Growth Stages of Startups\n        - Effective Methods for Ideation\n    - Personal\n        - People I Share Dreams With\n        - Teaching and My Growth\n        - Cambodia Volunteer Service Retrospective\n\nSince I've made this commitment to myself on the blog, let's make sure to achieve it!\n\nJust wait a little longer, 2023~!","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2022-12-25T16:00:00.000Z","url":"/blog/en/2022-12-26-pre-retrospective/"},{"id":"en/2023-11-19-first-post","title":"First Post of 2023","description":"","content":"I never imagined in my wildest dreams that my first post of 2023 would be written from a hospital bed.\n\nWell, it wasn't from overwork or anything job-related. On my way back from playing basketball at night, I got into a proper head-on collision with a car that ran a red light, and my car (Sorento) was completely totaled - basically scrap metal.\n\nThe other driver had his wife in the passenger seat and what seemed to be a coworker in the back seat. He mistakenly thought he had a left-turn signal and made a left turn during a straight green light. Because of that, I, who was cruising nicely in the second lane at 50~60 km/h, had a head-on collision and once again experienced that moment when adrenaline gets pumped massively into your system.\n\nThe first time I experienced that phenomenon was when [I saved someone who was trying to commit suicide by jumping](https://news.nate.com/view/20091213n06658?fbclid=IwAR3wZ-IMRg0_tf87h_aEP5yVmKr-hDrBQJWq2eMFh8Jkh3mgcMUnoraOBEw). (Come to think of it, my life has been quite eventful)\n\nEven then, the moment that lady fell from the balcony was etched in my mind like a photographic memory due to the massive adrenaline rush, and those split seconds felt incredibly long. This time too, as the car was approaching me, I was gripping the steering wheel and looking ahead when I felt this strange out-of-body experience...\n\nFortunately, I'm okay, so I'm grateful to be alive and able to blog about it. \nㅋㅋㅋㅋ\n\nSuddenly need to go for treatment, so ending this abruptly....","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2023-11-19T04:00:00.000Z","url":"/blog/en/2023-11-19-first-post/"},{"id":"en/2024-01-06-p-day","title":"P's Day - January 6, 2024","description":"","content":"Recap Jan 6, 2024\nDisclaimer: This is written from a quite subjective perspective.\n\nPhase 1 - PT..?\nFollowing the boss's command to meet at Yafit Studio by 11 AM for a workout session, I arrived to find... no trainer. Apparently, the \"other trainer who would be there\" that he mentioned was me. Nobody else knew except the boss. The boss, who I think is one of the few people in the world with a P-type personality several times stronger than mine, said, \"Alright, self-proclaimed 20-year fitness expert Jay Lee, teach us how to work out!\"\n\nSuddenly becoming a one-day personal trainer, I quickly came up with several workout routines and somehow managed to have a fun hour-long session. There were a few exercises I had casually taught the boss when he followed me to morning workouts in New Jersey. Back then, he couldn't quite get the movements right, but now seeing him execute them flawlessly, I could clearly see all his hard work paying off. Plus, Sung-kyung hyung, who said he was working out for the first time in 7-8 years, performed the movements so well you couldn't tell there was any gap. I guess his muscle memory from his Seoul National University physical education days was instantly recalled and engraved in his muscles.\n\nThe boss sweated buckets, and to replenish all that lost sodium, we went for sundae-guk (Korean blood sausage soup) after the workout... (only the boss ordered the extra-large size)\n\nPhase 2 - Sudden Acceleration\nRight after the workout ended, we suddenly decided to go to a ski resort. It was such a P-type spontaneous suggestion from the P-type guys, and being 99% pure P-type myself, I boldly accepted the proposal. I had only worn casual workout clothes for the gym, but we ended up heading straight to Hoengseong right after the workout.\n\nAfter arriving in Hoengseong in no time, while Sung-kyung hyung passed out for two hours from the morning workout plus yesterday's fatigue, the boss suggested we go grocery shopping for a meal. We went to the famous J-Mart in Yongpyong. He definitely said we were just buying food for one meal, but... he went into sudden acceleration mode, spent 200,000 won, and then said, \"Let's use this for tomorrow's lunch too.\" (There's so much good food in Hoengseong though ㅠㅠ.. but it was delicious)\n\nSo after eating two bowls of rice with spicy soft tofu ramen with gochujang and big rice bowls, we headed out to go snowboarding...\n\nPhase 3 - On-boarding....\n\nThe scenery at Yongpyong Ski Resort was absolutely beautiful. Seeing the resorts at the entrance of Yongpyong reminded me of the academic conferences I enjoyed(?) every winter at Yongpyong during my graduate school days.\n\nThere were times when I gave oral presentations well after drinking all night, skiing or snowboarding, and then sleeping for just two hours... suddenly feeling dizzy.\n\nIn the moderately cold weather of 0~3°C with light snow falling, we got on the lift.\n\nI confidently locked up my snowboard deck on my left foot and went up, but... shockingly, while sitting, my hand couldn't reach my right foot that was on the deck...\n\nNow that I calculate it, it had been over 8 years since I last snowboarded, so why was I so confident... I remember when I first tried snowboarding, it took me 3 hours just to stand up.\n\nIt was embarrassing in front of the guys waiting for me, but I did hip flexor and psoas stretches right there and tried to lock up again, but it wasn't working well.\n\nThis damn body... even when my abs contracted, I almost got cramps from the planks I did in the morning.\n\nWith the same tension as being left with a 1.51m putt right in front of a conceded line during a golf bet, I concentrated intensely and successfully locked up!\n\nHowever, the next problem was not being able to stand up. Yes, I couldn't reach the middle of the snowboard deck while squatting.\n\nBut recalling old memories, I managed to stand up by flipping my belly 180 degrees using my abs(?) while wearing the snowboard.\n\nPhase 4 - Snowboarding! And..\n![Desktop View](/assets/img/2024_01_06_2.jpg)\n\nRather than embarrassment, I thought \"men need guts,\" and the snowboarding I did after struggling to stand up was so good it made me forget all the embarrassment.\n\nFlying through the perfectly timed falling snow and tumbling down to eat snow was pure happiness.\n\nWhat was even luckier was that Sung-kyung hyung used to be an instructor who taught ski and snowboard instructors back in university.\n\nLearning the twisting technique for direction changes from hyung in a crash course, I somehow absorbed it into my body fairly quickly and had a really fun time snowboarding.\n\nFor right turns, the core twist like a golf backswing, for left turns, the twist like a golf follow-through... (this is what happens when you're obsessed with golf)\n\nAnyway, it was so much fun.\n\nThe boss was smoothly skiing on his newly acquired advanced skis, and Sung-kyung hyung, claiming it was his first time in 10 years, was cutting through the snow like some kind of missile...\n\nAfter enjoying 3 hours of night skiing, we returned to our accommodation, ordered some decently delicious dakgalbi (spicy stir-fried chicken) for pickup (in addition to the 200,000 won worth of groceries), and had a great time drinking good alcohol at the accommodation.\n\nRecap end.","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2024-01-05T15:00:00.000Z","url":"/blog/en/2024-01-06-p-day/"},{"id":"en/2024-08-23-daily","title":"Thoughts on Vehicle Software Mass Production - Part 1","description":"","content":"Our CCU2 project is getting close to mass production now, and man, the business trips have been non-stop lately.\n\nAfter working in the automotive industry for about 10 years, this is my second time going through MP (Mass Production), and when this phase approaches, everyone gets super tense.\n\nIt's literally like a daily war - identifying and tackling issues that come up every single day. I decided to document this period because if I don't write it down now, I'll probably forget it all later.\n\nI'm not sure how often I'll be able to write, but I'm going to try to record as much as possible.\n\nWhen I think about when this \"period\" actually starts, it's quite a long stretch.\n\nUsually, after completing the PoC (Proof of Concept) and finishing vendor selection through bidding, that's when you can consider it the Mass Production phase. The PoC period alone can be 6 months on the short end - in CCU2's case, it took over 2 years to complete feature development. After vendor selection was completed, we entered the Mass Production phase at the end of 2023, and the first vehicle model featuring the CCU2 project will come out at the end of 2024 ~ early 2025.\n\nAnd based on this vehicle model, it'll expand to dozens of other models rolling around roads worldwide, so from the OEM's perspective, the first MP of the project's first vehicle model is absolutely crucial.\n\nThe key events during this Mass Production period, in order, are: Wire car → Proto car → Master Car → Pilot 1 → Pilot 2 → M Stage → SoP.\n\nUsually there's about a 3-month gap between these phases, so you're looking at 12-15 months of validation and refinement.\n\nDuring PoC, we worked hard together with the OEM's R&D organization. But during Mass Production, we team up with the OEM's R&D organization to get past various verification processes from different perspectives within the OEM - Quality Assurance organization, Cyber security organization, etc.\n\nDuring Wire Car, individual components (ECUs) that go into the vehicle are literally wired together for the first time.\n\n## The Reality of Automotive Software Development\n\nWhat people don't realize about automotive software is how different it is from typical software development. You can't just push an update and fix bugs later - once these cars are on the road, any issue becomes a massive recall situation.\n\n### The Validation Hell\n\nEvery single feature goes through layers and layers of validation:\n\n1. **Unit Testing** - Does the code work in isolation?\n2. **Integration Testing** - Do all the components work together?\n3. **System Testing** - Does it work in the actual vehicle environment?\n4. **Cybersecurity Validation** - Can it be hacked?\n5. **Quality Assurance** - Does it meet automotive standards?\n6. **Field Testing** - Does it work in real-world conditions?\n\nAnd this happens for EVERY. SINGLE. CHANGE.\n\n### The People Side\n\nWhat's fascinating is watching how different organizations within the OEM interact during this phase. During PoC, everyone was collaborative and focused on \"can we make this work?\" But during MP, it becomes \"can we prove this won't break?\"\n\nThe R&D folks who were your buddies suddenly become your advocates, helping you navigate through various internal review boards. It's like they're saying, \"Okay, we believe in this technology, now help us convince everyone else.\"\n\n### The Stress Factor\n\nThe stress level is honestly insane. Every day brings new issues that could potentially delay the launch. And in automotive, launch delays are measured in millions of dollars per day.\n\nI've seen grown engineers almost cry over bugs that would be considered minor in web development. But when you realize that bug could affect thousands of vehicles on the road, the weight of responsibility hits different.\n\n## What Makes It All Worth It\n\nDespite the stress and crazy hours, there's something magical about seeing your software actually controlling a real vehicle. When you see the first prototype car successfully start up with your code running the show... that's a feeling you don't get in most software jobs.\n\nPlus, the automotive industry is going through this massive transformation right now. We're literally building the future of transportation. Every line of code we write is contributing to making cars smarter, safer, and more connected.\n\n## Looking Ahead\n\nWe're still in the thick of it with CCU2, but I can already see the light at the end of the tunnel. The first vehicles with our software will be rolling off production lines soon, and that's both terrifying and exciting.\n\nI'll try to document more of this journey as we get closer to SoP (Start of Production). There are so many interesting technical challenges and human dynamics that I want to capture before they fade from memory.\n\n*Stay tuned for more war stories from the automotive software trenches! 🚗💻*","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus","SDV"],"pubDate":"2024-08-22T15:00:00.000Z","url":"/blog/en/2024-08-23-daily/"},{"id":"en/2023-11-29-creating-dh-lee-chatbot-journey","title":"Journey to Creating DH Lee Chatbot - Part 1","description":"","content":"# Restarting CPS Season 2\n\nI conducted a Python course called CPS (Crash Python course for SANS family) for about 3 months starting from early 2023.\nI was busy gaslighting the audience that \"Python is easy and fun to learn and you can do so much with it!\" when suddenly ChatGPT appeared like a comet, and during the course, even I as the instructor couldn't tell if this was a Python course or a \"How to Use ChatGPT Well\" course - it became that scattered as I finished the 3-month course.\n\nProfessor Noh Su-rim, who manipulated me into creating the course and enthusiastically attended it, reverse-gaslighted me into starting Season 2 in early 2024.\nThis time, the core of next year's course will be to actually do something using LLM (Large Language Model) pre-trained models like GPT3.5 and Llama.\n\nAnd I also decided to start blogging daily, something I was too lazy to do before.\nLet's see how this goes...\n\nThe first thing to do is to convert the numerous lecture videos, voice recordings, and chat logs of the respected DH Lee through Voice to Text and other frameworks, label and preprocess the data, train it on a pretrained model, and create a DH Lee AI Model API Service.\n\nI want to first use this API as a Chatbot on Telegram, Slack, KakaoTalk, etc., and later collaborate with Midjourney and other commercial visual AI services.\n\nWell, the goal for now is for me as the instructor to quickly work through all the steps, and I plan to document this process on the blog.\n\nAnd I'm daily discussing and detailing this process through ChatGPT4's voice chat service, and recently the features have gotten so good that it even creates documents - how can I not post this on the blog?\n\nThat concludes my introduction...\n\nBelow, I'll record the content I discussed with ChatGPT4's AI assistant.\n\n# Guide to Transcribing DH Lee's Voice Data and Training with GPT-3.5\n\n## Transcribing Voice to Text Using Microsoft Azure\n\n### Setting Up Microsoft Azure Speech Service\n1. **Create an Azure Account** and enable billing.\n2. **Create a Speech Service Resource** in Azure Portal and obtain your API key and endpoint URL.\n3. **Install Azure SDK** for Python using `pip install azure-cognitiveservices-speech`.\n\n### Python Script for Transcription\n```python\nimport azure.cognitiveservices.speech as speechsdk\n\ndef transcribe_audio(file_path, service_region, subscription_key):\n    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=service_region)\n    audio_input = speechsdk.AudioConfig(filename=file_path)\n\n    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n\n    result = speech_recognizer.recognize_once_async().get()\n    return result.text\n\nsubscription_key = \"YourAzureSubscriptionKey\"\nservice_region = \"YourServiceRegion\"\naudio_file_path = \"path/to/your/audiofile.wav\"\n\ntranscription = transcribe_audio(audio_file_path, service_region, subscription_key)\nprint(transcription)\n```\n- Replace `\"YourAzureSubscriptionKey\"`, `\"YourServiceRegion\"`, and `\"path/to/your/audiofile.wav\"` with your details.\n\n### Post-Transcription Steps\n- Review and correct the transcriptions.\n- Store the transcriptions in a structured format.\n\n## Training Transcribed Data with GPT-3.5\n\n### Data Preparation for GPT-3.5\n- Organize and clean the transcribed data.\n- Format the data as per OpenAI's guidelines for fine-tuning.\n\n### Fine-Tuning Process\n- Access the GPT-3.5 API through OpenAI, which may offer fine-tuning capabilities.\n- Upload your prepared dataset to OpenAI and fine-tune the model on this data.\n- Define training parameters as needed.\n\n### Integration and Testing\n- Integrate the fine-tuned GPT-3.5 model into your application via API.\n- Test the model's performance to ensure it aligns with DH Lee's style and content.\n\n### Considerations\n- Ensure you have adequate computational resources and expertise.\n- Obtain DH Lee's consent and consider ethical aspects of using his data.\n- Be mindful of the costs associated with API usage and training.\n\n---\n\n**Note:** This document is a summarized guide based on the conversation and should be adapted for specific project needs.","categories":["TechSavvy","AI","C++"],"tags":["TechSavvy","AI","Python","C++"],"pubDate":"2023-11-28T15:00:00.000Z","url":"/blog/en/2023-11-29-creating-dh-lee-chatbot-journey/"},{"id":"en/2023-11-30-yocto-configuration-guide","title":"Yocto Configuration Guide - Part 1","description":"","content":"# Yocto Configuration\n\nWhen you need to work with the Yocto project, you typically create recipes for your own software or your company's software, combine various layers including these recipes, and create a unified Linux image for use. In this case, the Yocto Project serves the role of combining various layers to create a single image.\n\nAnd when you think about it, our software sometimes needs to run in various environments (architectures), and there are cases where you need to build with different configurations based on the same source code.\n\nSince Yocto is widely used in the automotive industry, let's use automotive industry Yocto configuration as an example.\n\nLet's assume our software module that needs to be built is called TestModule, and TestModule doesn't just need to support one car model but various vehicle models.\n\nThen when building TestModule, we need to support various configurations from the source code level to support different vehicle models.\nIt would be nice if TestModule were a script-based application like Python or Bash, but most likely it's written in languages like C/C++ or Rust, Go.\nThis means not only source code but also build systems like Makefile, CMake, Scons, Meson, Bazel for building would be used variously.\n\nIf we call the build system's configure tool a Configure Tool, this Configure Tool also needs to support various options.\n\nIn other words, we need to support various configurations at the source code level, Configure tool level, and even at the Yocto level.\n\nConfiguration design is really important, and if this design is managed through a high-level build framework like Yocto, it's much more convenient and flexible than managing configuration through the build system's Configure Tool.\n\nIn this post, I want to explore how Yocto manages configuration.\n\n## Yocto Configuration\n\nThere are several ways to manage configuration in Yocto, but in this post, I want to explore how to manage configuration using Yocto's Bitbake.\n\n1. How to manage configuration using Yocto's Bitbake\n2. How to pass configuration to Bitbake when building with Yocto's Bitbake without using Yocto's Bitbake\n\nI'll continue writing in the next post...\n\n## Reference\n\n* [Yocto Project Reference Manual](https://www.yoctoproject.org/docs/3.1.1/ref-manual/ref-manual.html)\n* My brain","categories":["TechSavvy","Yocto"],"tags":["TechSavvy","ProgrammingLanguage","Yocto"],"pubDate":"2023-11-30T01:00:00.000Z","url":"/blog/en/2023-11-30-yocto-configuration-guide/"},{"id":"en/2025-02-05-retro-1","title":"My Awkward Retrospect #1 - Looking Back After 4 Years","description":"","content":"# Getting Started: Confession of a Lazy Person\n\nLooking at my previous retrospectives, I see the last ones were from 2021 and 2022. Yeah... I've been pretty lazy about this whole reflection thing, haven't I? 😅 \n\nAs a New Year's resolution for 2025, I'm going to try to write down short records whenever I have time, even if they're just random thoughts.\n\nSonatus has grown at an unbelievable pace, and I like to think I've grown along with it. I'm approaching my 5th year at the company now. My trusty Dell laptop that's been with me for 4 full years is definitely showing its age, and coincidentally, 4 years is also when my stock options fully vest. Seems like the perfect time to look back on these 4 years of experience.\n\nLately, I've been thinking a lot about \"the essence of life.\" I have this habit of asking people I meet questions like \"When are you happiest?\" but I realized I've never really organized my own thoughts about it. So I thought I'd use this blog to sort through my concerns and thoughts in an informal, chatty way.\n\n# My Core Values: Mission-driven, Mover, Giver\n\n## 1. Mission-driven\n\nI've loved math since I was little, especially tackling difficult problems for hours on end. Even when I looked at the answer key and it didn't make sense to me, I'd ask the teacher again and again, then proudly show off how I solved it a different way.\n\nAt Hyundai Autron, I was already planning my exit pretty early on. When I joined Sonatus, I actively looked for work and created my own role within the company.\n\nI'm happiest when I'm creating something or solving problems in complex areas that few people understand. When my alternative military service was ending and I was deciding between continuing in \"automotive\" or pivoting to \"crypto,\" what ultimately attracted me was the complexity of the problems I'd be solving.\n\nThe problems I tackled in 2024 were more complex than I could have imagined, and I had an absolute blast working through them. Sure, it was exhausting too, but I think tackling increasingly complex problems is the essence of my life.\n\n## 2. Mover\n\nI prefer jumping into action rather than overthinking things. When faced with new challenges, I feel more excitement than fear. This means I take a lot of action and make a lot of mistakes (haha). I'm far from a perfectionist and often need help from others, but I also gain a lot from trying so many different things.\n\nAs the company has grown rapidly, the scope for me to move freely has narrowed a bit, which is somewhat disappointing. But I've learned that solving complex problems with multiple people is much harder and more challenging than I expected - and the rewards are proportionally greater.\n\n## 3. Giver\n\n![Desktop View](/assets/img/sonatus_all.jpg)\n\nI think the reason I've been able to meet so many great people and build relationships with them is probably because I tend to give first. One of my earliest childhood memories is taking a box of snacks I got at my grandfather's house and sharing them with kids at the neighborhood playground.\n\n![Desktop View](/assets/img/cambodia_jay.jpeg)\n\nI still have a vivid memory of the kids in Cambodia during a volunteer trip, how happy they were wearing the basketball jerseys I gave them. I want to continue being someone who gives without expecting anything in return, which means I need to earn more to be able to give more (still a long way to go though!).\n\nIn December 2024, our CCU2 project that had been running for nearly 4 years finally reached mass production (MP), and team members from around the world gathered in Korea for a busy schedule. Despite not getting much sleep, I really wanted to treat everyone to delicious Korean food. Thanks to that, we all became friends, and I especially enjoyed finding cultural connections with our colleagues from Poland - we created some really positive synergy together.\n\n# Challenges I'm Facing\n\n## The Evolution of My Role\n\nWhen I first joined Sonatus as one of the early employees, I wore many hats - I was doing customer support, infrastructure work, field systems, container management, you name it. There was this exciting chaos where everyone had to figure things out as we went.\n\nNow that we're a 300+ person company with clear organizational structure, roles have become much more defined. While this makes business sense, I sometimes miss the days when I could jump between different areas and tackle whatever needed solving.\n\n## The Classic Startup Dilemma\n\nThere's this interesting tension that happens as startups grow. The very things that made the early days exciting - the flexibility, the broad scope of responsibility, the \"figure it out as you go\" mentality - become less feasible as the company matures.\n\nI'm trying to find ways to maintain that sense of excitement and challenge within a more structured environment. It's like learning to be creative within constraints, which is its own interesting problem to solve.\n\n# What's Next?\n\nI'm at an inflection point where I need to think about what the next chapter looks like. The skills and mindset that got me here might need to evolve for what comes next.\n\nSome questions I'm pondering:\n- How do I stay mission-driven when the missions become more defined by others?\n- How do I continue being a mover in a more structured environment?\n- How do I scale my giving as both my resources and responsibilities grow?\n\n# Wrapping Up\n\nWriting this down has been helpful for organizing my thoughts. I realize that even though the external circumstances have changed dramatically over these 4 years, my core values have remained pretty consistent.\n\nMaybe that's the key - holding onto what fundamentally drives you while adapting how you express those values in different contexts.\n\n*More retrospective thoughts coming soon. Time to stop being lazy about this whole reflection thing!*","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus","SDV"],"pubDate":"2025-02-02T15:00:00.000Z","url":"/blog/en/2025-02-05-retro-1/"},{"id":"en/2025-02-08-daily","title":"What Makes Me Happy These Days","description":"","content":"# A Heart-to-Heart with My Branch Manager\n\nYesterday I had my regular 1-on-1 meeting with our Korean branch manager, and honestly, these conversations are always something special. Unlike typical corporate 1-on-1s that focus on work goals and performance metrics, our chats dive into the deeper stuff - \"How should we live life?\", \"How's your relationship with your girlfriend going?\", \"Everything okay at home?\" It's refreshingly human.\n\nWhat I've always found amazing about my manager is how genuinely curious he is about what matters to each of us as people. He doesn't just care about our work deliverables - he wants to understand what drives us, what we value, what makes us tick. There's something deeply moving about a leader who approaches his team with such authentic humanity.\n\nThis time around, we talked about what kinds of work energize me, how I approach problems, and what my natural working style looks like. I honestly thought we'd discuss that internal position change proposal I got from our VP, but instead we ended up having a much more fundamental conversation about who I am as a person.\n\n# Finding Common Ground: How We Both Approach Work\n\nMy manager shared something really interesting with me. He said that if you want to change positions, you need to make the person who has the power to decide that change naturally feel like it's necessary. This really resonated with me - it's exactly how I think about things too.\n\nWhen I tackle projects, I don't usually wait until I have everything perfectly planned out. Instead, I like to jump in, figure out what's actually needed as I go, and then naturally guide the people around me toward the solutions I think will work best. It's about helping others see the direction I'm thinking and making it feel like their choice to go that way.\n\nTalking it through, we realized we share another trait: we're both not super detail-oriented people. Most of the software developers I work with are incredibly meticulous (they have to be - details matter a lot when you're trying to avoid bugs!), but that's just not how our brains work.\n\nWhat I'm really good at is \"getting things rolling.\" I care a lot about customer relationships and try to solve urgent requests as quickly as possible. While some developers get frustrated when customers suddenly change requirements or make last-minute requests, I see it differently.\n\nTake Sebastian, my daily sync partner in Poland - he believes life should be lived with incredibly detailed planning, and he gets really stressed when his plans get disrupted. (The guy even planned his three kids to be born in the same month for scheduling efficiency! 😄)\n\nBut I think we're here to help our customers succeed. I don't believe projects should only follow predetermined plans. There should always be buffer time, and being able to quickly adapt to customer needs is crucial.\n\n# What Actually Gets Me Excited\n\nI've been thinking deeply about what really interests me lately. I joined Sonatus as employee #4x, and now we're a global company with over 300 people across 9 offices worldwide - and we're still growing incredibly fast. Looking back at why I left a stable job to join this tiny startup, it really came down to my love for new challenges.\n\nI get bored in stable environments where there isn't much to do. I struggle in situations where I can only do assigned tasks and don't have the authority to be proactive. In the early days, I went way beyond just being a software engineer - I handled customer support, infrastructure, field systems, container management, you name it.\n\nBut as the company has grown and our systems have become more mature, roles have become much more clearly defined. While this makes total sense from a business perspective, I've been feeling like I need to find more interesting challenges.\n\nThe Field Application Engineer position offers the advantage of working with Korean customers where we already have strong relationships, so I could work with our VP from the BD team to paint bigger pictures. On the flip side, the Customer Service Engineer role would be about building relationships with overseas customers from scratch - mostly POC-stage projects right now, which means pioneering in uncharted territory.\n\nWhat really gets me excited is breaking new ground in challenging situations. Just like when I first joined Sonatus - we were short-staffed, nobody could predict if the company would succeed, but that uncertainty and challenge made it incredibly interesting and rewarding.\n\n# Wrapping Up\n\nReflecting on today's conversation, I feel like I have a clearer picture of where I've been and where I want to go. My manager's insights always give me new perspectives to think about. While it's natural for roles to evolve as organizations grow, I think the key is to keep finding work that genuinely excites you and pursuing those challenges.\n\nTomorrow I should turn these thoughts into a concrete career plan. Change can be scary, but finding new opportunities within that change has always been one of my strengths.\n\n*Writing this down and reflecting on it really helps organize my thoughts. I should definitely keep up this journaling habit.*","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2025-02-28T00:00:00.000Z","url":"/blog/en/2025-02-08-daily/"},{"id":"en/2025-03-04-blog-renewal","title":"Blog Framework Renewal with Claude 3.7 Sonnet - Done in 1 Hour!","description":"","content":"# Finally Did It! Blog Framework Renewal\n\nI finally renewed my blog framework that I've been putting off for ages! This framework was actually built in 2020, and I've just been adding posts here and there, but problems were piling up.\n\nSpecifically, I had issues with overlapping content across different categories, messy date formats, and inconsistent organization. I kept thinking \"I'll fix this when I have time...\" and today I finally tackled it.\n\nBut something amazing happened. I solved ALL the problems in just **1 hour**! How was this possible? Thanks to **Claude 3.7 Sonnet** and Agent technology!\n\n## The Amazing Power of Claude 3.7 Sonnet + Agent\n\nClaude has been one of the AI models I've been keeping an eye on lately. This new Claude 3.7 Sonnet from Anthropic is supposed to have incredible coding abilities, so I wanted to test it out.\n\nI thought my blog's issues would be a perfect test case, so I used the Agent feature to let it directly read and modify files.\n\nAt first I was skeptical - \"Will this actually work?\" But wow, I was blown away. Claude instantly understood my blog's structure, identified the problems one by one, and provided solutions. What's even more amazing is that it directly modified the code and ran Jekyll builds to show me the results immediately.\n\nIt quickly solved these issues:\n\n1. **File Collision Problems**: Files in the 'Daily' category were conflicting with each other, so it added permalinks to give each file a unique URL.\n2. **Category Case Consistency**: Categories like 'deepthinking' and 'daily' were lowercase while others like 'TechSavvy' were camelCase, so it standardized everything with proper capitalization.\n3. **Docker-Based Environment**: Switched from local Ruby setup (which was a pain) to Docker so I can build in the same environment anywhere.\n\n## Why I Switched to Docker\n\nPreviously, setting up the local Ruby environment and running Jekyll was different on every OS, which was quite annoying. It was fine on macOS, but working on my company Windows laptop meant dealing with environment setup every time.\n\nWith Claude's suggestion to adopt Docker, this problem completely disappeared. Now I can just run `docker-compose up` in any environment and the blog runs locally immediately. Plus, the build process is now standardized with `docker-compose run build`.\n\nWhat I love most is that I'm now completely free from Ruby version and gem dependency issues. I can focus purely on writing without worrying about environment configuration.\n\n## The New Workflow\n\nHere's how my new Docker-based workflow looks:\n\n```bash\n# Start development server\ndocker-compose up\n\n# Build the site\ndocker-compose run build\n\n# Clean build artifacts\ndocker-compose run clean\n```\n\nSuper simple and consistent across all platforms!\n\n## Claude's Problem-Solving Process\n\nWhat impressed me most was Claude's systematic approach to problem-solving:\n\n1. **Analysis Phase**: It first scanned through all my blog files and identified patterns and inconsistencies\n2. **Planning Phase**: It created a comprehensive plan addressing each issue with specific solutions\n3. **Implementation Phase**: It actually modified the files, showing me exactly what changed\n4. **Verification Phase**: It built the site and verified that all issues were resolved\n\nThis is exactly how I would approach the problem myself, but Claude did it way faster and more thoroughly.\n\n## Technical Improvements Made\n\n### 1. Permalink Standardization\nBefore:\n```yaml\n# No permalink - causing conflicts\ntitle: \"2025-03-04 blog renewal\"\n```\n\nAfter:\n```yaml\ntitle: \"2025-03-04 blog renewal\"\npermalink: \"/posts/2025-blog-renewal/\"\n```\n\n### 2. Category Naming Convention\nBefore:\n```yaml\ncategories: [\"deepthinking\", \"daily\"]  # Inconsistent\n```\n\nAfter:\n```yaml\ncategories: [\"DeepThinking\", \"Daily\"]  # Consistent PascalCase\n```\n\n### 3. Docker Configuration\nCreated a clean `docker-compose.yml`:\n```yaml\nversion: '3'\nservices:\n  jekyll:\n    image: jekyll/jekyll:latest\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/srv/jekyll\n    command: jekyll serve --watch --drafts --incremental\n```\n\n## What This Means for AI-Assisted Development\n\nThis experience really opened my eyes to what's possible with modern AI coding assistants. Claude didn't just provide suggestions - it actually understood the context, identified root causes, and implemented comprehensive solutions.\n\nKey takeaways:\n- **Context Understanding**: Claude grasped the entire project structure instantly\n- **Problem Identification**: It found issues I hadn't even noticed\n- **End-to-End Solutions**: Not just advice, but actual implementation\n- **Quality Assurance**: It verified its own work by building and testing\n\n## Future Plans\n\nNow that the framework is solid, I'm planning to:\n1. **Write More Consistently**: No more environment setup friction\n2. **Improve Content Organization**: Better tagging and categorization\n3. **Add New Features**: Maybe search functionality or better navigation\n4. **Mobile Optimization**: Make sure it looks great on all devices\n\n## Reflection\n\nThis whole experience made me realize how much friction can kill productivity. I'd been putting off this blog renewal for literally years because the setup was annoying. With Docker and Claude's help, what seemed like a weekend project became a 1-hour task.\n\nIt's a great reminder that sometimes the right tools can completely transform how we approach problems. Instead of fighting with environment issues, I can now focus on what actually matters - writing and sharing ideas.\n\n*Now I have no excuse not to blog more often! 😅*","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Blog","AI","Claude","LLM"],"pubDate":"2025-03-03T15:00:00.000Z","url":"/blog/en/2025-03-04-blog-renewal/"},{"id":"en/2025-03-05-docker-automotive-embedded-lessons-1","title":"Docker in Automotive Embedded Systems: Lessons from the Field (Part 1)","description":"","content":"![Docker in Automotive Embedded Systems](/assets/img/docker.jpeg)\n\n# Docker in Automotive Embedded Systems: Lessons from the Field (Part 1)\n\nHey there! I'm starting a new series where I'll share the various challenges and solutions I've encountered while implementing Docker in automotive embedded systems. This series will cover real-world problems I've faced and the lessons learned from solving them. This first post is about the subtle interactions between Docker containers and mount points - and boy, did this one keep me up at night! 😅\n\n## 🕵️‍♀️ The Mystery: \"The Case of the Vanishing Files\"\n\nOne day, something really weird happened in our automotive embedded system. After a reboot, I launched a Docker container and... all the files that were there before had just vanished! The container started up just fine, but all the files in the mounted directory had disappeared into thin air. Like ghosts! 👻\n\n## 🔍 Root Cause Analysis: The Critical Relationship Between Docker Startup and Mount States\n\nAfter some detective work, I found out this issue was deeply related to **system boot sequence timing** in embedded environments.\n\nHere's what was happening:\n\n1. Our system starts the Docker daemon through init.d scripts\n2. The system also mounts network storage via NFS during boot time  \n3. **The crucial discovery**: Depending on when Docker starts, containers can reference completely different mount states!\n4. If Docker daemon starts first and storage gets mounted later, containers end up referencing empty directories or stale states\n5. This makes containers think \"there are no files here\" after reboot\n\nThis might seem like a simple timing issue, but it's actually a **critical system architecture design problem**. In automotive embedded environments, service dependencies and startup ordering are absolutely crucial.\n\n## 💡 Temporary Fix: Mount Verification Before Docker Startup\n\nTo solve this, I added mount verification logic to the docker.init script:\n\n```bash\n# Storage volume mount status check\ntrials_mount=5\nwhile [ $trials_mount -gt 0 ]; do\n    dfoutput=$(df -PTh mount_path)\n    echo \"df output: $dfoutput\" >/dev/kmsg\n    \n    if echo \"$dfoutput\" | grep -q \"nfs\"; then\n        echo \"NFS mount confirmed, starting Docker daemon\" >/dev/kmsg\n        break\n    else\n        echo \"NFS not ready, waiting... (attempts left: $trials_mount)\" >/dev/kmsg\n        trials_mount=$((trials_mount-1))\n        sleep 2\n    fi\ndone\n\nif [ $trials_mount -eq 0 ]; then\n    echo \"ERROR: NFS mount failed after multiple attempts\" >/dev/kmsg\n    exit 1\nfi\n\n# Now start Docker daemon\nstart-stop-daemon --start --quiet --pidfile $DOCKER_PIDFILE --exec $DOCKER_DAEMON -- $DOCKER_OPTS\n```\n\n## 🔧 The Real Solution: Proper Service Dependencies\n\nWhile the above fix worked as a band-aid, the real solution was implementing proper service dependencies. In automotive systems, we need to guarantee that:\n\n1. **Network services** are fully operational\n2. **Storage systems** are mounted and verified  \n3. **Only then** should containerized services start\n\nHere's how I restructured the service dependencies:\n\n```bash\n# In our systemd service file\n[Unit]\nDescription=Docker Application Container Engine\nAfter=network-online.target storage-mount.target\nWants=network-online.target storage-mount.target\nRequires=storage-mount.target\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd\nRestart=on-failure\n```\n\n## 📝 Key Takeaways\n\n1. **Boot sequence matters**: In embedded systems, the order of service startup can make or break your application.\n\n2. **Mount states are tricky**: Docker containers \"remember\" the mount state from when they were created/started.\n\n3. **Always verify dependencies**: Don't assume external resources are ready just because your service started.\n\n4. **Automotive = reliability**: In automotive applications, these kinds of race conditions can be catastrophic.\n\n## 🚀 What's Next?\n\nIn the next part of this series, I'll talk about another fun challenge: handling container updates in automotive systems where downtime isn't really an option. Spoiler alert: it involves some creative juggling with container orchestration!\n\nHave you run into similar timing issues with Docker in embedded systems? I'd love to hear about your experiences and how you solved them!\n\n*This was definitely one of those \"why is this so hard?\" moments that taught me a lot about the intricacies of system integration. The automotive world doesn't forgive timing issues!*","categories":["TechSavvy","Container"],"tags":["Docker","Embedded","Automotive","Linux","Container"],"pubDate":"2025-03-05T00:00:00.000Z","url":"/blog/en/2025-03-05-docker-automotive-embedded-lessons-1/"},{"id":"en/2025-03-14-capabilities","title":"Wrestling with Linux Capabilities: A Day in the Build System Trenches","description":"","content":"![Linux Capabilities](/assets/img/linux_capabilities.jpeg)\n\n# Wrestling with Linux Capabilities: A Day in the Build System Trenches\n\nHey there! Today I want to share an interesting problem I ran into with Linux capabilities in our build system and how I worked through it. \n\n## 🏗️ Understanding Our CM Architecture and OTA System\n\nOur Container Manager (CM) has a pretty unique architecture. The CM is designed to build CAPs (Container Application Packages) directly on the target device. This isn't just a random implementation choice - it's actually a strategic decision to efficiently combine our existing OTA (Over-The-Air) update structure with Docker's layered filesystem concept.\n\nDocker's layered filesystem organizes images into multiple layers to optimize storage efficiency and build times. By combining this concept with our OTA update system, we can update specific containers or applications without having to redeploy the entire system. This approach provides crucial benefits for embedded systems with limited bandwidth and storage.\n\nBut this architecture presents some interesting challenges in our build system, especially when it comes to special permission settings like Linux capabilities! 🤔\n\n## 🔍 Digging Into the Problem\n\nLet me show you what was happening in our recipe code:\n\n```bash\n# Setting capabilities on EIDS binary\ndo_set_capabilities() {\n    local image_name=$(find ${S} -type d -maxdepth 1 -mindepth 1 -exec basename {} \\;)\n    \n    bbdebug 1 \"Setting capabilities for image: ${image_name}\"\n    \n    # Find the EIDS binary\n    local eids_binary=$(find ${S}/${image_name}/rootfs -name \"eids\" -type f)\n    \n    if [ -n \"${eids_binary}\" ]; then\n        bbdebug 1 \"Found EIDS binary at: ${eids_binary}\"\n        \n        # Set capabilities\n        setcap cap_net_raw,cap_net_admin+ep \"${eids_binary}\"\n        \n        # Verify capabilities were set\n        getcap \"${eids_binary}\"\n    else\n        bbwarn \"EIDS binary not found in ${S}/${image_name}/rootfs\"\n    fi\n}\n```\n\nThe issue was that `setcap` was failing during the build process. After some investigation, I realized that our build environment didn't have the necessary capabilities to set extended attributes on files.\n\n## 🛠️ The Solution Journey\n\nHere's how I tackled this step by step:\n\n### Step 1: Understanding the Root Cause\nThe problem was that our Yocto build environment was running inside a container that didn't have `CAP_SETFCAP` capability, which is required to set file capabilities.\n\n### Step 2: Build Environment Fix\nI had to modify our build container to include the necessary capabilities:\n\n```dockerfile\n# In our build container\nRUN apt-get update && apt-get install -y libcap2-bin\n\n# Run container with additional capabilities\ndocker run --cap-add=SETFCAP ...\n```\n\n### Step 3: Recipe Improvements\nI also made the capability setting more robust:\n\n```bash\ndo_set_capabilities() {\n    local image_name=$(find ${S} -type d -maxdepth 1 -mindepth 1 -exec basename {} \\;)\n    \n    bbdebug 1 \"Setting capabilities for image: ${image_name}\"\n    \n    # Find the EIDS binary\n    local eids_binary=$(find ${S}/${image_name}/rootfs -name \"eids\" -type f)\n    \n    if [ -n \"${eids_binary}\" ]; then\n        bbdebug 1 \"Found EIDS binary at: ${eids_binary}\"\n        \n        # Check if we can set capabilities\n        if ! setcap -v cap_net_raw,cap_net_admin+ep \"${eids_binary}\" 2>/dev/null; then\n            bbwarn \"Failed to set capabilities on ${eids_binary}\"\n            bbwarn \"This may be due to filesystem or container limitations\"\n            return 1\n        fi\n        \n        # Verify capabilities were set\n        local caps=$(getcap \"${eids_binary}\")\n        bbdebug 1 \"Capabilities set: ${caps}\"\n    else\n        bbwarn \"EIDS binary not found in ${S}/${image_name}/rootfs\"\n        return 1\n    fi\n}\n```\n\n## 🎯 Key Takeaways\n\n1. **Container Capabilities Matter**: When building inside containers, you need to be mindful of what capabilities your build environment has.\n\n2. **Filesystem Support**: Not all filesystems support extended attributes. Make sure your build environment uses a compatible filesystem.\n\n3. **Error Handling**: Always add proper error handling and debugging output to make troubleshooting easier.\n\n4. **Documentation**: These kinds of issues are perfect candidates for documentation - future you (or your teammates) will thank you!\n\n## 🚀 What's Next?\n\nI'm planning to create a more comprehensive guide about handling Linux capabilities in containerized build environments. There are definitely more edge cases to explore, especially around cross-compilation and different target architectures.\n\nHave you run into similar issues with capabilities in your build systems? I'd love to hear about your experiences and solutions!\n\n*This was definitely one of those \"fun\" debugging sessions that teaches you something new about the Linux security model. Always learning something new in this field!*","categories":["TechSavvy","Container"],"tags":["Linux","Container","Capabilities","Yocto","Docker","Security"],"pubDate":"2025-03-14T00:00:00.000Z","url":"/blog/en/2025-03-14-capabilities/"},{"id":"en/2025-06-25-claude-code-installation-guide","title":"Installing Claude Code in Existing Linux Containers (Battle-Tested Guide)","description":"","content":"![Claude Code Installation Guide](/assets/img/claude.jpeg)\n\n# Installing Claude Code in Existing Linux Containers (Battle-Tested Guide)\n\n## Table of Contents\n1. [Claude Code Overview](#claude-code-overview)\n2. [Accessing Running Containers](#accessing-running-containers)\n3. [System Environment Check](#system-environment-check)\n4. [Node.js Installation (NVM Recommended)](#nodejs-installation-nvm-recommended)\n5. [Claude Code Installation](#claude-code-installation)\n6. [Authentication Setup](#authentication-setup)\n7. [Usage](#usage)\n8. [Troubleshooting](#troubleshooting)\n9. [Verified Installation Script](#verified-installation-script)\n\n## Claude Code Overview\n\nClaude Code is a terminal-based AI coding tool developed by Anthropic that provides:\n- Edit files and fix bugs across your entire codebase\n- Answer questions about code architecture and logic\n- Run and fix tests, linting, and other commands\n- Manage Git workflows (resolve merge conflicts, create PRs, etc.)\n- Explore docs and resources through web search\n\n**System Requirements**: Node.js 18+ (Recommended: 20+ LTS)\n\n## Accessing Running Containers\n\n### 1. Check Container List\n```bash\n# Check running containers\ndocker ps\n\n# Example output:\nCONTAINER ID   IMAGE     COMMAND       CREATED        STATUS        PORTS     NAMES\nabc123def456   ubuntu    \"/bin/bash\"   2 hours ago    Up 2 hours              my-dev-container\n```\n\n### 2. Access Container\n```bash\n# Access container using container name\ndocker exec -it my-dev-container /bin/bash\n\n# Or using container ID\ndocker exec -it abc123def456 /bin/bash\n\n# If bash is not available, try sh\ndocker exec -it my-dev-container /bin/sh\n```\n\n## System Environment Check\n\nOnce inside the container, let's check what we're working with:\n\n```bash\n# Check OS version\ncat /etc/os-release\n\n# Check available package managers\nwhich apt-get    # Debian/Ubuntu\nwhich yum        # RHEL/CentOS\nwhich apk        # Alpine\n\n# Check if curl/wget is available\nwhich curl\nwhich wget\n\n# Check current shell\necho $SHELL\n```\n\n## Node.js Installation (NVM Recommended)\n\n### Why NVM?\n- Easy version management\n- No root permissions needed\n- Works in most container environments\n\n### Install NVM and Node.js\n```bash\n# Install NVM\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\n\n# Reload shell configuration\nsource ~/.bashrc\n# or\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n\n# Install latest LTS Node.js\nnvm install --lts\nnvm use --lts\n\n# Verify installation\nnode --version\nnpm --version\n```\n\n### Alternative: Direct Node.js Installation\n\nFor **Ubuntu/Debian**:\n```bash\n# Update package list\napt-get update\n\n# Install Node.js\ncurl -fsSL https://deb.nodesource.com/setup_20.x | bash -\napt-get install -y nodejs\n\n# Verify\nnode --version\nnpm --version\n```\n\nFor **Alpine Linux**:\n```bash\n# Install Node.js\napk add --no-cache nodejs npm\n\n# Verify\nnode --version\nnpm --version\n```\n\n## Claude Code Installation\n\n### Method 1: NPM Installation\n```bash\n# Install Claude Code globally\nnpm install -g @anthropic-ai/claude-code\n\n# Verify installation\nclaude --version\n```\n\n### Method 2: Direct Download (if npm fails)\n```bash\n# Download and install\ncurl -fsSL https://claude.ai/install.sh | sh\n\n# Add to PATH if needed\nexport PATH=\"$HOME/.claude/bin:$PATH\"\necho 'export PATH=\"$HOME/.claude/bin:$PATH\"' >> ~/.bashrc\n```\n\n## Authentication Setup\n\n### 1. Get API Key\n1. Visit [Claude.ai](https://claude.ai)\n2. Sign in to your account\n3. Go to Settings → API Keys\n4. Generate a new API key\n\n### 2. Configure Authentication\n```bash\n# Set up authentication\nclaude auth login\n\n# Or set environment variable\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\necho 'export ANTHROPIC_API_KEY=\"your-api-key-here\"' >> ~/.bashrc\n```\n\n### 3. Verify Authentication\n```bash\n# Test connection\nclaude --test-auth\n```\n\n## Usage\n\n### Basic Commands\n```bash\n# Start interactive session\nclaude\n\n# Analyze specific files\nclaude analyze src/main.js\n\n# Fix issues in files\nclaude fix src/\n\n# Generate code\nclaude generate \"create a REST API endpoint for user authentication\"\n\n# Help with Git\nclaude git \"help me resolve merge conflicts\"\n```\n\n### Example Interactive Session\n```bash\n$ claude\nClaude Code v1.0.0\nType 'help' for available commands or start describing what you'd like to do.\n\n> Can you help me fix the TypeScript errors in this project?\nI'll analyze your TypeScript files and help fix the errors. Let me start by examining your project structure...\n\n[Claude analyzes files and provides fixes]\n```\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n**1. Permission Denied Errors**\n```bash\n# If you get permission errors with npm\nnpm config set prefix ~/.npm-global\nexport PATH=~/.npm-global/bin:$PATH\necho 'export PATH=~/.npm-global/bin:$PATH' >> ~/.bashrc\n```\n\n**2. Node.js Version Issues**\n```bash\n# Check Node version\nnode --version\n\n# Update to latest LTS if needed\nnvm install --lts\nnvm use --lts\n```\n\n**3. Network/Firewall Issues**\n```bash\n# Test internet connectivity\ncurl -I https://api.anthropic.com\n\n# Check if proxy is needed\necho $HTTP_PROXY\necho $HTTPS_PROXY\n```\n\n**4. Container Environment Issues**\n```bash\n# Some containers might need additional packages\napt-get install -y ca-certificates curl gnupg\n\n# For Alpine\napk add --no-cache ca-certificates curl\n```\n\n### Debug Mode\n```bash\n# Enable debug logging\nexport CLAUDE_DEBUG=1\nclaude --version\n```\n\n## Verified Installation Script\n\nHere's a battle-tested script that handles most common scenarios:\n\n```bash\n#!/bin/bash\nset -e\n\necho \"🚀 Installing Claude Code in Container...\"\n\n# Detect OS\nif command -v apt-get >/dev/null 2>&1; then\n    OS=\"ubuntu\"\n    PACKAGE_MANAGER=\"apt\"\nelif command -v yum >/dev/null 2>&1; then\n    OS=\"centos\"\n    PACKAGE_MANAGER=\"yum\"\nelif command -v apk >/dev/null 2>&1; then\n    OS=\"alpine\"\n    PACKAGE_MANAGER=\"apk\"\nelse\n    echo \"❌ Unsupported OS\"\n    exit 1\nfi\n\necho \"📊 Detected OS: $OS\"\n\n# Install dependencies\necho \"📦 Installing dependencies...\"\ncase $PACKAGE_MANAGER in\n    \"apt\")\n        apt-get update\n        apt-get install -y curl ca-certificates gnupg\n        ;;\n    \"yum\")\n        yum install -y curl ca-certificates\n        ;;\n    \"apk\")\n        apk add --no-cache curl ca-certificates\n        ;;\nesac\n\n# Install Node.js via NVM\necho \"📦 Installing Node.js...\"\nif ! command -v node >/dev/null 2>&1; then\n    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\n    export NVM_DIR=\"$HOME/.nvm\"\n    [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n    nvm install --lts\n    nvm use --lts\nelse\n    echo \"✅ Node.js already installed: $(node --version)\"\nfi\n\n# Install Claude Code\necho \"🤖 Installing Claude Code...\"\nnpm install -g @anthropic-ai/claude-code\n\n# Verify installation\necho \"✅ Verifying installation...\"\nclaude --version\n\necho \"🎉 Installation complete!\"\necho \"💡 Next steps:\"\necho \"   1. Run 'claude auth login' to authenticate\"\necho \"   2. Run 'claude' to start using Claude Code\"\n```\n\n### Save and Run the Script\n```bash\n# Save the script\ncat > install-claude.sh << 'EOF'\n[paste the script above]\nEOF\n\n# Make executable and run\nchmod +x install-claude.sh\n./install-claude.sh\n```\n\n## Final Notes\n\n### Best Practices\n1. **Always test in a non-production container first**\n2. **Keep your API key secure** - never hardcode it in scripts\n3. **Use environment variables** for configuration\n4. **Regularly update Claude Code** for new features and fixes\n\n### Container Persistence\nRemember that changes made inside a container will be lost when the container is removed unless:\n- You commit the container to a new image\n- You use volumes to persist data\n- You rebuild your Dockerfile to include these installations\n\n### Quick Container Setup for Development\n```bash\n# Create a development container with Claude Code pre-installed\ndocker run -it --name claude-dev ubuntu:22.04 /bin/bash\n\n# Inside container, run the installation script\n# Then commit the container\ndocker commit claude-dev my-claude-dev:latest\n```\n\n*Happy coding with Claude! 🚀 This guide has been tested on Ubuntu, Alpine, and CentOS containers.*","categories":["TechSavvy","Container"],"tags":["Claude","AI","Docker","Linux","Container","Node.js","Installation"],"pubDate":"2025-06-25T00:00:00.000Z","url":"/blog/en/2025-06-25-claude-code-installation-guide/"},{"id":"en/2025-06-25-devcontainer-mcp-bootstrapper","title":"Automating Dev Environment Setup: Building a DevContainer MCP Bootstrapper","description":"","content":"![DevContainer MCP Bootstrapper](/assets/img/docker.jpeg)\n\n# Automating Dev Environment Setup: Building a DevContainer MCP Bootstrapper\n\nHey there! Today I want to talk about this cool project I recently built: the [DevContainer MCP Bootstrapper](https://github.com/jayleekr/devcontainer-mcp-bootstrapper).\n\nIt's a bootstrapper that automatically installs Claude MCP (Model Context Protocol) servers and sets up all your dev tools in one go. There were some pretty interesting stories during the development process that I'd love to share!\n\n## 🤔 Why Did I Build This?\n\nI had this realization while writing the Claude Code installation guide recently. Setting up development environments in containers every single time was getting ridiculously tedious.\n\nI kept running into these repetitive situations:\n\n1. **Every time I created a new DevContainer**: Had to redo Git configs, shell aliases, Vim settings, etc.\n2. **Claude MCP setup**: Manually installing and configuring Context7 and Supermemory MCP servers each time\n3. **Dev tools**: Constantly re-setting up Docker aliases and useful functions\n\nAll this repetitive work was driving me nuts, so I thought \"Let's just automate everything in one step!\"\n\n## 🚀 What Did I Automate?\n\n### Core Features\n\n**1. Automatic Claude MCP Server Installation**\n- Context7: Latest docs and code example search\n- Supermemory: Personal memory management between AI tools\n\n**2. Development Tool Configuration**\n- Git global settings and useful aliases\n- Productivity-boosting shell aliases and functions\n- Basic Vim configuration\n- Docker shortcuts and utilities\n\n**3. Environment Personalization**\n- Zsh with Oh My Zsh setup\n- Custom prompt themes\n- Useful bash functions for daily development\n\n## 🛠️ How Does It Work?\n\nThe magic happens through a simple curl command:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash\n```\n\nHere's what happens under the hood:\n\n### Step 1: Environment Detection\n```bash\n# Detect the operating system and package manager\ndetect_os() {\n    if command -v apt-get >/dev/null 2>&1; then\n        OS=\"ubuntu\"\n        PACKAGE_MANAGER=\"apt\"\n    elif command -v yum >/dev/null 2>&1; then\n        OS=\"centos\"\n        PACKAGE_MANAGER=\"yum\"\n    elif command -v apk >/dev/null 2>&1; then\n        OS=\"alpine\"\n        PACKAGE_MANAGER=\"apk\"\n    else\n        echo \"Unsupported OS\"\n        exit 1\n    fi\n}\n```\n\n### Step 2: MCP Server Installation\nThe script automatically downloads and configures the latest MCP servers:\n\n```bash\ninstall_mcp_servers() {\n    echo \"Installing Claude MCP servers...\"\n    \n    # Install Context7\n    if ! command -v context7 >/dev/null 2>&1; then\n        wget -O /tmp/context7 \"${CONTEXT7_URL}\"\n        chmod +x /tmp/context7\n        sudo mv /tmp/context7 /usr/local/bin/\n    fi\n    \n    # Install Supermemory\n    npm install -g @supermemory/mcp-server\n}\n```\n\n### Step 3: Development Environment Setup\n```bash\nsetup_dev_environment() {\n    # Git configuration\n    git config --global alias.st status\n    git config --global alias.co checkout\n    git config --global alias.br branch\n    git config --global alias.ci commit\n    \n    # Useful shell functions\n    cat >> ~/.bashrc << 'EOF'\n# Docker shortcuts\nalias dps='docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\"'\nalias dimg='docker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"'\nalias dclean='docker system prune -f'\n\n# Quick directory navigation\nalias ..='cd ..'\nalias ...='cd ../..'\nalias ll='ls -la'\nEOF\n}\n```\n\n## 🎯 Cool Features\n\n### Smart Configuration Detection\nThe bootstrapper is pretty smart about not overwriting existing configurations:\n\n```bash\n# Only set up Git config if not already configured\nif [ -z \"$(git config --global user.name)\" ]; then\n    echo \"Setting up Git configuration...\"\n    read -p \"Enter your Git username: \" git_username\n    read -p \"Enter your Git email: \" git_email\n    git config --global user.name \"$git_username\"\n    git config --global user.email \"$git_email\"\nelse\n    echo \"Git already configured, skipping...\"\nfi\n```\n\n### Modular Installation\nYou can pick and choose what to install:\n\n```bash\n./bootstrap.sh --mcp-only          # Only install MCP servers\n./bootstrap.sh --dev-tools-only    # Only set up dev tools\n./bootstrap.sh --full              # Everything (default)\n```\n\n### Error Handling and Rollback\nIf something goes wrong, the script can clean up after itself:\n\n```bash\ncleanup_on_error() {\n    echo \"Installation failed, cleaning up...\"\n    # Remove partially installed components\n    rm -f /usr/local/bin/context7\n    npm uninstall -g @supermemory/mcp-server\n    echo \"Cleanup completed\"\n}\n```\n\n## 📝 Lessons Learned\n\n### 1. Cross-Platform Compatibility is Tricky\nDifferent container images use different package managers and have different default configurations. I had to make the script adaptive:\n\n```bash\ninstall_package() {\n    local package=$1\n    case $PACKAGE_MANAGER in\n        \"apt\")\n            sudo apt-get install -y \"$package\"\n            ;;\n        \"yum\")\n            sudo yum install -y \"$package\"\n            ;;\n        \"apk\")\n            sudo apk add \"$package\"\n            ;;\n    esac\n}\n```\n\n### 2. User Experience Matters\nInitially, the script was completely silent during installation. I learned that users want to see what's happening:\n\n```bash\nshow_progress() {\n    local current=$1\n    local total=$2\n    local desc=$3\n    echo \"[$current/$total] $desc\"\n}\n```\n\n### 3. Configuration Backup is Essential\nAlways backup existing configurations before modifying:\n\n```bash\nbackup_config() {\n    local config_file=$1\n    if [ -f \"$config_file\" ]; then\n        cp \"$config_file\" \"${config_file}.backup.$(date +%Y%m%d-%H%M%S)\"\n        echo \"Backed up $config_file\"\n    fi\n}\n```\n\n## 🚀 What's Next?\n\nI'm planning to add more features:\n\n1. **IDE Integration**: Automatic VS Code extensions and settings\n2. **Language-Specific Tools**: Go, Python, Node.js development environments\n3. **Team Configurations**: Shared team settings and tools\n4. **Cloud Integration**: Automatic cloud CLI setup (AWS, GCP, Azure)\n\n## 💡 Pro Tips\n\nIf you're building similar automation tools:\n\n1. **Make it idempotent**: Running the script multiple times should be safe\n2. **Provide good feedback**: Users want to know what's happening\n3. **Handle failures gracefully**: Always have a cleanup strategy\n4. **Test on multiple environments**: What works on Ubuntu might not work on Alpine\n\n## 🎉 Try It Out!\n\nIf you're tired of manually setting up dev environments like I was, give it a shot:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash\n```\n\nThe whole thing takes about 2-3 minutes and you'll have a fully configured development environment ready to go!\n\n*Life's too short to manually configure the same dev environment over and over again! 🚀*","categories":["TechSavvy","Container"],"tags":["DevContainer","MCP","Docker","Claude","AI","Bootstrap","Automation","Development"],"pubDate":"2025-06-25T01:00:00.000Z","url":"/blog/en/2025-06-25-devcontainer-mcp-bootstrapper/"},{"id":"en/2025-07-24-ai-workflow-productivity","title":"AI Multiplexing Workflow: ADHD-Level Productivity Boost (Part 1)","description":"Sharing workflow strategies and productivity insights from running multiple AI tools simultaneously","content":"![AI Workflow Productivity](/assets/img/ai-workflow.jpg)\n\n# AI Multiplexing Workflow: ADHD-Level Productivity Boost (Part 1)\n\nHey there! Today I want to share something a bit different.\n\nThese days, when I'm at my computer, I feel like I have **ADHD at its peak** - running multiple AI tools simultaneously. I want to share my journey of finding some order in this chaos and building a system that actually works.\n\n## 🤖 My Current AI Stack\n\nI'm spending **over $100 per month** running these tools daily:\n\n- **Cursor Pro** - Main coding environment\n- **ChatGPT Pro** - Emotional labor and customer support (lol)\n- **Claude Code** - Aggressive coding tasks\n- **Gemini CLI** - Documentation specialist\n- **Gemini Pro 2.5** - Research and new project planning\n\nWhile I'm definitely using them heavily, I've been **teaching AI workshops weekly**, which has helped me develop some solid know-how. I can feel my productivity gradually improving as I refine my approach.\n\nI think I'm getting the hang of it, so I decided to document this journey.\n\n## 🎯 Overall Workflow Overview\n\n```mermaid\ngraph TB\n    subgraph \"AI Stack (Monthly $100+)\"\n        A1[\"🔵 Cursor Pro<br/>Main Coding\"]\n        A2[\"🟢 ChatGPT Pro<br/>Emotional Labor & Customer Support\"]\n        A3[\"🟣 Claude Code<br/>Aggressive Coding\"]\n        A4[\"🔴 Gemini CLI<br/>Documentation Specialist\"]\n        A5[\"🟡 Gemini Pro 2.5<br/>Research & Planning\"]\n    end\n    \n    subgraph \"Common Workflow\"\n        B1[\"1️⃣ Fresh Context Window<br/>Start New\"]\n        B2[\"2️⃣ Organized Context<br/>Provide Structure\"]\n        B3[\"3️⃣ Response Format<br/>Make Them Think\"]\n        B4[\"4️⃣ Clear<br/>Request\"]\n        B5[\"5️⃣ Feedback<br/>Process\"]\n        B6[\"6️⃣ Context Document<br/>Updates\"]\n        \n        B1 --> B2 --> B3 --> B4 --> B5 --> B6\n    end\n    \n    subgraph \"Workspace Configuration (5-8 instances)\"\n        C1[\"📡 Remote SSH\"]\n        C2[\"💻 Native\"]\n        \n        C1 --> C11[\"Product Code\"]\n        C1 --> C12[\"Build Server 1-2\"]\n        C1 --> C13[\"Test Environment 1-2\"]\n        \n        C2 --> C21[\"Blog/Documentation\"]\n        C2 --> C22[\"Teaching Materials 1-2\"]\n    end\n    \n    subgraph \"Tool-Specific Usage\"\n        D1[\"Claude Code<br/>💸 3-hour limit\"] --> D11[\"Aggressive<br/>Coding Tasks\"]\n        D2[\"Cursor + Chat\"] --> D22[\"Agentic Mode<br/>Ping-ponging\"]\n        D3[\"ChatGPT Pro\"] --> D33[\"Email<br/>Emotional Labor\"]\n        D4[\"Gemini 2.5 Pro\"] --> D44[\"Documentation<br/>Excellence\"]\n    end\n    \n    A1 -.-> B1\n    A3 -.-> D1\n    A4 -.-> D4\n    \n    style A1 fill:#e1f5fe\n    style A2 fill:#e8f5e8\n    style A3 fill:#f3e5f5\n    style A4 fill:#ffebee\n    style A5 fill:#fffde7\n```\n\nThis diagram shows the complete structure of my current AI multiplexing workflow!\n\n## 📋 Common Workflow Applied to All LLMs\n\nFrom experience, regardless of which AI I'm using, this approach consistently produces better results:\n\n### 1. Frequently Open Fresh Context Windows\n- Don't hesitate to clean up long conversations and start fresh\n- Focused conversations are much more effective\n\n### 2. Provide Well-Organized Context to Make Them Think\n- Don't ask for immediate answers - let them understand the situation first\n- The key is starting with keyword extraction\n\n### 3. Present Good Response Formats or Make Them Think About It\n- Instead of \"respond in this format\"\n- Try \"first think about what format would work best for this response\"\n\n### 4. Make Clear Requests\n- Vague requests lead to vague answers\n- Ask for specific, measurable deliverables\n\n### 5. Use Feedback Process to Keep Them Thinking\n- Don't expect perfect answers on the first try\n- Use feedback for gradual improvement\n\n### 6. Continuously Update Context Documents\n- Keep markdown files with project config, workflow, etc.\n- Continuously improve them so AI can understand better\n\n## 🛠️ Actual Workspace Operation Method\n\n**Every workspace is a git repo** where I launch cursor instances. Usually have **5-8 running**:\n\n### Remote Development (SSH-based)\n- **Product Code** (remote dev-container via remote ssh)\n- **Production image build server workspace** 1-2 instances\n- **Test environment workspace** 1-2 instances\n\n### Native Development\n- **Blog/Documentation workspace** \n- **Teaching materials workspace** 1-2 instances (since I have multiple courses...)\n\n## 🎯 Tool-Specific Usage Strategies\n\n### Claude Code ($100 tier)\nI only use this for **aggressive coding work** on about 2 repositories.\n\n> 💸 But even Claude Code has **usage limits that reset every 3 hours**, so it's not enough... Having these restrictions even after paying premium prices is frustrating ㅠㅠ\n\n### Cursor + Chat\nFor tasks that require **iterative code improvement through conversation**, I use Agentic Mode or ping-ponging.\n\nWhen dealing with actual client work, I mostly use **Cursor + Claude Sonnet 4.0** or **Gemini 2.5 Pro**.\n\n### ChatGPT Pro\nHonestly, I don't use it much and I'm **considering canceling the subscription**... but it's still really convenient for **customer support emails and emotional labor**. Maybe that alone makes it worth it? lol\n\n### Gemini 2.5 Pro\nOne thing's for sure: **As of January 2025, Gemini 2.5 Pro is absolutely excellent for documentation**.\n\nSo for documentation work, I prefer using **Gemini CLI**.\n\n## 📝 Secret for Starting New Projects\n\nWhen starting new projects, I focus on properly writing **PRD (Product Requirements Document)**.\n\nOf course, creating a complete PRD myself is challenging, so I use **Gemini 2.5 Pro for research**. To help AI agents that sometimes work well and sometimes don't perform more consistently, I researched PRD methodologies.\n\nI've also created [PRD Methodology and Templates](https://www.notion.so/PRD-22c248114595800487e7efbdb115a8a7?pvs=21) - check it out if you're interested!\n\n## 🔚 Wrapping Up...\n\nHonestly, I only started writing this **while waiting for Claude Code usage to reset**... lol\n\nIn the next post, I'll dive deeper into specific prompt strategies for each tool and real project application cases.\n\nHow are you all using AI tools? Please share in the comments!\n\n---\n\n*This post is based on real experiences from daily AI tool usage and hopes to help fellow developers facing similar challenges.*","categories":["Tech","AI"],"tags":["AI","Productivity","Cursor","ChatGPT","Claude","Gemini","Workflow"],"pubDate":"2025-07-24T00:00:00.000Z","url":"/blog/en/2025-07-24-ai-workflow-productivity/"},{"id":"en/building-scalable-microservices-with-kubernetes","title":"My Microservices Journey: A Year of Struggles and Discoveries","description":"An honest account of adopting microservices architecture for autonomous vehicle platforms, based on real-world experience with both failures and successes","content":"# My Microservices Journey: A Year of Struggles and Discoveries\n\nHello everyone! Today I want to honestly share my microservices adoption journey over the past year. There might be more failure stories than success stories, but I'm writing this with courage because I think it might help someone out there.\n\n## It started like this: \"We've hit some kind of wall\"\n\nAbout two years ago, our team's vehicle data platform started growing, and we began encountering really frustrating situations:\n\n- **30 minutes for one deployment**: I'd go get coffee and it would still be deploying when I came back 😅\n- **One error brings down everything**: Getting failure calls at dawn became part of daily life\n- **Coordinating schedules with other teams**: We heard \"please deploy when our team finishes development\" way too often\n\nAt first, I thought \"this isn't too bad, right?\" But as the team grew and services became more complex, I definitely felt the limitations. So we decided to transition to microservices.\n\n## First big mistake: Wrong service decomposition\n\n### I learned too late that you shouldn't split by technology\n\nWhen we decided to adopt microservices, the first thing we did was figure out \"how to split the services?\" Initially, I thought really simply:\n\n**How we first split them** (looking back, it was really the wrong approach):\n```\n❌ database-service (everything DB-related)\n❌ api-service (everything API-related)\n❌ auth-service (authentication-related)  \n❌ notification-service (notification-related)\n```\n\nAfter using this for a few months, it was really frustrating. To add one feature, we had to modify multiple services simultaneously, and ultimately had to deploy them together repeatedly.\n\n**So we changed it to this** (business-centered approach):\n```\n✅ vehicle-management (everything about vehicle management)\n✅ trip-analytics (trip analysis-related)\n✅ user-profiles (user profiles)\n✅ billing-payments (payment-related)\n```\n\nAfter making this change, each team could develop independently. It made a really big difference!\n\n#### Actual domain decomposition example\n\n```typescript\n// Domain decomposition for autonomous driving platform\ninterface DomainBoundaries {\n  vehicleFleet: {\n    responsibilities: ['vehicle registration', 'status monitoring', 'firmware management'];\n    dataOwnership: ['vehicles', 'sensors', 'diagnostics'];\n    apis: ['/vehicles', '/fleet/status', '/diagnostics'];\n  };\n  \n  tripManagement: {\n    responsibilities: ['trip creation', 'route optimization', 'real-time tracking'];\n    dataOwnership: ['trips', 'routes', 'locations'];\n    apis: ['/trips', '/routes', '/tracking'];\n  };\n  \n  userExperience: {\n    responsibilities: ['user interface', 'notifications', 'feedback'];\n    dataOwnership: ['users', 'preferences', 'feedback'];\n    apis: ['/users', '/notifications', '/feedback'];\n  };\n}\n```\n\n### 📊 Data consistency strategy\n\n#### Implementing event sourcing pattern\n\n```typescript\n// Event-based data synchronization\ninterface DomainEvent {\n  eventId: string;\n  aggregateId: string;\n  eventType: string;\n  timestamp: Date;\n  version: number;\n  data: any;\n}\n\nclass VehicleEventHandler {\n  async handleTripCompleted(event: DomainEvent) {\n    const { tripId, vehicleId, mileage, fuelConsumption } = event.data;\n    \n    // 1. Vehicle Service: Update vehicle status\n    await this.vehicleService.updateMileage(vehicleId, mileage);\n    \n    // 2. Analytics Service: Store trip data\n    await this.analyticsService.recordTripData({\n      tripId, vehicleId, mileage, fuelConsumption\n    });\n    \n    // 3. Billing Service: Publish billing calculation event  \n    await this.eventPublisher.publish('billing.calculate', {\n      tripId, mileage, userId: event.data.userId\n    });\n  }\n}\n```\n\n#### Managing distributed transactions with SAGA pattern\n\n```yaml\n# trip-booking-saga.yml\nsaga:\n  name: \"TripBookingSaga\"\n  steps:\n    - service: \"user-service\"\n      action: \"reserve-credits\"\n      compensate: \"release-credits\"\n      \n    - service: \"vehicle-service\"  \n      action: \"reserve-vehicle\"\n      compensate: \"release-vehicle\"\n      \n    - service: \"trip-service\"\n      action: \"create-trip\"\n      compensate: \"cancel-trip\"\n      \n    - service: \"notification-service\"\n      action: \"send-confirmation\"\n      compensate: \"send-cancellation\"\n```\n\n## Kubernetes cluster configuration\n\n### 🏗️ Infrastructure architecture\n\n```yaml\n# cluster-architecture.yml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-config\ndata:\n  # Production cluster configuration\n  nodes: |\n    master-nodes: 3 (HA configuration)\n    worker-nodes: 12 (auto-scaling)\n    \n  resources:\n    cpu: \"48 cores per node\"\n    memory: \"192GB per node\"\n    storage: \"2TB NVMe SSD\"\n    \n  networking:\n    cni: \"Calico\"\n    service-mesh: \"Istio\"\n    ingress: \"NGINX + Cert-Manager\"\n```\n\n### 📦 Service-specific deployment configuration\n\n#### 1. High availability service (Vehicle Management)\n\n```yaml\n# vehicle-service-deployment.yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vehicle-service\n  labels:\n    app: vehicle-service\n    version: v2.1.3\nspec:\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: vehicle-service\n  template:\n    metadata:\n      labels:\n        app: vehicle-service\n        version: v2.1.3\n    spec:\n      containers:\n      - name: vehicle-service\n        image: myregistry/vehicle-service:v2.1.3\n        ports:\n        - containerPort: 8080\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: url\n        - name: REDIS_URL\n          valueFrom:\n            configMapKeyRef:\n              name: cache-config\n              key: redis-url\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vehicle-service\nspec:\n  selector:\n    app: vehicle-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: ClusterIP\n```\n\n#### 2. HPA (Horizontal Pod Autoscaler) configuration\n\n```yaml\n# vehicle-service-hpa.yml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vehicle-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vehicle-service\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"100\"\n```\n\n## Lessons learned from operational experience\n\n### 💡 Success factors\n\n#### 1. Team structure and organizational alignment\n\n```mermaid\ngraph TD\n    A[Product Team] --> B[Domain Team 1: Vehicle]\n    A --> C[Domain Team 2: Trip]\n    A --> D[Domain Team 3: User]\n    \n    B --> E[Backend Developer]\n    B --> F[DevOps Engineer]  \n    B --> G[QA Engineer]\n    \n    H[Platform Team] --> I[Infrastructure]\n    H --> J[Monitoring]\n    H --> K[Security]\n```\n\n**Leveraging Conway's Law**: Recognizing that organizational structure determines architecture and designing intentionally\n\n#### 2. Gradual migration\n\n```typescript\n// Gradual transition with Strangler Fig pattern\nclass LegacyTripService {\n  async createTrip(tripData: TripData): Promise<Trip> {\n    // Branch between new/old systems with feature flags\n    if (this.featureFlag.isEnabled('NEW_TRIP_SERVICE', tripData.userId)) {\n      return this.newTripService.createTrip(tripData);\n    } else {\n      return this.legacyCreateTrip(tripData);\n    }\n  }\n  \n  private async legacyCreateTrip(tripData: TripData): Promise<Trip> {\n    // Existing monolithic logic\n  }\n}\n```\n\n#### 3. Observability-first development\n\n```typescript\n// Consider metric collection from code writing time\nclass PaymentService {\n  async processPayment(payment: Payment): Promise<PaymentResult> {\n    const timer = this.metrics.startTimer('payment_processing_duration');\n    \n    try {\n      this.metrics.increment('payment_attempts_total', {\n        payment_method: payment.method,\n        amount_range: this.getAmountRange(payment.amount)\n      });\n      \n      const result = await this.paymentGateway.charge(payment);\n      \n      this.metrics.increment('payment_success_total');\n      return result;\n      \n    } catch (error) {\n      this.metrics.increment('payment_failure_total', {\n        error_type: error.constructor.name\n      });\n      throw error;\n    } finally {\n      timer.end();\n    }\n  }\n}\n```\n\n### 🚨 Lessons learned from failures\n\n#### 1. The trap of too-small services\n\n**Problem**: Excessive network calls and complex orchestration\n\n```typescript\n// ❌ Over-granular services\ninterface MicroServices {\n  userIdService: 'generates user IDs';\n  userNameService: 'manages user names'; \n  userEmailService: 'handles user emails';\n  userPhoneService: 'manages phone numbers';\n}\n\n// ✅ Right-sized services\ninterface RightSizedServices {\n  userManagementService: 'complete user lifecycle';\n  authenticationService: 'login, logout, tokens';\n  profileService: 'user preferences, settings';\n}\n```\n\n#### 2. The danger of distributed monoliths\n\nDistributed but still tightly coupled systems:\n\n- Synchronous call chains between services\n- Shared databases\n- Need for simultaneous deployment\n\n**Solution**: Introduced event-driven architecture and CQRS patterns\n\n#### 3. The importance of testing strategy\n\n```typescript\n// Ensure service compatibility with contract testing\ndescribe('Vehicle Service Contract', () => {\n  it('should return vehicle data in expected format', async () => {\n    const pact = new Pact({\n      consumer: 'trip-service',\n      provider: 'vehicle-service'\n    });\n    \n    await pact\n      .given('vehicle exists')\n      .uponReceiving('a request for vehicle data')\n      .withRequest({\n        method: 'GET',\n        path: '/vehicles/123'\n      })\n      .willRespondWith({\n        status: 200,\n        headers: { 'Content-Type': 'application/json' },\n        body: {\n          id: like('123'),\n          status: like('available'),\n          location: {\n            lat: like(37.5665),\n            lng: like(126.9780)\n          }\n        }\n      });\n      \n    const vehicle = await vehicleClient.getVehicle('123');\n    expect(vehicle).toHaveProperty('id');\n    expect(vehicle).toHaveProperty('status');\n  });\n});\n```\n\n## Performance measurement and continuous improvement\n\n### 📊 Key performance indicators (KPIs)\n\n#### Technical metrics\n\n| Metric | Target | Current | Improvement |\n|--------|--------|---------|-------------|\n| Deployment frequency | 5 times/day | 8 times/day | ✅ +60% |\n| Deployment lead time | 30 min | 8 min | ✅ -73% |\n| MTTR (Mean Time To Recovery) | 1 hour | 15 min | ✅ -75% |\n| Change failure rate | <5% | 2.3% | ✅ -54% |\n\n#### Business metrics\n\n- **Service availability**: 99.97% → 99.99%\n- **Response time**: P95 500ms → 150ms\n- **Concurrent users**: Can handle 10,000 → 100,000 users\n\n### 🔄 Continuous improvement process\n\n```yaml\n# Monthly retrospective process\nretrospective:\n  what_went_well:\n    - \"Minimized failures with canary deployment\"\n    - \"Quick problem identification with monitoring dashboard\"\n    \n  what_needs_improvement:\n    - \"Resolve cross-team dependencies\"\n    - \"Expand test automation coverage\"\n    \n  action_items:\n    - name: \"Expand event-based communication\"\n      owner: \"architecture-team\"\n      due_date: \"2025-02-28\"\n    - name: \"Achieve 80% E2E test coverage\"  \n      owner: \"qa-team\"\n      due_date: \"2025-02-15\"\n```\n\n## What's next\n\n### 🚀 Next steps roadmap\n\n#### Q1 2025: Platform Engineering enhancement\n- **Developer Portal** implementation (Backstage-based)\n- **Self-service infrastructure** introduction\n- **Developer productivity metrics** collection\n\n#### Q2 2025: AI/ML integration\n- **Predictive autoscaling** (machine learning-based)\n- **Anomaly detection** automation\n- **Performance optimization** AI assistant\n\n#### Q3-Q4 2025: Global expansion\n- **Multi-region** architecture\n- **Geographic data distribution**\n- **Edge computing** adoption\n\n## Practical application guide\n\n### ✅ Step-by-step checklist\n\n#### Design phase\n- [ ] Clearly define domain boundaries\n- [ ] Separate data ownership\n- [ ] Decide communication patterns (sync/async)\n- [ ] Plan failure scenarios\n\n#### Development phase  \n- [ ] API versioning strategy\n- [ ] Standardize logging/metrics\n- [ ] Write contract tests\n- [ ] Apply security policies\n\n#### Deployment phase\n- [ ] Build CI/CD pipeline\n- [ ] Canary/blue-green deployment\n- [ ] Set up monitoring dashboard\n- [ ] Define alert rules\n\n#### Operations phase\n- [ ] Define SLA/SLO\n- [ ] Failure response playbook\n- [ ] Regular disaster recovery training\n- [ ] Performance tuning and optimization\n\n### What I really learned (my realizations)\n\nAfter a year of struggling, here are the most important things I realized:\n\n1. **Don't try to change everything at once**: We were too ambitious at first and really suffered. Changing things one by one slowly is much safer\n2. **Start with monitoring**: When something goes wrong, if you can't find the cause, it becomes really overwhelming. Log and metric collection is essential\n3. **Team structure needs to change too**: Conway's Law that organizational structure mirrors system structure is really true\n4. **Invest in automation**: When you have many services, you absolutely can't manage them manually\n\n### Honest confession\n\nMicroservices isn't a magic solution that solves all problems. Sometimes it brings new complexity, and sometimes monoliths might be a better choice.\n\nBut if you adopt it properly, it can really improve team productivity and system stability. Our team also had a hard time, but we're really satisfied now.\n\nIf you're considering adopting microservices, feel free to ask questions anytime. I think I can share the struggles I went through in advance! 😊\n\n---\n\n**Next, we'll share these stories:**\n- Performance optimization tips I learned while operating Kubernetes (planned for next week)\n- Thoughts on whether event-driven architecture is really necessary (around February)\n\n**If you want to discuss more:**\n- Contact me on LinkedIn: [linkedin.com/in/jaylee](https://linkedin.com/in/jaylee)\n- Meet me on GitHub: [github.com/jayleekr](https://github.com/jayleekr)","categories":["Tech","Architecture"],"tags":["microservices","kubernetes","architecture","devops","scalability","docker"],"pubDate":"2025-01-18T00:00:00.000Z","url":"/blog/en/building-scalable-microservices-with-kubernetes/"},{"id":"en/2025-tech-trends-for-developers","title":"Tech Trends We Should Watch Together in 2025","description":"A software engineer's perspective on 2025's key technology trends and practical implementation strategies, shared with warmth and personal experience","content":"# Tech Trends We Should Watch Together in 2025\n\nHello everyone! As the new year begins, I'm sure many of you are wondering, \"What technologies should I learn this year?\" I find myself asking the same question every year around this time, but this year feels especially significant with the rapid pace of technological change.\n\nOver the past year, working on autonomous vehicle projects, I've experienced so many changes firsthand. Watching AI tools evolve from simple code completion to becoming real 'development partners' made me think, \"Wow, we're really in a new era now.\"\n\nToday, I'd like to share the changes I've felt in the field, along with the tech trends we should pay attention to in 2025. I'll try to keep it approachable and focus on stories you can apply directly in practice.\n\n## Here's what we'll explore together\n- 🤖 How AI is really changing our development work\n- ☁️ Why cloud is no longer optional but essential  \n- 🌐 The story of web and app boundaries truly disappearing\n- 🔒 Why security has become so critical\n- ⚡ The real impact of developer experience on productivity\n- 📋 How to actually plan your learning journey\n\n## 🤖 AI has really become my development partner\n\n### From a code-writing tool to a thinking companion\n\nTo be honest, early last year I was skeptical: \"AI writes code? Will that really be useful?\" But now it's completely different. I can't imagine developing without GitHub Copilot anymore.\n\nWhat surprised me most was that AI doesn't just generate repetitive code. It actually makes pretty good suggestions for complex system design too.\n\n#### A recent experience that was really helpful\n\nNot long ago, I had to apply an event sourcing pattern while designing a microservices architecture. After struggling alone, I asked AI, \"I have these requirements - how should I design this?\" and it suggested a really clean structure:\n\n```typescript\n// Event sourcing pattern suggested by AI\ninterface DomainEvent {\n  eventId: string;\n  aggregateId: string;\n  eventType: string;\n  timestamp: Date;\n  version: number;\n  data: any;\n}\n\nclass EventStore {\n  async saveEvents(streamId: string, events: DomainEvent[], expectedVersion: number): Promise<void> {\n    // Optimistic concurrency control logic suggested by AI\n    const currentVersion = await this.getStreamVersion(streamId);\n    if (currentVersion !== expectedVersion) {\n      throw new ConcurrencyError('Stream version mismatch');\n    }\n    \n    await this.persistEvents(streamId, events);\n  }\n}\n```\n\nOf course, it's not perfect yet. Sometimes it suggests weird code or misses security issues, so I still need to review everything carefully. But overall, it's really improved my development productivity.\n\n#### I think these changes are coming this year\n- **AI for code reviews too**: I think we'll see AI doing the first review when we submit PRs\n- **Automated testing**: Tools that find edge cases I missed and create tests for them are emerging\n- **Documentation is a given**: Automatic document updates when code changes will become standard\n\n### How should we prepare starting now?\n\nTo be really honest, I think not knowing how to use AI tools will put you behind. But don't feel pressured! You can approach it step by step:\n\n1. **First**: Try using tools like GitHub Copilot or Cursor in daily work\n2. **Next**: Think about \"How can I ask better questions to get better answers?\" (this is called prompt engineering)\n3. **Later**: Build AI-assisted code review processes with your team\n\n## ☁️ Cloud has really become an unavoidable choice\n\n### From server rooms to edge, boundaries are disappearing\n\nThese days when developing, I really feel that \"it's getting harder to do things on-premises.\" This is especially true when thinking about global services.\n\nI really realized this recently when building a real-time data processing system. Users worldwide wanted fast responses, but existing CDNs had their limits. So we introduced edge computing, and the results were amazing.\n\n#### Here's the structure we actually applied\n\n```yaml\n# Kubernetes edge deployment configuration\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edge-processor\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: edge-processor\n  template:\n    metadata:\n      labels:\n        app: edge-processor\n    spec:\n      containers:\n      - name: processor\n        image: myapp/edge-processor:v1.2.3\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n        env:\n        - name: REGION\n          value: \"asia-northeast1\"\n```\n\n#### Technologies to watch\n\n1. **WebAssembly (WASM)**: High-performance code execution at the edge\n2. **Kubernetes at Edge**: Lightweight container management with K3s, MicroK8s  \n3. **eBPF**: Kernel-level programming for network/security optimization\n\n#### Performance comparison results\n\n| Deployment Method | Response Time | Cost (Monthly) | Scalability |\n|-------------------|---------------|----------------|-------------|\n| Traditional Server | 200ms | $500 | Manual |\n| Serverless | 150ms | $300 | Automatic |\n| Edge Computing | 50ms | $400 | Intelligent |\n\n## 🌐 The web platform evolution\n\n### New possibilities with Web Platform APIs\n\nIn 2025, the web platform will blur the boundaries with native apps even more. New Web APIs like **PWA 2.0** and **WebGPU** are greatly expanding the possibilities of web applications.\n\n#### New APIs to watch\n\n```javascript\n// High-performance computing with WebGPU\nconst adapter = await navigator.gpu.requestAdapter();\nconst device = await adapter.requestDevice();\n\nconst computeShader = device.createShaderModule({\n  code: `\n    @compute @workgroup_size(64)\n    fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n      // Parallel computation running on GPU\n      let index = global_id.x;\n      output[index] = input[index] * 2.0;\n    }\n  `\n});\n\n// Direct local file manipulation with File System Access API\nconst fileHandle = await window.showSaveFilePicker();\nconst writable = await fileHandle.createWritable();\nawait writable.write(data);\nawait writable.close();\n```\n\n#### Practical application example\nRecently, when developing a web-based data visualization tool using WebGPU, we achieved **10x faster** rendering performance:\n\n- **Before**: Rendering 10,000 nodes with Canvas 2D API took 500ms\n- **After**: Same task completed in under 50ms with WebGPU\n\n## 🔒 Security and privacy enhancement\n\n### Zero Trust architecture expansion\n\nThe 2025 security paradigm centers on the Zero Trust principle: **\"Never trust, always verify.\"** The limitations of traditional perimeter security models are becoming apparent, especially with the expansion of remote work and cloud migration.\n\n#### Actual implementation case\n\n```typescript\n// Zero Trust authentication based on JWT tokens\ninterface SecurityContext {\n  userId: string;\n  permissions: Permission[];\n  deviceFingerprint: string;\n  locationVerified: boolean;\n  mfaCompleted: boolean;\n}\n\nclass ZeroTrustMiddleware {\n  async validateRequest(req: Request): Promise<SecurityContext> {\n    // 1. Token verification\n    const token = this.extractToken(req);\n    const payload = await this.verifyJWT(token);\n    \n    // 2. Device fingerprint verification\n    const deviceId = this.getDeviceFingerprint(req);\n    if (!await this.isKnownDevice(payload.userId, deviceId)) {\n      throw new SecurityError('Unknown device');\n    }\n    \n    // 3. Location-based verification\n    const location = this.getClientLocation(req);\n    if (!await this.validateLocation(payload.userId, location)) {\n      await this.requestAdditionalAuth(payload.userId);\n    }\n    \n    return {\n      userId: payload.userId,\n      permissions: await this.getUserPermissions(payload.userId),\n      deviceFingerprint: deviceId,\n      locationVerified: true,\n      mfaCompleted: payload.mfa === true\n    };\n  }\n}\n```\n\n#### Privacy-enhancing technologies\n\n1. **Differential Privacy**: Protecting personal information with statistical noise\n2. **Homomorphic Encryption**: Performing computations on encrypted data\n3. **Secure Multi-party Computation**: Collaborative computation without exposing data\n\n## ⚡ Developer Experience (DX) innovation\n\n### A paradigm shift in development productivity\n\nDeveloper tools in 2025 focus on **quality and stability** rather than just speed. Maintenance and scalability have become as important as fast development.\n\n#### Next-generation development environment\n\n```json\n// New development workflow configuration\n{\n  \"devContainer\": {\n    \"image\": \"mcr.microsoft.com/devcontainers/typescript-node:18\",\n    \"features\": {\n      \"ghcr.io/devcontainers/features/github-cli:1\": {},\n      \"ghcr.io/devcontainers/features/docker-outside-of-docker:1\": {}\n    },\n    \"postCreateCommand\": \"npm install && npm run setup-dev\",\n    \"customizations\": {\n      \"vscode\": {\n        \"extensions\": [\n          \"ms-vscode.vscode-typescript-next\",\n          \"bradlc.vscode-tailwindcss\",\n          \"esbenp.prettier-vscode\"\n        ]\n      }\n    }\n  }\n}\n```\n\n#### Tools to watch\n\n1. **Dev Containers**: Ensuring consistent development environments\n2. **Bun**: All-in-one JavaScript runtime and package manager\n3. **Biome**: Fast linting/formatting tool\n4. **Turborepo**: Monorepo build optimization\n\n## Practical implementation strategy\n\n### 🎯 Step-by-step learning roadmap\n\n#### Phase 1: Immediate application (1-2 months)\n- [ ] Daily use of AI coding tools (GitHub Copilot, Cursor)\n- [ ] Basic cloud service utilization (AWS Lambda, Vercel Edge)\n- [ ] Adoption of modern development tools (Bun, Biome)\n\n#### Phase 2: Foundation building (3-6 months)  \n- [ ] Container/Kubernetes hands-on experience\n- [ ] Apply security best practices\n- [ ] Build performance monitoring systems\n\n#### Phase 3: Expertise deepening (6-12 months)\n- [ ] Design edge computing architectures\n- [ ] WebAssembly utilization projects\n- [ ] Implement Zero Trust security models\n\n### 🚨 Pitfalls to avoid\n\n1. **Technology stack overflow**: Getting carried away by new tech and increasing system complexity\n2. **Performance obsession**: Trusting only benchmark results while ignoring real usage environments  \n3. **Security as afterthought**: Prioritizing development speed and postponing security elements\n\n### 📊 Technology priority matrix\n\n| Technology Area | Learning Difficulty | Immediate Utility | Long-term Value | Recommendation |\n|-----------------|-------------------|------------------|-----------------|----------------|\n| AI Coding Tools | Low | High | High | ⭐⭐⭐⭐⭐ |\n| Cloud Native | Medium | Medium | High | ⭐⭐⭐⭐ |\n| WebGPU/WASM | High | Low | High | ⭐⭐⭐ |\n| Zero Trust | High | Medium | High | ⭐⭐⭐⭐ |\n| Edge Computing | Medium | Medium | Medium | ⭐⭐⭐ |\n\n## Let's grow together in 2025!\n\n### What I felt was most important\n\nWatching all these technological changes, I realized that ultimately, **people** are at the center:\n\n- **AI is a friend**: I think of it as a partner that helps create better code, not a replacement\n- **Cloud is fundamental**: It's no longer optional but has become basic literacy\n- **Security is a habit**: We need to cultivate the habit of thinking about it from the beginning, not adding it later\n- **Slow and steady**: Don't try to learn everything at once; take it step by step\n\n### How should we approach this going forward?\n\nI've planned it this way, in case it helps:\n\n1. **3-month cycles**: It's hard to predict the distant future, so set goals in short cycles\n2. **Start with small projects**: Try new technologies with toy projects first\n3. **Together with colleagues**: It's easy to get tired doing it alone, so study or work on side projects together\n\n### Last thoughts\n\nIt's only been a short time since 2025 started, but so many changes are already happening. Sometimes I feel overwhelmed thinking \"Do I need to keep up with all of this?\" but I believe we can definitely do it if we take it one step at a time.\n\nMost importantly, I think technology is just a tool, and what problems we want to solve is more important. I hope we can use these new tools wisely to create better services and become developers who help more people.\n\nLet's grow together this year! If you have questions or want to share experiences, please feel free to contact me through comments or email anytime. 🚀\n\n---\n\n**Related posts**:\n- [5 Key Competencies to Become a Senior Developer](/blog/en/senior-developer-competencies) (next post)\n- [Microservices Architecture Implementation Guide](/blog/en/microservices-guide) (planned for March)\n\n**References**:\n- [Stack Overflow Developer Survey 2024](https://survey.stackoverflow.co/2024/)\n- [GitHub State of the Octoverse 2024](https://github.blog/2024-11-06-the-state-of-the-octoverse-2024/)\n- [CNCF Annual Survey 2024](https://www.cncf.io/reports/cncf-annual-survey-2024/)","categories":["Tech","Trends"],"tags":["2025","technology","trends","development","AI","cloud","web3"],"pubDate":"2025-01-22T00:00:00.000Z","url":"/blog/en/2025-tech-trends-for-developers/"},{"id":"deepthinking/retrospect/2020-12-31-2020","title":"2020 회고록","description":"","content":"![Desktop View](/assets/img/autron2.jpg)\n\n## 2020년 회고록\n\n누가 그러던가 대리~과장이 가장 일을 잘할 때고 많이 할 때라고..\n\n눈 감았다 뜨니 4Q가 왔다.\n\n헐이다 헐\n\n4.5년간의 대학원 생활때는 이론적인 부분에 집중한 연구를 했고, 3년간의 군복무 기간동안에는 사용자를 위해 최대한의 실행력을 요구하는 일을 했다가, 이제는 그 두 부분이 모두 필요한 플랫폼 개발쪽의 일을 하고 있다고 보면 된다.\n\n지금 하는일을 좀 더 자세히 이야기 하자면 Adaptive AUTOSAR 플랫폼, 즉 AP 미들웨어라 흔히들 부르고, 차량에 탑재되어야할 고성능 컴퓨팅 칩들에 들어갈 매우 중요한 요소다.\n\n주요 기능들은 차량어플리케이션 개발자들이 설계대로 개발을 편하게 하게 하도록 구조화된 코드를 자동으로 생성해주고, 이런 차량어플리케이션이 OTA를 통해 원격 업데이트할 수 있는 인터페이스를 제공하고, 기타 다양한 차량의 상황에 따른 행동을 달리 할 수 있도록 인터페이스도 제공 등으로 앞으로 다가올 자율주행시대를 맞이하기 위한 매우 중요한 것들이다.\n\n사실 이러한 소프트웨어적 기능들을 장기적인 안목으로 추상적으로 설계하고 개발해 나아가는 것을 대표적으로 잘하는 회사들이 유럽회사들이며 그들이 리딩해서 진행중이다.\n\n유럽 및 일본의 전통적인 차량 제조회사들은 차량제조를 위한 생태계(흔히들 n차 업체 n-Tier 라 부르는 것)와 공존하며 나아가는 것이 매우 중요하고 상식으로 받아 들여 왔다.\n\n이 모든 것들이 Tesla라는 변종이 나타나며 흔들었다. \n\n소비자와 시장은 Tesla를 택했고 정말 시장의 힘은 무섭기 그지없엇다.\n\n소프트웨어 개발방법론인 애자일적인 관점으로 차를 접근하는 것이 핵심이다.\n\n설계-&gt;개발-&gt;검증 하는 텀을 획기적으로 짧게 잡았고, 이렇게 바뀌거나 새롭게 추가된 기능들을 업데이트 할 수 있도록 했다.\n\n말은 누구나 할 수 있을 것 처럼 쉽다.\n\n하지만.. \n\n앞으로 어떠한 센서나 하드웨어가 추가적으로 필요하게 될지 모르고,\n\n정말 한치앞을 모르는 장기적 로드맵이 가져다 주고,\n\n모든 리스크를 감당할 수 있는 비지니스적 구조를 만들어 가는 것은 \n\n전통의 차량제조회사들에게는 수년에서 수십년의 시간동안 축적되어 이미 정치적 및 사회적 구조 제반 시설 등이 복잡하게 얽혀 Chaos 이론처럼 보여 바꾸기 무지 어려운 부분이다.\n\nOTA(On-The-Air)를 통한 업데이트하는 기능 자체는 이미 모바일시장과 셋탑박스시장을 통해 어느정도 성숙했지만,\n\n이 기능들이 동작하는 대상이 자동차가 되는 순간, 고객 안전이 바탕이 되어야하므로 해당 기능이 제도적 & 법률적으로 넘어야 할 산이 매우 높다.\n\n유럽은 이러한 것들을 표준 및 법으로 단단하게 막아두었다.\n\n유럽은 이를 만족하는 플랫폼을 개발하기 위해 느리지만 확실한 길을 나아가고 있었는데, Tesla는 완전히 다른 접근법으로 엄청난 비지니스적 리스크를 감당해가며 새로운 차량제조의 길을 열었다.\n\n개인적으로 이러한 바탕에는 CEO의 역량이 가장 대단하다고 생각한다.\n\nPaypal 마피아 Elon Musk의 엄청난 Fund raising 능력과 광고료를 전혀 쓰지않지만 알아서 홍보가 되는 Tesla 식 Social Marketing 기법 등이 그의 대표적 역량이다.\n\n그리고 무엇보다도 정말 뛰어난 엔지니어들이 꿀벌처럼 홀리게 하는 능력과 그러한 조직을 만드는 것들이 가장큰 업적이라 생각한다.\n\n엔지니어들은 문제를 해결하면서 희열을 느끼는 사람들인데, 정말 매번 획기적인 방법으로 전세계적으로 통용되는 구조를 하나씩 깨가며 문제를 풀고 있으니 내가 그곳의 엔지니어라도 즐거울 것 같다. (Google도 못했으니 말 다했다고 볼수 있다)\n\n이야기가 Tesla 쪽으로 빠졌네.. 워낙에 벤치마킹을 많이하다보니.. 나중에 자세히 한번 써야겠다\n\n어디까지 했더라 본론으로 돌아오자\n\n현재 자율주행 분야는 아래부터 위로 반도체, CPU & GPU Architecture, SoC Packaging, Operating System, Middleware, Backend, Frontend 분야를 망라한 모든 전세계 공룡 회사들이 각 회사들의 강점을 들이밀며 자동차라는 새롭게 다가올 거대한 컴퓨터를 향해 달려들고 있다.\n\n학부때 배웠던 Convergence의 개념처럼 모든 것이 소용돌이처럼 하나로 엮이고 있어 유래없는 혁신이 빠르게 다가올 것이 자명하다.\n\n이 모든게 하나의 브랜딩화가 되어 독점화 되었을때 그 것이 가져다주는 힘은 Google & Samsung, Apple을 통해 잘 알았을테니 당연한 수순이지 않을까 싶다.\n\n근 1년간 일하는 것이 정말 재미있었다. 개인적 생각으로는 전세계의 뛰어난 엔지니어들과 함께 문제를 해결해 나아간다는데 있고 해결해야될 문제천지라서(?) 그런 것 같기도\n\n사실 R&D하는 분야가 모든 공룡회사들에게 필요한 공통적인 부분이라 전세계 모두가 비슷비슷한 부분이 많아 실력이 비슷비슷할 것 같지만 격차가 매우 크다.\n\n엔지니어에게 요구하는 지식양의 수준도 그만큼 어마어마하게 높은데 내게는 이를 넘어가는 재미가 쏠쏠한 것 같다.\n\n전통의 대기업은 보통은 사수가 무언가를 가르쳐주고 그를 배워 회사의 부품이 되는 것이 일반적이지만, 이 분야는 정말 한치앞도 모르기 때문에 내바로 윗 사람도 모른다.\n\n가장 많이 공부하는 사람이 가장 많이알고,  내가 아는 것도 몇년이면 옛 기술이 되어버리는 시대이다.\n\n그래서 그런지 그냥 내가 알아서 앞을 헤쳐나가는 맛이 있다.\n\n창업을 해볼까 고민도 들기도하고 몇몇 제의가 들어오기도 하고 하지만..\n\n많이 부족하고 갈길이 멀기 때문에 연구개발에 매진하기로...\n\n2020년 Adios","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2020-12-30T15:00:00.000Z","url":"/blog/deepthinking/retrospect/2020-12-31-2020/"},{"id":"deepthinking/retrospect/2021-04-15-retro","title":"Adaptive AUTOSAR를 연구&개발하며..","description":"","content":"간만에 일기 겸 긴 글을 남기고 싶어서 이 글을 시작한다.\n\n필자는 Adaptive AUTOSAR 표준 기반 플랫폼 개발을 해온지 일년반정도 됐다.\n\nAdaptive AUTOSAR는 AUTOSAR 컨소시엄에서 제정해오고있는 차세대 Automotive Application을 위한 공통의 아키텍쳐를 Open System으로 정의하고 있는 표준 아키텍쳐를 의미한다. \n\n이 기술 표준 아키텍쳐가 지향하는 바는 지속가능하고 재사용 가능한 차량환경에서의 Software Application이 동작하고 개발될 수 있도록하는 전체적인 큰 그림을 그리는 것이다.\n\n어렵다고? \n\n당연하다. \n\n가타부타 말이 많지만 AUTOSAR라는 이 아키텍쳐는 애초에 표준화하기를 사랑하는 유럽(이라쓰고 독일이라씀)을 주축으로한 자동차 업계의 거대회사들이 뭉쳐서 유럽의 진입장벽을 높히게된 결정적인 것이었다. (현대차에서 25년동안 엔지니어를 하신 업계의 산증인이신 아버지에게 들었기때문에 나름 신빙성이 높다 ㅋㅋ)\n\n유럽인들은 이렇게 참 표준화하는 것을 좋아한다. \n\n컨소시엄을 구성하고 참여자 모두가 치열하게 고민하면서 공통의 이해관계 사이에서 최적의 단어 및 어구들을 담아 기록한 것이 이 유럽인들(게르만)이 사랑하는 표준이다.\n\n필자는 대학원 시절 부터 여러 표준화 단체에서 진행 중 or 제정한 여러 표준들을 Tech 들을 연구했다. \n\n그 당시만해도 표준이라는 것은 필자에게 법(?) 처럼 다가왔고, 표준의 양이 주는 엄청난 무게감에 그냥 공부한다는 느낌으로 봤었다.\n\n참 시간이라는게 무섭다고, 무지한 필자도 계속해서 여러 표준들을 접하다보니 원초적인 질문들을 던지게 되었었다.\n\n이런 고민의 고민을 했었건 기억들 때문인지 필자가 AUTOSAR라는 것을 접했을때 단순히 예전처럼 활자모음으로만은 다가오지는 않는 것을 보니 헛된 시간을 때려박은 것은 아닌 것 같아 안도의 한숨을 내쉰다.\n\n필자처럼 Slow learner 도 없을텐데.. Anyway\n\n사실 표준은 제정하는데 참으로 많은 시간이 걸린다. 표준은 제안하고 토의하고 또 토의하고 제정하고 또 계속해서 수정되고 하는 반복된 작업을 거치며 완성된다. \n\n어찌보면 거대한 자본을 기반으로한 거대 기관들이 모두가 합의를 해 만든 표준이 이러한 긴 시간동안 만들어온 그 무게는 엄청나다. \n\n필자가 위에서 언급했듯 유럽인(게르만)들은 이런식으로 세상을 구조화하기를 참으로 좋아하고 이렇게 거대 회사들을 한세기 가까이 지배해왔다. \n\n이러한 구조화된 아키텍쳐를 기반으로 자동차 업계에 OEM, Tier-1, Tier-2와 함께 경영학 관점의 관료제는 근 한세기를 지배해온 자동차 그룹들의 특징이다. Volkswagen, BMW, Benz, Toyota, GM 등..\n\n이 중 Toyota 그룹의 관료제는 하버드 비지니스 스쿨의 가장 성공적 경영체계 사례로 단골일 정도이니 말이다.\n\n정말로 단단한 유리천장(요즘말로 ㅎㅎ)을 만들어놨지만.. 누군가가 그 유리를 깨버렸다.\n\n바로 Tesla다.\n\n이 넓은 기술 표준기반에 제반되는 넓은 회사들의 생태계.. \n\n이 무거움을 오히려 단점으로 보고, 하나의 회사에서 모든 것을 다 해버리겠다는 원대한 계획.\n\n과연 잘 할수 있을까 하는 모두의 의심을 15년 가까이 해왔지만,\n\n수직계열화를 해낸 이 회사\n\n사실 필자가 공부해온 표준화와는 거리가 많이 멀지만, 이들은 비지니스의 관점에서 완전히 다르게 자동차 업계를 접근했고 결국 이 두꺼운 유리천장을 깨버렸다고 생각한다.\n\n필자는 기술의 실행력을 중요시 여기는 미국 실리콘벨리의 문화를 그대로 보여주는 사례라고 생각하는데, 이는 정말 유럽이 지향하는 세계와는 반대라고 본다.\n\n필자는 테슬라가 깨버린 것들 중에 자동차 시장의 소프트웨어 생태계가 정말 가장 대단하다고 생각한다. \n\n테슬라는 모바일 시장에서 빠르게 발전한 저전력기반의 컴퓨터 아키텍쳐를 기반으로, 오픈소스 기반의 운영체제, 오픈소스 기반의 어플리케이션을 개발해서 지금의 혁신을 일궜고, 정말 많은 테크 그루들의 사랑을 받아오고있다.\n\nAdaptive AUTOSAR 표준은 2017년을 기점으로 본격적으로 시작했는데, 필자의 견해로는 이때만해도 거대 자동차 그룹들을 그렇게 급하지 않았다.\n\n2019년을 기점으로 2020년~2021년을 거쳐 자본시장의 간택을 받으며 Tesla가 마켓에서 엄청난 Valuation을 받으며 모든 자동차 그룹사를 합친 것보다 시가총액이 넘어서게 되었는데, 이 시점을 시작으로 각 그룹사들은 각자가 Software쪽에 혁신을 보여주겠다며 요즘은 난리도 아니다.\n\n세계 최대 자동차 그룹인 Volkswagen은 대놓고 2025년까지 6500명의 Software Engineer를 뽑을것이며 Tesla에게 두눈뜨고 당하지 않겠다라고 선포하고 있고, Toyota 그룹은 그들이 자랑하는 관료제를 일부 내려 놓으며 Software 업계의 차등 임금 보상을 주겠다고 하며 나서고 있다.\n\n필자가 일하는 현대차 그룹도 현대자동차, 모비스, 오트론 등의 선행팀이 모여 Adaptive AUTOSAR 표준기반 플랫폼 및 어플리케이션을 개발중이다. \n\n재미있는건 선행팀으로 시작했지만 이제는 단순히 선행이아닌 꼭 해야하는 업무로 변했다.\n\nAdaptive AUTOSAR 표준기반의 플랫폼이 제공하는 다양한 기능들 및 생태계는 Tesla의 수직계열화에 대응하는 일련의 작업이기 때문이다.\n\n이 Challenging 한 일이 정말로 재미있다.\n\n엄청난 범위의 지식을 기반으로 이를 비지니스적으로 접근하여 최대한의 빠른 실행력으로 뿜어내는 것이 핵심인 이 분야에서 나같은 천천히 가는 사람이 맞을지는 모르겠지만..\n\n누구보다 천천히 꾸준히 가는 것만은 자신있다.","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","AdaptiveAUTOSAR","AUTOSAR","ClassicAUTOSAR","ECU","CPU","GPU","OTA"],"pubDate":"2021-04-14T15:00:00.000Z","url":"/blog/deepthinking/retrospect/2021-04-15-retro/"},{"id":"deepthinking/retrospect/2021-12-31-retro","title":"2021 회고록","description":"","content":"2021년은 내 짧은 커리어 역사상(?) 가장 다이나믹했던 해 였던 것 같다.\n\n올해 머리속에 자리잡은 강렬한 기억들의 대부분은 Sonatus와 조금이라도 연결된 걸보니 올해는 온통 내 머리속에 Sonatus 뿐인듯 하다.\n\n물론 그와 병렬적으로 다른 모임들에도 많이도 참여했다. ASG(ADAS Study Group, 내가 지음ㅋㅋ) 2년차, Life Mentor 이동훈 부사장님이 말못하는 나를 위해 만들어 주신 클하대학교 SCP 스피치 모임, 공부에 미친 사람들이 모여있는 클하대학교 산업스터디에서 부끄럽게도 모빌리티 세션 전문가로서 세션까지 많은 것들을 했지만, 가서도 맨날 회사 혹은 회사에 관련된 이야기만 한 것 같은 느낌이다.\n\n이제는 집처럼 느껴지지만 연초만해도 정말 가고싶어 발버둥 쳤던 회사 Sonatus\n\n극적인 합류 후 이어지는 정말 다이나믹한 일들의 연속.\n\n6개월 밖에 되지 않았지만 그 짧은기간동안 겪은 것들은 내가 일전에 5년은 공부하며 일했던 양보다도 많고 질도 다른 듯한 느낌이다.\n\n나는 삶이 Atomic Orbital과 비슷하다고 생각하는데 뭔가 Quantum Leap을 해낸 것 같은 뜻 깊은 한해인듯하다.\n\n되돌아 보면 학계에 있을때나 전문연할때나 대기업에 있을때나, 내가 아무리 주체적으로 주인의식 갖고 일을하고 공부를 하고 해도 뭔가 닿을 수 없는 Glassdoor 가 있는 느낌이었다.\n\n그래서 본능적으로 나의 그런 욕구를 해소해 줄 수 있는 것들을 찾아 공부를 해보기도하고, 다양한 사람들을 만나보기도하고, 온라인상에서 소위 뛰어나다는 사람들이 모여있는 커뮤니티에 들어가보려고 열심히 찾고 들어가서 이야기도 많이 나눠보고 했었다.\n\nSonatus는 이런 내 Needs를 단번에 해결시켜주었다.\n\n나의 Needs는 비단 Tech 관련한 것들 뿐만아니라,  글로벌하게 벌어지는 사회적 현상에 대한 거시적관점, 또 그런것들을 Emerging하는 Tech들로 해결하고자하는 다양한 사람들에 대한 호기심, 또 그것들이 Financial 하게 Market에서 어떻게 바라봐지는지에 대해 알아가는 즐거움, 이런 비슷한 생각들을 가지고 살아가는 사람들과 새로이 맺어지는 관계에서 오는 또다른 자극, 이렇게 아주 복잡단순하면서도 계속해서 변화하는 욕구로서 나를 살아 숨쉬게하는 하나의 원동력이다.\n\n이런 나의 복잡한 Needs는 Sonatus에 합류하면서 모두가 연결되어 내 머리속에 새로운 지평선을 열어주었다. 이제는 새로운 궤도에 정착하기위해 끊임없이 또 새로운 것들을 끊임없이 채워가며 살고 또 다음 Quantum Jump를 기다려야지.\n\n자 그럼 내가 2021에 느낀 것들을 대강 풀어가겠다.\n\n## Tech\n\n우리의 Boss Jeff 가 바라보는 거시적 관점에서의 Connected World의 변화의 흐름. 그 누구도 거스를 수조차 없는 거침없이 빠른 흐름을 읽고 거기서 기회를 찾았다.\n\nJeff는 실리콘벨리에서 6번이나 시장에서 성공적으로 Exit한 실리콘벨리에서도 유명한 에이스다(Big Tech만큼은 아니지만)\n\n그가 바라보는 Tech World에서 논문, 특허, 표준은 너무나 느리다. (물론 나중엔 이런것들도 다 챙겨야한다. 하지만 다 소잃고 외양간 고칠수는 없지않는가?)\n\n이 엄청나게 빠른 흐름을 타기위해서는 Open Source라는 강력한 타이탄의 도구를 이용해야한다.\n\n이토록 막강해진 Decentralized 한 Open Source 생태계에 의해 Open Source Software를 중심으로 변화해가는 Hardware Design과 Architecture 생태계도 이해해야한다.\n\n아마 이 흐름에 타지 못하는 회사는 비지니스 업계에서 도태될 것이 자명할듯하다.\n\n또 단순히 미국 Big Tech가 주도하는 이러한 흐름에 단순히 타기만 한다면, 영원히 Big Tech 아래에만 있겠지.\n\nDecentralized Open Source 생태계를 중심으로 앞으로 인간의 삶을 바꿀 모든 비지니스의 영역들이 바뀔 것이다. Bio, Health Care, Blockchain, Mobility, Quantum Computer 이 모든게 그 Open Source 생태계를 중심으로 움직일 것이다. 어쩔수 없는 흐름이다.\n\n## Finance\n\n양적완화가 시작된지 꽤 오래됐는데 코로나인해 힘들어진 경기를 살리겠다고 더욱더 많은 돈이 풀리면서 안그래도 돈이 많은 시장에 더 많아졌다. 그 돈들이 각국의 정부가 바라는 시나리오대로 사용되어 민간의 경기 불황이 해결되었으면 좋겠지만 결국엔 부가 많은 이들을 더 배불리게 하는 쪽으로 많이 기울었다.\n\n하지만 이러한 흐름을 잘만 이용한다면 Quantum Jump할 수 있는 기회가 될 수도 있다.\n\n현재 미국은 모든 이러한 Quantum Jump를 Tech 회사들이 이끌고 있으며 그 중심에는 실리콘벨리의 Startup 들이 있다.\n\n이러한 변화는 닷컴버블부터 스마트폰이 대중적으로 깔릴때까지 성공적으로 부를 창출한 신흥 부자들이 시작한 VC들의 Golden Era가 시작되는 2010년부터인듯하다.\n\nJeff의 말만들어도 10년전만해도 Startup이 성공적으로 Exit을 한다는 것은 Cisco나 Intel 과 같은 미국의 극강한 전통의 회사들에게 M&A되는 것이 었는데, FAMANG이 득세하는 세상이 찾아오고 나선 완전히 Exit의 패러다임이 바뀌었다고 한다.\n\n그래서 Jeff의 꿈도 7번째 회사인 Sonatus는 대기업에 M&A가 목표가 아니라 Unicorn을 넘어선 Decacorn 회사가 되는 것이라 한다. 그리고 그럴 수 있는 시대이기도 하고 말이다.\n\nSonatus에는 이렇게 연속적으로 Exit을 경험했던 Engineer들이 참많다.  Sonatus는 그런 경험을 한 사람들중 괜찮았던 사람들끼리 물어물어 합류하고 만들어진 회사이고, Stanford 동문들이 중심이 되어 똑똑하고 열정있는 인재들이 함께 하며 커지는 회사이다.\n\n나도 그 물살에 끼어 잘해보려고 하고 있는거고, 앞으로 나의 삶의 초기(?) 투자금으로 역할을 잘 해주었으면 좋겠다. ㅋㅋ\n\n## People\n\n그 전엔 나도 워라벨을 챙겨야하는 게 아닌가라는 생각도 많이했는데, 많이 변했다. Work = Life 라서 Balance 할게 없기 떄문이다 ㅋㅋ\n\n생각해보면 Work라는걸 굳이 떼어내어 생각할 필요가 있나 싶기도하다. 같이 Work하는 사람들이 정말 좋은 사람들이고 평생을 함께하고 싶은 사람들인데 굳이 내가 그 둘을 분리하기는 싫다.\n\n요즘은 Work가 내 삶의 Motivation이고 Joy인데, Network를 한답시고 어줍잖은 모임에 나가고 모르는 사람들이랑 술마시고 할 시간이 거의 없다.\n\n그리고 예전엔 단순히 주는 삶을 좋아했다면, 이제는 내가 받은 것이 고마워서 더 주고싶은 사람들에게 시간을 쏟는 편이다. 누가 먼저라 할것 없이 서로 Give And Take를 하는 사람들과 교류하는 시간이 너무 행복하고 즐겁다.\n\n단순히 일을 같이 하는게 아니라 사회적현상에 대해 논의하고, 우리가 아는 Tech로 어떻게 풀어갈지 고민해보고, 또 새로운 Tech가 나왔을떄 Sharing하고하는 이런 삶이 너무 즐겁다.\n\n생각해보면 우리는 항상 사람이 부족한 빠르게 Scale하는 회사인데도 사람한명을 뽑는데 엄청나게 신중하다.\n\n정말로 모난사람이 거의 없고, 서로 알려주기를 좋아하고, 도맡아서 작은일이라도 하려고하고, 또 이런것을 회사에 Contribution하는 것이라 생각하는 배려심 깊은 사람들밖에 없는듯 하다.\n\n나 역시도 좋은 사람인것 같아 덩달아 기분도 좋고 말이다. ㅋㅋ\n\n## Workout\n\n나의 좋은 습관을 꼽으라 하면 난 주저없이 운동을 뽑을 것이다.\n\n농구는 약 20년, 재활겸 농구를 잘하기 위해 시작한 웨이트 트레이닝만 15년을 했다.\n\n그리고 이제는 내 스트레스를 덜어내고 두뇌회전을 돕는 역할까지한다.\n\n사실 이런 것들은 원래 계속해오던 것이고 밥먹는 것 만큼 익숙한 루틴인데, 올해 가장 뿌듯한것은 회사사람들을 운동을 가르켜주는 것이다.\n\n시작은 Cross Fit도 한지 몇년된 Korean Team의 리더형이 첫날에 운동복을 챙겨오라고 하면서 였는데, 이제는 그 이후로 같이 운동가는 사람들의 모든 운동을 다 봐주고있다.\n\n농구를 하며 잘해보고 싶어 내가 연구하고 몸으로 겪었던 것들을 압축해서 가르쳐주니 다들 너무 신기해하고 좋아하는 모습에 참 뿌듯한듯하다.\n\n거의 반 상주트레이너처럼 운동도 가르쳐주고, 원하면 식단까지 짜주고.. ㅋㅋㅋ\n\n운동이 인간의 삶에 가져다주는 무척이나 긍적적인 시그널들을 몸소 가르쳐주며, 그들의 삶에 컨트리뷰션할 수있어서 더 좋다.\n\n이젠 우린 뗄래야 뗄수없어~~\n\n더 길게 적고싶은 것들을 이정도로 마무리해야겠다.\n\n2022에는 Tech knowledge도 더 성숙했으면 좋겠고, 영어도 영혼의 담소를 나눌만큼 잘해졌으면 좋겠고, 글도 좀 더 많이써서 글쓰기도 늘었으면 좋겠다.\n\nMilestone을 디테일하게 짜서 꼭 달성하리라~!\n\nAdieu Dynamic 2021","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2021-12-30T16:00:00.000Z","url":"/blog/deepthinking/retrospect/2021-12-31-retro/"},{"id":"deepthinking/retrospect/2022-12-26-pre-retro","title":"2022 회고록","description":"","content":"2022년 !!! 우와 진짜 번개불에 콩궈먹듯이 지나갔다.\n\n사실상 2021년 2022년 두해는 어떻게 갔는지 모르겠다...\n블로깅을 진짜 많이하려고 작성한해에 제일 못했다...\n반성한다 ㅠㅠ\n\n연말도 마무리할겸 올한해 해내지 못한 블로그 리스트를 아래에 남겨본다.\n\n(아래 남긴 리스트중 몇몇은 블로그로 내년초까지해서 남길 것이다)\n\n- 2022 밀린 블로그 컨텐츠\n    - Tech\n\t\t- C++\n\t\t\t- C++프로젝트의 효율적인 Configuration 관리법에 대한 고찰\n\t\t\t- C++Memory Protection 기법 \n\t\t\t- What is io_uring and how does it help for us?\n\t\t- Yocto\n\t\t\t- Yocto에서 ELF를 관리하는 방법\n\t\t\t- 협업시스템으로서의 Yocto에 대한 고찰\n\t\t\t- Yocto system을 이용한 효율적인 빌드와 배포에 대한 고찰\n\t\t- Bazel\n\t\t\t- What is Bazel ?\n\t\t\t- Bazel VS Yocto\n\t\t- Docker\n\t\t\t- 효율적인 임베디드 개발환경에 대한 고찰 (With Docker)\n\t\t\t- Docker 동작원리 뜯어보기\n\t\t\t- Linux Namespace로 관점에서 바라본 Docker Container\n\t\t- ETC\n\t\t\t- 배포 아티펙트 및 사후관리 (Reproducible Build, Debuginfod)\n\t\t\t- Linux RT Patch (Preemption)에 대한 고찰\n\t\t\t- VPN으로 보호되는 Intranet에서 Github의 Trigger를 받는 방법\n\t\t\t- Reverse Engineering with HYDRA\n\t- Network\n\t\t- Startup의 성장시기별 필요한 인재에 대한 고찰\n\t\t- Idiation을 하는 효과적 방법에 대한 고찰\n\t- Personal\n\t\t- 내가 꿈을 함께 하는 사람들\n\t\t- 강의와 나의 성장\n\t\t- 캄보디아 봉사 회고\n\n\n나와의 다짐을 블로그에 남긴만큼 꼭 달성하도록 하자!\n\n조금만 기다려줘 2023년아~\n\nㄴ","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2022-12-25T16:00:00.000Z","url":"/blog/deepthinking/retrospect/2022-12-26-pre-retro/"},{"id":"deepthinking/retrospect/2025-02-05-retro-1","title":"2025-02-05 애매한 Retrospect 1","description":"","content":"# 들어가며 : 게으른 나\n\n이전 회고록을 보니 2021년, 2022년이 마지막이더군요. 정말 게으르게 살아왔다는 반성을 하게 됩니다. 이를 대신해, 2025년에는 시간이 날 때마다 짧은 기록이라도 부지런히 남겨보려 합니다.  \n회사인 Sonatus가 믿을 수 없을 정도로 성장하는 것만큼 저 역시 성장했고, 어느덧 입사 5년 차가 되어 갑니다. 4년을 꽉 채운 제 Dell 노트북은 성능이 예전 같지 않고, 4년이면 제 주식도 풀 베스팅이 되는 시점이니 이 시점에서 4년간의 경험을 되돌아보고 싶어집니다.\n\n최근에는 '삶의 본질'에 대해 자주 생각합니다. 만나는 사람들에게 \"어떨 때 행복해?\" 같은 질문을 습관처럼 던지는데, 정작 제 머릿속 고민을 따로 정리한 적은 없더군요. 그래서 이 블로그에 비공식적인 잡담 형태로 제 고민과 생각을 정리해보고자 합니다.\n\n# Values : Mission-driven, Mover, Giver\n\n## 1. Mission-driven\n\n어릴 때부터 수학이 좋았고, 특히 어려운 문제를 오래 붙잡는 일을 즐겼습니다. 답안을 봐도 납득이 안 되면 선생님께 재차 물어보고, 제가 스스로 다른 방식으로 풀어내는 것을 자랑할 정도였죠.  \n현대 Autron에서 일할 당시에도 곧바로 나갈 마음을 먹었고, Sonatus에 입사해서는 일거리를 찾아다니며 제 역할을 만들어갔습니다.  \n복잡하고 이해하는 사람도 적은 영역에서 무언가를 창조해내거나 문제를 해결할 때 가장 즐겁습니다. 박사전문연구요원 기간이 끝나고 '자동차'를 계속할지, 'Crypto' 쪽으로 갈지 고민했을 때도, 결국 해결하려는 문제가 복잡하다는 점이 매력적으로 다가왔습니다. \n\n2024년 해결해 나간 문제는 상상이상으로 복잡했고, 그 과정에서 너무 즐거웠습니다. 뭐 물론 너무 힘들기도 했지만요. 더 복잡한 문제를 해결해나가는 것이 제 삶의 본질이라고 생각합니다.\n\n## 2. Mover\n\n길게 고민하기보다는 바로 실행에 옮기는 것을 좋아합니다. 새로운 도전을 앞두면 두려움보다는 설렘이 더 크죠. 그래서 실행도 많이 하고 실수도 많이 합니다(하하). 완벽주의와는 거리가 멀어 주변 도움을 자주 받지만, 그만큼 많은 걸 시도해서 얻는 것도 큽니다.  \n회사가 빠르게 성장하면서 저 개인이 자유롭게 움직일 수 있는 범위가 좁아진 점은 조금 아쉽지만, 여러 사람이 함께 복잡한 문제를 해결하는 것은 생각보다 더 어렵고 힘들며, 그만큼 얻을 수 있는 것도 많다는 점을 깨달았습니다.\n\n## 3. Giver\n\n![Desktop View](/assets/img/sonatus_all.jpg)\n\n제가 좋은 사람들을 많이 만나고 친분을 쌓을 수 있었던 건, 아마 제가 먼저 주려는 성향 때문이 아닌가 싶습니다. 가장 어린 시절 기억 중 하나가 할아버지 댁에서 받은 과자 상자를 동네 놀이터에서 아이들에게 나눠줬던 모습이니까요.  \n![Desktop View](/assets/img/cambodia_jay.jpeg)\n캄보디아 봉사활동 때 제가 준 농구 유니폼을 입고 행복해하던 아이들의 모습은 아직도 선명합니다. 앞으로도 대가 없이 주는 사람이 되고 싶으니, 그만큼 더 많이 벌어야겠다는 생각도 듭니다(갈 길은 멀지만요).  \n2024년 12월, 4년 가까이 진행된 CCU2 프로젝트가 양산(MP)에 이르러 전 세계 팀원이 한국에 모여 바쁜 일정이 이어졌지만, 잠도 못 자는 상황에서도 맛있는 한식을 꼭 대접하고 싶었습니다. 덕분에 저를 포함해 모두가 친구가 되었고, 특히 폴란드에서 온 동료들과의 문화적 공통점을 찾으며 서로에게 긍정적인 시너지를 느낄 수 있었습니다.\n\n아래 두사진은 Poland에서온 제 절친 Sebastian과 현대차 남양연구소에서 아침에들어가 밤 10시에 이슈를 해결하고 나와, 판교에서 치맥하며 자축했던 사진입니다. ㅋㅋ\n\n![Desktop View](/assets/img/IMG_9664.jpeg)\n\n![Desktop View](/assets/img/IMG_9668.jpeg)\n\n\n# 일 : Automotive or Connected world\n\n2011년에 V2X(Vehicle To Everything) 전공으로 대학원에 들어간 후, 전문연구요원 3년을 빼고도 어느덧 자동차 업계를 11년 가까이 겪었습니다. 이제 자동차의 소프트웨어 정의(SDV) 시대로 가는 최전선에서 문제를 풀어가고 있죠.  \n기술적·사회구조적·국가 인프라·기업 내부의 정치적 이슈 등, 그림이 복잡하고 해답이 쉽게 보이지는 않습니다. 다만 Sonatus와 현대차가 해결하고자 하는 문제와 방향성은 분명하기에, 조금씩이라도 현실화돼 가고 있다는 실감이 있습니다.  \n덕분에 2024년에는 회사가 드디어 손익분기점을 달성했고, 글로벌 지사도 9개나 늘어났습니다. 유명 인사를 C레벨로 영입하면서 내부 체계도 탄탄해졌습니다. 덕분에 다른 OEM들의 문제도 해결해나갈 수 있다면, IPO 등 다양한 기회가 열릴 것 같아 기대가 큽니다.  \n2025년은 Sonatus가 흔하디흔한(?) 유니콘급 회사로 남느냐, 혹은 폭발적으로 더 성장해 나가느냐를 가를 중대한 시점이라고 CEO Jeff가 강조합니다. 작은 회사였을 때는 그 나름의 재미가 있었지만, 지금은 또 다른 즐거움과 기대로 가득 차 있네요.\n\n## Container Manager : LX3 양산차량에 적용된 Docker 기술\n\n개인적으로 가장 많은 부분을 디자인하고, 고객을 설득했던 기억이 남는 프로젝트가 바로 **Container Manager**입니다. 이 기술을 LX3 양산차량에 처음으로 적용해낸 순간은 지금 생각해도 감격스럽습니다.  \n말 그대로 Docker를 이용해 인비히클(In-vehicle) 환경에서 컨테이너가 돌아갈 수 있도록 한 솔루션이고, 기존 OTA 솔루션과도 조화를 이뤄야 했습니다. 사실 데이터센터 시장에는 이미 잘 돌아가는 컨테이너 관리 기술이 많지만, 차량 환경에서는 넘어야 할 산이 정말 많더군요.\n\n그중 가장 큰 산 하나가 바로 **OTA 법규와 로직을 만족**시키면서 컨테이너를 관리·생성하는 문제였습니다. CCU2는 게이트웨이 역할을 하기도 하고, In-vehicle 환경에서는 보안(Security)이 매우 중요하기 때문에 기존 Docker에서 보안적으로 취약할 만한 부분을 특화·보강해나가는 과정이 쉽지 않았습니다.  \n일종의 AP(Application Processor)에 적용된 Secure Boot 개념을 컨테이너가 실행되기 전까지 확장해서 적용한다든지, 컨테이너가 돌고 나서도 Filesystem의 무결성(Integrity)을 보장하는 방법을 찾는 과정은 상상 이상으로 힘들었습니다.\n\n보안을 강조하는 고객사를 설득하는 것도 상당한 일이었고, 고객사 내부 사이버 보안감사팀과의 소통 비용도 무시무시했습니다. 원래라면 개발 2~3년, 검증 1년을 거쳐 제품이 차에 실리는데, 검증 기간 중에도 바뀐 점이 너무 많아 개발이 끝나도 끝난 것 같지 않았죠(거의 새 프로덕트를 만든 기분?).  \n\n그래도 전 차종으로 확장될 가능성이 큰 만큼, 일단 첫 모델에 실리기만 하면 앞으로는 확장될 일만 남았다는 게 위안이었습니다. 더욱이 고객사도 이제는 소프트웨어를 외주에만 맡길 수 없는 시대가 되어 내부 개발 역량을 강화하려고 하고 있으니, 이 **Container Manager**는 앞으로 더 넓게 쓰일 가능성이 높다고 봅니다.\n\n# 운동과 건강\n\n![Desktop View](/assets/img/hyrox.jpg)\n\n나이가 40을 바라보는데도 몸은 여전히 자라고 있어 신기합니다.  \n2021년 건강검진 당시 188cm에 90kg이었는데, 지금은 97~99kg까지 늘었고 골격근량도 50kg에 가까워졌습니다. 그 변화를 이끈 요인들을 정리해봤습니다.\n\n![Desktop View](/assets/img/IMG_1642.jpeg)\n\n## 1. 심폐 능력(Zone2 트레이닝)의 중요성\n\n원래 농구 한 번 하면 2~3시간은 거뜬할 정도로 심폐 기능이 좋았지만, 2024년 1월 CrossFit을 시작하면서 고강도 트레이닝을 견딜 지구력을 높이기 위해 Dr. Peter Attia의 \"Outlive\"를 접하게 됐습니다.  \nZone2 트레이닝이 미토콘드리아 효율을 높여주고, 강인한 심폐 지구력은 운동뿐 아니라 업무에도 큰 도움을 준다는 걸 몸소 깨달았습니다. 정신이 맑아지는 것은 물론, 실제로 더 똑똑해진 기분이 들기도 하고요. 최근엔 젊어진 것 같다는 말까지 듣는데, 기분 좋은 착각이라도 마다할 이유가 없죠.\n\n## 2. 식습관의 중요성\n\n간헐적 단식과 명절 단식 등을 꾸준히 해왔지만, 2023년쯤 몸 상태가 뭔가 좋지 않다는 느낌이 들었습니다. 돌아보니 원인은 '간헐적 단식 + 간헐적 폭식과 폭음'이었죠.  \n좋지 않은 음식을 '멀리' 하는 게 훨씬 중요하다는 JYP의 말을 되새기며, 프리바이오틱스를 챙겨 먹고 통곡물을 활용한 식단으로 돌아오려 애썼습니다.  \n예컨대,  \n- 매우 매운 음식(특히 뼈찜)을 먹으면 3~4일간 컨디션이 급속히 나빠진다.  \n- 폭음(술)은 당연히 신체에 무리를 준다.  \n- 기름지고 당분 많은 음식(치킨 등)을 먹으면 반나절 이상 무기력해진다.\n\n이런 경험을 통해 \"상태가 나빠졌을 때 식단으로 집중 회복하자\"라는 원칙을 세웠고, 덕분에 2024년에는 야근이 많고 체력 부담도 컸지만 성장세를 유지할 수 있었습니다.\n\n# 마치며\n\n![Desktop View](/assets/img/IMG_1887.jpeg)\n빈둥거리는 삶을 별로 좋아하지 않는 것은 아마 아버지를 닮아서겠죠. 2024년 아버지도 임원으로 계신 회사가 코스닥에 상장하면서 눈코 뜰 새 없이 바빴는데, 그런 모습이 꼭 제 모습 같다는 느낌이 들었습니다.  \n물론 \"행복이 정말 중요한 펀더멘탈\"이라고 보는 관점은 아버지와 조금 다릅니다. 하지만 \"가족이 제일 중요하다\"는 말씀에는 전적으로 동의합니다. 저에게 가족이야말로 행복이니까요.  \n그래서 2025년에는 이 행복을 지키기 위해 더 꾸준히 배우고, 글도 많이 쓰고, 몸도 움직이며, 나누는 한 해로 만들어볼 생각입니다.\n\n![Desktop View](/assets/img/IMG_7707.jpg)\n\n*Special Thanks to my family and beautiful girlfriend Ki-Won.*","categories":["DeepThinking","Retrospect"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus","SDV"],"pubDate":"2025-02-02T15:00:00.000Z","url":"/blog/deepthinking/retrospect/2025-02-05-retro-1/"},{"id":"techsavvy/ai/2023-11-29-cps2_1","title":"DH Lee chatbot 만들기의 여정-1","description":"","content":"# 다시시작하는 CPS 2기\n\nCPS(Crash Python course for SANS family)라는 Python강의를 2023년 초부터 3개월 정도를 진행했었다.\n파이썬은 쉽고 재밌게 배울수 있으며 청중들이 이걸로 할수 있는게 엄청 많아요~ 하고 가스라이팅 하고있는데,\nChatGPT가 갑자기 혜성처럼 나타나서 강의하는 도중에 강사인 나도 이게 파이썬 강의인지 ChatGPT잘쓰기 강의인지 알수가 없을정도로 산만하게 3개월의 강의를 마무리 했더랬다.\n\n강의를 만들게 나를 조종도하셨고 열성으로 들어주신 우리 노수림교수님께 역으로 또 가스라이팅해주셔서 2024년 초에 또 2기를 시작할까하는데,\n이참에 아예 LLM(Large Language Model) Pre-trained Model 중 GPT3.5, Lamma를 이용해 뭔가를 해보자라는것이 내년 강의의 골자가 될것이다.\n\n그리고 게을로 못하던 블로깅도 매일 하자라고 마음먹었다.\n어디한번 해보자구..\n\n존경하는 DH Lee의 수많은 강의영상, 보이스, 채팅 기록을 Voice to Text등 Framework을 통해 데이터 변환하고, 레이블링 및 전처리하여,\nPretrained Model에 학습시켜 DH Lee AI Model API Service를 만들어내는것이 가장 먼저 할일이다.\n\n이 API 텔레그램이나 Slack, 카톡등에 Chatbot으로 먼저 이용해보고, 나중에 Midjourney나 다른 상용 비쥬얼 AI서비스들과 콜라보하여 만들어볼까하는 욕심도있다.\n\n뭐 일단 강사인 내가 빠르게 모든 스텝을 Work through 해보는게 목표이고, 이 과정을 블로그로 남길예정이다.\n\n그리고 매일 이러한 과정을 ChatGPT4의 음성채팅서비스를 통해 논의해가며 디테일화 하는 과정을 하고있는데, 최근 기능들이 좋아져서 문서까지 아주 만들어주니 이걸 블로그에 올리지 아니할 수 있는가..\n\n이상 마무리하고.. \n\n아래는 ChatGPT4 흑인아지매랑 논의한 내용을 기록한다.\n\n# Guide to Transcribing DH Lee's Voice Data and Training with GPT-3.5\n\n## Transcribing Voice to Text Using Microsoft Azure\n\n### Setting Up Microsoft Azure Speech Service\n1. **Create an Azure Account** and enable billing.\n2. **Create a Speech Service Resource** in Azure Portal and obtain your API key and endpoint URL.\n3. **Install Azure SDK** for Python using `pip install azure-cognitiveservices-speech`.\n\n### Python Script for Transcription\n```python\nimport azure.cognitiveservices.speech as speechsdk\n\ndef transcribe_audio(file_path, service_region, subscription_key):\n    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=service_region)\n    audio_input = speechsdk.AudioConfig(filename=file_path)\n\n    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n\n    result = speech_recognizer.recognize_once_async().get()\n    return result.text\n\nsubscription_key = \"YourAzureSubscriptionKey\"\nservice_region = \"YourServiceRegion\"\naudio_file_path = \"path/to/your/audiofile.wav\"\n\ntranscription = transcribe_audio(audio_file_path, service_region, subscription_key)\nprint(transcription)\n```\n- Replace `\"YourAzureSubscriptionKey\"`, `\"YourServiceRegion\"`, and `\"path/to/your/audiofile.wav\"` with your details.\n\n### Post-Transcription Steps\n- Review and correct the transcriptions.\n- Store the transcriptions in a structured format.\n\n## Training Transcribed Data with GPT-3.5\n\n### Data Preparation for GPT-3.5\n- Organize and clean the transcribed data.\n- Format the data as per OpenAI's guidelines for fine-tuning.\n\n### Fine-Tuning Process\n- Access the GPT-3.5 API through OpenAI, which may offer fine-tuning capabilities.\n- Upload your prepared dataset to OpenAI and fine-tune the model on this data.\n- Define training parameters as needed.\n\n### Integration and Testing\n- Integrate the fine-tuned GPT-3.5 model into your application via API.\n- Test the model's performance to ensure it aligns with DH Lee's style and content.\n\n### Considerations\n- Ensure you have adequate computational resources and expertise.\n- Obtain DH Lee's consent and consider ethical aspects of using his data.\n- Be mindful of the costs associated with API usage and training.\n\n---\n\n**Note:** This document is a summarized guide based on the conversation and should be adapted for specific project needs.","categories":["TechSavvy","AI","C++"],"tags":["TechSavvy","AI","Python","C++"],"pubDate":"2023-11-28T15:00:00.000Z","url":"/blog/techsavvy/ai/2023-11-29-cps2_1/"},{"id":"techsavvy/c/2020-11-27-constexpr","title":"constexpr","description":"","content":"## constexpr\n\n### 0. Preface\n\n필자는 constexpr 가 왜 이리도 헷갈리는지 진짜 잠을 설칠 정도였다.\n도저히 못참겠어서 이렇게 정리하기로 한다.\n\nconstexpr는 Modern cpp 인 c++11 이상에서 지원되고 잇다.\nconstexpr의 스펙은 STL 버전이 진화하면서 따라 달라지고 있다고 한다.\n\n다양한 강의자료 및 사용처들을 살펴봤는데 다들 사용하는 용도가 다른 것 같아서 그런건지, 이해하고 있는 게 다 다른 것 같다.\nconstexpr의 원작자의 의도는 분명 무언가가 있었겠지만 지금은 처음과는 많이 다른 것 같다.\n\n다만 모든 사람들이 공통적으로 하는 말은 있다.\n\n\"How is it different with const?\"\n\nconst는 그야말로 constant, 즉 상수이다.\n한번 Compile 되면 Runtime 중에 변경이 불가능한 영역의 데이터가 되어 버린다는 뜻이다.\n\nconstexpr도 의도는 비슷하다. 프로그래머가 Compile 타임에 변수나 함수가 결정되도록  하는 용도로 사용된다.\n\n하지만 constexpr는 근데 다소 모호하다.\nconstexpr 로 선언한 변수나 함수는 Compile 중에도 결정될 수 있고 Runtime 중에도 결정 될수 있다. \n[cpp reference c++1x constexpr](https://en.cppreference.com/w/cpp/language/constexpr)\n\n그럼 다음 예제들을 살펴보며 그 모호함에 빠져보기로 하자..\n\n### 1. 일반 예제\n\n``` cpp\n#include <iostream>\nint fibonacci(int n){\n    if (n &gt;= 2) \n        return fibonacci(n-1) + fibonacci(n-2);\n    else\n        n;\n}\n\nint main(){\n    std::cout &lt;< fibonacci(10) &lt;<'\\n';\n}\n````\n\n위 와 같은 피보나치 수열을 계산하는 코드가 있다고 가정하자.\n\n위 코드는 Compile 후 프로그램을 Run 할 때 해당 값을 계산하여 알 수 있다.\n\n``` sh\n\n$ /usr/bin/time ./fibonacci \n102334155\n0.56user 0.00system 0:00.56elapsed 100%CPU (0avgtext+0avgdata 3324maxresident)k\n0inputs+0outputs (0major+126minor)pagefaults 0swaps\n\n```\n\nRun 타임에 값이 계산되므로 꽤나 시간이 걸린다.\n\n### 2. Template \n\n만약 프로그래머가 Runtime 시 효율을 생각한다면 Compile 타임에 피보나치 수열을 계산한 값을 알고 싶다면 어떻게 할까?\n\n먼저 C++ [Template Meta Programming 기법](https://ko.wikipedia.org/wiki/%ED%85%9C%ED%94%8C%EB%A6%BF_%EB%A9%94%ED%83%80%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)을 이용해보자\n\n\n``` cpp\n\n#include <iostream>\n\ntemplate <int N>\nstruct fibonacci\n{\n    static int64_t const value =fibonacci&lt;N-1&gt;::value + fibonacci&lt;N-2&gt;::value;\n};\ntemplate<>\nstruct fibonacci&lt;0&gt;\n{\n    static int64_t const value = 0;\n};\ntemplate<>\nstruct fibonacci&lt;1&gt;\n{\n    static int64_t const value = 1;\n};\n\nint main(){\n    std::cout &lt;< fibonacci&lt;40&gt;::value &lt;<'\\n';\n}\n```\n\nC++ Template 기법을 이용하면 Compile 타임에 그 Compiler 가 내부적으로 Code를 생산하여 작업해준다.\n\n그러므로 위 **fibonacciI&lt;40&gt;::value** 값은 Compile 타임에 정해진다.\n\n``` sh\n\n$ /usr/bin/time ./fibonacci_template \n102334155\n0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 3388maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n\n```\n\nCompile 타임에 값이 계산되었기 때문에 실행시간이 짧다.\n\n### 3. constexpr\n\n그렇다면 **constexpr** 를 사용하면 어떨까?\n\n``` cpp\n\n#include <iostream>\nconstexpr int  fibonacci(int n){\n    return n>=2 ? fibonacci(n-1) + fibonacci(n-2): n;\n}\n\ntemplate&lt;int N&gt;\nstruct constN{\n    constN(){ std::cout &lt;< N &lt;< '\\n';}\n};\n\nint main(){\n    constN <fibonacci(40)&gt; a; // Compile time\n    //std::cout &lt;<fibonacci(40)&lt;<'\\n';  //Run time\n}\n\n```\n\n``` sh\n\n$ /usr/bin/time ./fibonacci_constexpr \n102334155\n0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 3412maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n\n```\n\nCompile 타임에 값이 계산되었기 때문에 실행시간이 짧다.\n\n재미있는 것이 **constexpr** 는 두가지 형태 모두 사용가능하다.\n\n위에 같은 코드에 아래 주석을 해제하면 값이 Runtime 에 결정되어 실행시간이 오래걸린다.\n\n``` sh\n\n$ /usr/bin/time ./fibonacci_constexpr \n102334155\n0.55user 0.00system 0:00.55elapsed 99%CPU (0avgtext+0avgdata 3376maxresident)k\n0inputs+0outputs (0major+125minor)pagefaults 0swaps\n\n```\n\n### 4. Conclusion\n\nconstexpr는 위와 같이 Compile 및 Run time에 둘다 사용이 가능하다.\nCompile시 constexpr의 요구조건에 맞지 않는다면 바로 Run time 에 계산이 되는 형태이다.\n\nconstexpr 를 이용한다면 같은 함수로 Compile time 에 값이 정해져 실행시간에서 효율을 낼 수도 있고, 원한다면  Debugger를 이용하여 Runtime 값을 Break Point에서 찍어볼 수 있다.\n\n결국 필자가 내린 결론은 \"constexpr는 프로그래머가 Compile시 값이 결정될 수도 있게 하겠다는 의도를 보여주는 기법\" 이다.","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++"],"pubDate":"2020-11-26T15:00:00.000Z","url":"/blog/techsavvy/c/2020-11-27-constexpr/"},{"id":"techsavvy/c/2021-04-06-about-l-r-value","title":"About l-r-value & Move Semantic of Modern C++","description":"","content":"## Modern C++의 l-r-value & Move Semantic 에 대하여\n\n### 0. Preface\n\n들어가기 전에 참조(&)와 포인터(*)의 기본 개념을  짚어보자.\n\n> 참조(&)는 포인터(*)와 같이 메모리 어딘가에 위치한 개체의 주소를 저장한다.\n> 하지만 참조는 한번 초기화 된 후 다른 개체를 참조하거나 null로 만들 수 없다.\n\n값(value)은 l-value 와 r-value 두가지로 분류가 가능하다.\nl-value 와 r-value 즉, 좌측값과 우측값의 정의는 C 언어로부터 내려온다.\n\n> 좌측값은 대입시에는 왼쪽 혹은 오른쪽에서 오는 식이고, 우측값은 대입시에 오직 오른쪽에만 오는 식이다.\n\n``` cpp\nint a = 0;  \na; // l-value\n0; // r-value\nPlayer player;\nplayer; // l-value\nPlayer(); // r-value\n```\n\n이는 C++ 에 넘어와서 다소 다른 방법으로 정의되었다. \n\n> 좌측값은 어떠한 메모리위치를 가리키며 &연산자로 그 위치를 참조할 수 있다. 우측값은 좌측값이 아닌값이다.\n> l-value 는 식이 끝난 후에 지속됨\n> r-value 는 식이 끝난 후에 지속되지 않음\n> l-value 는 이름있는 변수를 참조 (&)\n> r-value 는 일시적인 개체를 참조(&&)\n\n사실 이 정의도 너무나 모호하여 많은이들이 헷갈려한다.\n\n자 이제 차근차근 l-r-value에 대해 좀 더 자세히 알아보자..\n\n### 1. l-value\n\nl-value는 다소 명료하다.\n\n가능한 정리\n-  단일식을 넘어서 지속되는 개체!!\n   - 주소가 있음\n   - 이름이 있는 변수\n   - const 변수\n   - 배열 변수\n   - 비트 필드 (bit-fields)\n   - 공용 구조체 (unions)\n   - Class 멤버\n   - 좌측 값의 참조(&)로 반환하는 함수 호출\n   - 문자열 리터럴\n\n그렇다면 l-value의 대입 및 참조는 어떨까?\n\n아래 코드와 주석을 보며 이야기하자.\n\n``` cpp\n// 모두 l-value\nint a = 1;                    // global 변수 a -&gt; l-value\nint& function(){              // l-value function은 return 하는 a의 주소값으로 초기화\n    a = 3;                    // a-&gt; l-value, 대입가능  \n    return a;\n}\n\nint main()\n{\n    int i = 3;                // i 는 l-value\n    i = 4;                    // i 는 l-value, 대입가능\n                              // i -&gt; 4\n    int *ptr = &i;            // i 는 l-value & 연산자 참조가능, ptr는 i의 주소값을 가르킴\n                              // *ptr 는 i의 값 -&gt; 4\n    ptr = &a;                 // ptr 는 int * 타입의 pointer이므로 참조하는 주소를 변경가능\n                              // *ptr 는 a의 값 -&gt; 1\n    int & r = i;              // l-value인 i의 주소값으로 r은 초기화, 즉 r은 l-value 가 대입된 l-value\n    r = 5;                    // r이 가르키는 주소값에 그 위치에 값을 씀, 즉 i의 값이 변함(r 과 i 는 5)\n                              // r 과 i 둘 다 같은 주소를 가르키고있음 -&gt; 5\n    int c = function();       // l-vlaue의 참조는 초기화시에만 그 주소를 저장\n                              // c -&gt; function의 내부 스택의 연산후의 return 값인 바뀐 a 값 3\n    function() = 50;          // function 은 l-value, r-value 50을 대입가능\n                              // a 의 값이 50으로 변한다. \n                              // 그리고 ptr 는 a의 주소를 가르키므로 *ptr -&gt; 50 으로 대입된다.\n    int d = a;                // l-value d는 a 값인 50이 대입된다.\n    int *ptr_2 = &function(); // int *타입의 ptr_2는 &function 가능(funtion은 좌측값이므로)\n                              // function의 내부 스택의 연산후의 return 값인 바뀐 a 값 3\n                              // * ptr_2 -&gt; 바뀐 a의 값 -&gt; 3\n                              // c 의 값 도 변함 -&gt; 3\n                              // * ptr 의 값도 변함 -&gt; 3\n    return 0;\n}\n```\n\n좌측값의 대입은 위처럼 비교적 명료하여 쉽사리 이해된다.\n\n### 2. r-value\n\n하지만 r-value 는 정의가 다소 어렵다..\n\n일단 가능한 정리는 아래와 같다.\n\n- l-value가 아닌 개체... (대우 명제ㅎㅎ..)\n- 사용되는 단일식을 넘어 지속되지 않는 일시적인 값!!\n  - 주소가 없는 개체\n  - 리터럴(문자열 리터럴 제외)\n  - 참조로 반환하지 않는 함수 호출(e.g. int function())\n  - i++ 와 i-- (하지만, ++ 와 -- 연산자는 l-value 이다)\n  - 기본 산술식, 논리식, 비교식(+,-,*,=, &lt;,&gt; 등, 연산자 overload 시 달라질수 있음)\n  - 열거형(enum)\n  - 람다(lambda) (즉, 컴파일러는 람다(무명함수)를 일시적인 것으로 판단한다..)\n\n예시들을 보자.\n\n\nr-value 의 예제1\n\n``` cpp\nint num1 = 10;\nint num2 = 15;\n\nif (num1 &lt; num2) // (num1 &lt; num2)는 r-value\n{\n    ...\n}\n```\n\n\nr-value 의 예제2\n\n``` cpp\nint function();         // funtion 은 호출후 지속되지 않음 -&gt; r-value \nfunction();             // r-value\nint i = 0;\ni = function();         // r-value function의 return값을 대입한 i 는 l-value\nint *ptr = &function(); // ERROR!!! r-value의 주소는 참조할수 없다\n```\n\n초기 c/c++의 설계자들은 r-value를 일시적 개체로 정의했기 때문에 \n기본적으로 r-value에는 대입이되지 않게 만들어 두었다.\n\n하지만 Modern C++ (stdc++11)에서 r-value의 \"**move semantic**\" 을 도입한 이후에 달라졌다.\n\n### 3. Move Semantic\n\n아래와 같은 시나리오를 생각해보자.\n\n``` cpp\n//Math.cpp\nstd::vector&lt;float&gt; Math::ConvertToPercentage(const std::vector&lt;float&gt;& scores){\n    std:vector&lt;float&gt; percentages;\n    for (auto& score : scores){\n        //...\n    }\n    return percentages;\n}\n// main.cpp\n#include \"Math.h\"\nint main(){\n    std::vector&lt;float&gt; scores;\n    //...\n    scores = ConvertToPercentage(scores); \n    //...\n}\n\n```\n\nmain 함수에서 ConvertToPercentage(scores)를 호출하면,\nMath 클래스내의 이 함수는 std::vector&lt;int&gt; 타입의 percentages의 임시값인 r-value을 생성한다.\n그리고 대입연산자인 \"=\"에 의해 scores에 대입된다.\n\n이 경우 Modern C++ 이라 불리우는 C++1x 이전인 구 C++에서는 위와 같은 상황에서 어떻게 동작할까?\n\nscores의 초기화시 생성된 메모리 영역에 percentages의 임시 값이 **대입**되는 순간에 **복사**가 된다.\n그리고 임시 값인 r-value는 Math::ConvertToPercentage 함수 스택을 나오며 사라지게 된다.\n\n> 여기에서 상당히 불필요한 과정이 있는데, 이는 percentages 임시값을 **복사** 하는 순간이다.\n\n상식적으로 생각해보자.\n\n임시로 생성된 percentages의 결과값(r-value)이 저장된 메모리 영역과\nscores의 초기화 과정에서 생성된 메모리 영역을 단순히 바꿔(**Swap**)버리면,\n어짜피 임시로 생성된 percentages의 메모리영역은 스택을 빠져나오며 없어질 것이기 때문에 \"**복사**\"라는 과정이 필요없게 된다.\n\n이 불필요한 **복사**를 막는 방법이 바로 Modern C++ 가 자랑하는 r-value의 참조와 Move Semantic의 핵심이다.\n\n> 사실 최근엔 컴파일러느님들이 알아서 잘해준다.. 사실 r-value의 참조를 너무 남발해서 시스템이 느려지는 경우도 종종발생한다. ㄷㄷ\n\n### 4. r-value 참조 (&&) 와 std::move\n\nr-value 의 참조는 Modern C++ 인 C++11에 처음 등장한 연산자이다.\n기능상 & 연산자와 비슷한 역할을 한다.\n\n아래 예제는 move semantic을 이용하여 기본 타입들의 참조를 보여준다.\n\n``` cpp\n#include <memory>\n#include <utility>\nfloat CaculateAverage(){\n    float average=3;\n    // ...\n    return average;\n}\n\nint main(){\n    int num = 10;                           // num -&gt; l-value\n    //int && rNum = num;                      // Error!!! num 은 l-value -&gt; int & lnum 은 가능\n    int && rNum = std::move(num);           // l-value인 num을 참조할 수 있도록 해주는 move\n    num = 30;                               // num 을 바꾸면 rNum도 바뀐다 (reference 값이기때문)\n    \n    int && rNum1 = 10;                       // OK, 10 은 r-value\n\n    float && rAverage = CaculateAverage();   // OK CaculateAverage 는 r-value\n    float tmp_1 = std::move(rAverage);       // tmp_1 는 l-value, 즉 대입됨\n    float tmp_2 = rAverage;                  // tmp_2 는 l-value, 즉 대입됨\n    float && tmp_3 = std::move(rAverage);    // tmp_3 는 l-value, 그러나 r-value의 참조(reference) 값\n    rAverage = 1.0f;                         // rAverage 를 바꾸면 tmp_3 값만 바뀜, tmp_1, tmp_2 는 대입\n}\n```\n\n\n\n## Appendix.A References\n\n1. [https://m.blog.naver.com/yoochansong/222082508401](https://m.blog.naver.com/yoochansong/222082508401)\n2. [https://docs.microsoft.com/ko-kr/cpp/cpp/references-cpp?view=msvc-160&viewFallbackFrom=vs-2019](https://docs.microsoft.com/ko-kr/cpp/cpp/references-cpp?view=msvc-160&viewFallbackFrom=vs-2019)\n3. [https://skstormdummy.tistory.com/entry/%EC%9A%B0%EC%B8%A1-%EA%B0%92-%EC%B0%B8%EC%A1%B0-RValue-Reference](https://skstormdummy.tistory.com/entry/%EC%9A%B0%EC%B8%A1-%EA%B0%92-%EC%B0%B8%EC%A1%B0-RValue-Reference)\n4. [https://modoocode.com/189](https://modoocode.com/189)","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++","l-value","r-value","move"],"pubDate":"2021-04-05T15:00:00.000Z","url":"/blog/techsavvy/c/2021-04-06-about-l-r-value/"},{"id":"techsavvy/c/2020-11-27-cv-qualifiers","title":"CV-qualifiers","description":"","content":"## CV-qualifiers\n\n### 0. Preface\n\nconst 는 Contatnt 즉 상수를 표현하기 위한 기법이고 volatile 은 휘발성(?) 타입이라는 것을 표현하기위한 기법이다.\nSTL에서 cont는 volatile 과 함께 cv qualifiers 로 정의한다. [Link:](https://en.cppreference.com/w/cpp/language/cv)\n\n### 1. Notation\n\nC++ 에서 cv qualifiers 와 같은 type 한정자(?)는 type의 왼쪽 및 오른쪽 양쪽 다에 올 수 있다.\n\n예를 들어보자\n\n``` cpp\n    const int i = 100;\n    int const i = 100;\n```\n\n여러 프로그래밍 언어를 하는 독자들은 다소 헷갈릴 여지가 있는 것이, const 키워드는 보통 타입의 왼쪽에 표기되기 때문이다.\n\nC와 C++ 에서는 위 코드에서 둘다 맞고 동일한 표현이다.\n\n> 다만 const는 오른쪽에서 왼쪽으로 수식해야하고, 왼쪽에 수식대상이 없을때만 왼쪽에서 오른쪽으로 수식한다.\n\n왜 이런 기법을 만들었을까? \n\n#### 가독성 측면의 강점\n\n위 예제에서는 동일했지만 C/C++에서 Pointer와 사용시엔 다소 다르다.\n\n``` cpp\n    const char *const s = \"aaa\";\n    char const *const s = \"aaa\";\n```\n\n위 두 표현도 둘다 같은 표현이지만 const 표현이 두번 혼재되어있고, \n둘중 윗 예제에선 하나의 const 는 왼쪽에서 오른쪽을 하나의 const는 오른쪽에서 왼쪽을 수식한다.\n\n때문에 식이 복잡하다면 항상 오른쪽에서 왼쪽으로 const 를 표기하도록 하는 Convention을 지키는 것이 가독성에 유리할 것이다.","categories":["TechSavvy","C++"],"tags":["TechSavvy","ProgrammingLanguage","C++"],"pubDate":"2020-11-26T15:00:00.000Z","url":"/blog/techsavvy/c/2020-11-27-cv-qualifiers/"},{"id":"techsavvy/bash/2020-11-17-bash-getipaddr","title":"Bash for and while?","description":"","content":"## for\n\n``` sh\n$ cat for.sh \n#!/bin/bash\n\nLISTS=(\"a\" \"b\" \"c\") \n\n# Don't forget Brace!\necho ${LISTS[0]}\necho ${LISTS[1]}\necho ${LISTS[2]}\n# Don't forget Brace!\nfor item in ${LISTS[@]}\ndo\n    echo $item\ndone\n$ ./for.sh\na\nb\nc\na\nb\nc\n```\n\n아래는 find, eval, sed 를 함께 쓴 예제\n\n``` sh\nfor file in $(find . -name \"*.txt\")\ndo\n    echo $file\n    eval sed -i \"/confidential/d\" $file # confidential 이 포함된 라인 지우기\ndone\n```\n\n\n## while\n\n필자는 while보다는 for문을 더 좋아하는 스타일이라 잘 쓰진않지만,\n텍스트 파일로 부터 line 을 읽어들여 작업하는 용도로 좀 쓴다.\n\n```sh\n$ while read line; do git rm -r $line; done &lt; remove.lst\n```\n\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-getipaddr/"},{"id":"techsavvy/bash/2020-11-17-bash-andor","title":"Bash Directory Exists?","description":"","content":"## Directory Exists?\n\n``` sh\n$ cat directory_ex.sh \n#!/bin/bash\ntest_dir_exist(){\n    set -e \n    if [ -e \"/home/jayleekr/workspace/00_codes/05_info_archive\" ];then\n        echo \"DIR Exist\"\n        exit 1\n\n    fi\n}\ntest_fail(){\n    echo \"test_fail\"\n}\ntest_dir_exist\n```\n\n``` sh\n$ ./directory_ex.sh\nDIR Exist\n```\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-andor/"},{"id":"techsavvy/bash/2020-11-17-bash-directoryexists","title":"Bash and & or with if","description":"","content":"## And\n\n``` sh\n$ cat and_or.sh \n#!/bin/bash\ntest_and_or(){\n    TEST=\"11\"\n    TEST2=\"22\"\n    if [ \"$TEST\" == \"11\" ] && [ \"$TEST2\" == \"22\" ] ;then\n        echo \"String Exist by And\"\n    fi\n\n    if [ \"$TEST\" == \"33\" ] || [ \"$TEST2\" == \"22\" ] ;then\n        echo \"String Exist by Or\"\n    fi\n}\n\ntest_and_or\n```\n\n``` sh\n$ ./and_or.sh\nString Exist by And\nString Exist by Or\n```\n\n#### 1.3 Directory Exist\n\n``` sh\n$ cat directory_ex.sh \n#!/bin/bash\ntest_dir_exist(){\n    set -e \n    if [ -e \"/home/jayleekr/workspace/00_codes/05_info_archive\" ];then\n        echo \"DIR Exist\"\n        exit 1\n\n    fi\n}\ntest_fail(){\n    mkdir /a\n    echo \"test_fail\"\n}\ntest_dir_exist\n$ ./directory_ex.sh\nDIR Exist\n```\n\n### 2. set \n\n#### 2.1 set -e\n\nset -e 가 script 내에 실행되어있으면 해당 스크립트가 동작하는 환경은 script명령어가 error 를 발생하면서 죽었을때 다음 명령을 수행하지 않는다\n\n### 3. exit\n\n일반적으로 unix 에서는 0 은 성공, 1~255 는 error code 로 인식됨\n인식되는 범위는 0~255 16bit\n$? 로 결과값을 확인 가능\n\n``` sh\n$ cat test.sh\necho \"hello\"\nexit 100\n$ sh test.sh\nhello\n$ echo $?\n100\n```\n\n### 4. eval\n\neval 뒤의 string arg 들을 command 로 실행하게 해줌 \nstring 수준에서 최종 Command 조작 후 실행시 유용하다\n\n### 5. array(list)\n\nBash 에서는 괄호로 Array를 표현가능하다\n그 안에서 구분자는 아무래도 띄어쓰기(space bar) 이다\n\n``` sh\n$ cat array_ex.sh\n#!/bin/bash\nlists=(\"a\" b \"c\")\necho ${lists[1]}\necho ${lists[0]}\necho ${lists[3]}\necho ${lists[2]}\n$ sh array_ex.sh\nb\na\n\nc\n```\n\n### 6. sed\n\nsed Utility를 사용안해본사람은 있어도 한번만 사용한 사람은 없다 전해지는 전설의 레전드\n\n### 7. while\n\n필자는 while보다는 for문을 더 좋아하는 스타일이라 잘 쓰진않지만,\n텍스트 파일로 부터 line 을 읽어들여 작업하는 용도로 좀 쓴다.\n\n```sh\n$ while read line; do git rm -r $line; done &lt; remove.lst\n```\n\n### 8. for\n\n``` sh\n$ cat for.sh \n#!/bin/bash\n\nLISTS=(\"a\" \"b\" \"c\") \n\n# Don't forget Brace!\necho ${LISTS[0]}\necho ${LISTS[1]}\necho ${LISTS[2]}\n# Don't forget Brace!\nfor item in ${LISTS[@]}\ndo\n    echo $item\ndone\n$ ./for.sh\na\nb\nc\na\nb\nc\n```\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-directoryexists/"},{"id":"techsavvy/bash/2020-11-17-bash-string-comparision","title":"Bash String Comparision with if","description":"","content":"## Bash string comparision using \"if\"\n\n\"==\" and \"!=\" only can be used in case of string comparision.\n\n``` sh\nif [ \"$STRING\" == \"abc\" ];then\n    echo \"STRING is abc!\"\nfi \n\nor\n\nif [ \"$STRING\" = \"abc\" ];then\n    echo \"STRING is abc!\"\nfi \n```\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-string-comparision/"},{"id":"techsavvy/bash/2020-11-17-bash-array","title":"Bash list?","description":"","content":"## array(list)\n\nBash 에서는 괄호로 Array를 표현가능하다\n그 안에서 구분자는 아무래도 띄어쓰기(space bar) 이다\n\n``` sh\n$ cat array_ex.sh\n#!/bin/bash\nlists=(\"a\" b \"c\")\necho ${lists[1]}\necho ${lists[0]}\necho ${lists[3]}\necho ${lists[2]}\n$ sh array_ex.sh\nb\na\n\nc\n```\n\n아래 예제와 같이 Slicing도 지원된다.\n\n``` sh\nlists=(\"V0.1.0\" \"V1.0.0\")\necho \"[1] : \"${lists[1]}\necho \"[0] : \"${lists[0]}\necho \"[3] : \"${lists[3]}\necho \"[-1] : \"${lists[-1]}\n\nselected=${lists[-1]}\necho \"selected : \"$selected\n``` \n\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-array/"},{"id":"techsavvy/bash/2020-11-17-bash-eval","title":"Bash eval utility?","description":"","content":"## eval\n\neval 뒤의 string arg 들을 command 로 실행하게 해줌 \nstring 수준에서 최종 Command 조작 후 실행시 유용하다\n\n## usages \n\n\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-eval/"},{"id":"techsavvy/bash/2020-11-17-bash-for","title":"Bash Get Ip address?","description":"","content":"## Get Ip Address\n\n만약 ethernet interface가 eth0 인 경우\n\n``` sh\n$ IPADDR=$(ifconfig eth0|grep inet|head -1|sed 's/\\:/ /'|awk '{print $2}')\n$ echo $IPADDR\n172.17.0.3\n```\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-for/"},{"id":"techsavvy/bash/2020-11-17-bash-set","title":"Bash set utility?","description":"","content":"## set \n\n### set -e\n\nset -e 가 script 내에 실행되어있으면 동작하는 쉘 환경은 script명령어가 error 를 발생하면서 죽었을때 다음 명령을 수행하지 않는다\n\n### exit code\n\n일반적으로 unix 에서는 0 은 성공, 1~255 는 error code 로 인식됨\n인식되는 범위는 0~255 16bit\n$? 로 결과값을 확인 가능\n\n``` sh\n$ cat test.sh\necho \"hello\"\nexit 100\n$ sh test.sh\nhello\n$ echo $?\n100\n```\n\n### set -x\n\nset -x 가 script 내에 실행되어있으면 동작하는 쉘 환경은 모든 쉘 커맨드를 verbose 모드로 보여준다.\n\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-set/"},{"id":"techsavvy/bash/2020-11-17-bash-sed","title":"Bash sed?","description":"","content":"### 6. sed\n\nsed Utility를 사용안해본사람은 있어도 한번만 사용한 사람은 없다 전해지는 전설의 레전드\n\n\n\n\n## Appendix. References\n\n- General : [http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html)","categories":["TechSavvy","Bash"],"tags":["TechSavvy","ProgrammingLanguage","Bash"],"pubDate":"2020-11-16T15:00:01.000Z","url":"/blog/techsavvy/bash/2020-11-17-bash-sed/"},{"id":"deepthinking/daily/2023-11-19-daily","title":"2023 첫 포스팅","description":"","content":"2023년 첫 포스팅이 병원에 입원해서 적는 글이 될거라고는 꿈에도 상상하지 못했다.\n\n뭐 과로라던지 일과 관련된건 아니고 밤 농구하고 돌아오는길에 신호위반 차량에 아주 제대로 정면추돌해서 내차(소렌토)는 완전 폐차급으로 반파되었다.\n\n상대방 운전자는 보조석에 와이프분과 뒷자석에 직장동료(?)로 추정되는 분이 타고있었고 좌회전 신호를 받았다고 착각하셔서 직진 신호에 좌회전으로 달려드셨다. 그 바람에 2차선으로 잘가고있던 나는 50~60 km/h로 정면 추돌해서 순간적으로 엄청나게 아드레날린이 분비되는 그 경험을 다시 한번 겪게 되었다.\n\n내가 첫번째로 그 사건을 겪었을때는 [자살하려고 떨어지는 사람을 구했을때다.](https://news.nate.com/view/20091213n06658?fbclid=IwAR3wZ-IMRg0_tf87h_aEP5yVmKr-hDrBQJWq2eMFh8Jkh3mgcMUnoraOBEw) (내인생 생각해보니까 참 파란만장하다) \n\n그때도 그 아주머니가 베란다에서 떨어지는 그 순간이 뭔가 포토그래픽 메모리처럼 뇌리에 박혔을 정도로 엄청난 아드레날린이 분비되며 찰나의 순간들이 엄청나게 길게 느껴졌는데, 이번에도 차가 내쪽으로 다가오는데 핸들을 붙잡고 전방을 주시하던 내가 뭔가 유체이탈 된 느낌이 들더라...\n\n다행히 괜찮아서 이렇게 살아있음에 감사함을 느끼고 블로깅을 할수있음에 또 감사한다. \nㅋㅋㅋㅋ\n\n갑자기 진료가야해서 급마무리....","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2023-11-19T04:00:00.000Z","url":"/blog/deepthinking/daily/2023-11-19-daily/"},{"id":"deepthinking/daily/2024-01-06-dialy","title":"2024-01-06 P의 하루","description":"","content":"Recap Jan 6, 2024\nDisclaimer: 상당히 주관적인 뷰로 작성됨.\n\nPhase 1 - PT..?\n11시까지 야핏스튜디오에서 같이 피티를 하자시던 두목님의 지령을 받고 도착한 그곳에는 선생님은 안계셨다. 분명 다른 선생님이 계시니 그냥 오면 된다 했던 그날의 선생님은 나였다. 두목님 빼고 아무도 모르는. 세상에 몇안된다 생각하는 나보다 몇배는 강력한 P의 소유자 두목님께서 \"자 자칭 운동 20년 전문가 이재원 선생님 운동가르쳐보세요\" \n갑자기 일일 피티 선생님이 되어버린 나는 순식간에 몇가지 프로그램을 떠올려 한시간을 어찌어찌 즐겁게 운동했다. 뉴져지에서 내가 아침운동하러 갈때 따라온 두목님께 대강알려드린 몇가지 운동법이 있는데 그때만해도 잘 못따라하셨던 동작을 이제는 무지하게 잘하시는 걸보니 두목님의 노력이 내눈에는 아주 잘 보였다. 게다가 7-8년만에 운동하신다는 성경형님은 공백기간이 안느껴질만큼 동작을 잘소화하셨는데 역시 서울대 체대때의 기억이 근육에 각인되셨는지 머슬메모리를 순식간에 불러 일으키셨다.\n두목님께서는 엄청난 땀을 흘리셨는데 그 빠져나간 염분을 채우기 위해 우리는 운동 후 순대국을 먹었다..... (두목님만 특대)\n\nPhase 2 - 급발진\n그렇게 운동이 끝나고, 갑자기 스키장을 가게 되었다. P형님들의 P스러운 실로 갑작스러운 제안이었는데 순도 99프로 P인 나는 호기롭게 제안을 받아들였다. 심지어 운동하기위해 대강 트레이닝복만 입고나왔는데 운동 후 그대로 횡성으로 출발하게 되었다.\n횡성까지 단박에 도착한후 성경이형이 오전의 운동+전날의 피곤함으로 두시간 기절한사이 한끼먹을걸 위해 장을 보자시는 두목님. 용평에서 유명하다는 J마트로 간다. 분명 한끼먹을거만 사신다했는데... 급발진하셔서 20만원어치를 사시더니 내일 점심까지 이걸로 처리하자 하신다. (횡성에 맛있는거 많은데 ㅠㅠ.. 하지만 맛있었읍니다) \n그렇게 매운순두부고추장열라면에 큰햇반으로 밥두공기먹고 우리는 보드타러나간다...\n\nPhase 3 - On-boarding....\n\n도착한 용평스키장의 풍경은 너무나 아름다웠다. 용평스키장 초입쪽에 있는 리조트들을 보니 대학원시절 겨울마다 용평에서 즐겼던(?) 학회가 떠오른다.\n밤새 술먹고 스키or보드를 탄 후 두시간잔상태로 Oral발표 잘했었던 때가 있었는데.. 갑자기 어지럽다.\n적당히 추운 섭씨 0~3도의 날씨에 눈이 살짝 내리는 날씨에 우리는 리프트에 올랐다.\n정말 호기롭게 왼발에 보드데크를 락업하고 올라갔는데.. 충격적이게도 앉은상태에서 데크에 놓인 오른발에 내 손이 닿지 않더라...\n이제와서 계산해보니 보드를 타본지 8년이 넘었었는데 왜그리 자신있었는지.. 처음 보드를 접했을때 일어나는데 3시간이 걸렸던 일이 뇌리에 떠오른다.\n준비를 마치고 기다리는 형들에게 쪽팔리지만 그자리에서 일어나서 고관절 및 장요근 스트레칭을하고 다시 락업을 하려 노력하지만 잘되지 않는다.\n이 망할 몸뚱이.. 심지어 복근이 수축할때 오전에 했던 플랭크때문에 살짝씩 쥐도 나려했다. \n골프 내기중에 컨시드 바로 앞라인에 걸린 1.51m 퍼팅을 남긴상태의 긴장감과 같은 느낌으로 고도의 집중하여 락업을 성공했다!\n다만 다음 문제는 일어나지 못하는것이었다. 그렇다 쪼그려앉은 상태에서 보드 데크 가운데에 손이 안닿였던것이다.\n하지만 예전의 기억을 떠올려 보드를 신은채로 복근힘으로(?) 배를 180도 뒤집어까서 일어났다.\n\nPhase 4 - Snowboarding! And..\n![Desktop View](/assets/img/2024_01_06_2.jpg)\n쪽팔림보다는 남자는 패기다라는 생각으로 힘겹게 일어나서 탄 스노우보딩은 그간의 쪽팔림을 모두 잊게 해줄정도로 좋더라.\n타이밍좋게 흩날려주는 눈사이를 누비며 날라가서 고꾸라져 눈을 먹어도 행복하더라.\n정말 더욱더 운이 좋았던것은 성경이형이 대학교시절 스키&보드 강사들을 가르치는 강사였던것이다.\n형님께 속성으로 배운 꼬임(?)을 통한 방향전환을 나름 속성으로 그럭저럭 몸으로 받아들여 정말 재미있게 보드를 타고 놀았다.\n오른쪽 턴할때는 골프 백스윙할때의 복근의 꼬임, 왼쪽턴할때는 골프에서 팔로스루할때는 꼬임을... (골프에 미치면 이렇게됨)\n여튼 너무 재미있었다. \n두목님은 새로 개시하신 고급 스키를 슉슉타시고, 성경이형은 10년만에 타신다면서 눈사이를 무슨 미사일처럼 헤집으시고..\n그렇게 3시간의 야간 스키를 즐기고 우리는 숙소로 돌아와서 적당히 맛있는 닭갈비를 주문해 픽업후(20만원어치에 플러스로) 숙소에서 좋은 술마시며 놀았다.\n\nRecap end.","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2024-01-05T15:00:00.000Z","url":"/blog/deepthinking/daily/2024-01-06-dialy/"},{"id":"deepthinking/daily/2024-08-23-daily","title":"2024-08-23 Vehicle Software 양산에 대한 단상 1","description":"","content":"이제 하고있는 CCU2 프로젝트가 Mass production에 가까워져오면서 출장이 요즘 참 잦다.\nAutomotive Industry에서 약 10여년 일하면서 이번이 두번째로 겪는 MP인데, 이 시기가 다가오면 모두가 민감해진다.\n\n정말 매일매일이 올라오는 이슈들을 파악하고 처내는 전쟁에 가까운데, 이시기를 정리하고 넘어가지않으면 나중에 다 잊을 것 같아 이 기록을 남기기로 마음 먹었다.\n\n자주 남길 수 있을지는 모르겠지만 그래도 최대한 자주 남기기로한다.\n\n이 시기가 언제냐라고 생각해보면 꽤나 긴 기간이다.\n\n보통은 PoC(Proof of Concept)를 마치고 Bidding을 통해 업체선정이 완료되면 그때부터는 Mass Production 시기라고 볼 수있다.\nPoC기간만해도 짧으면 6개월 CCU2의 경우에는 2년이 넘게 진행하면서 Feature들의 개발을 마쳤고, 업체선정 완료 후 2023년 말부터 Mass Production 시기에 진입하여 2024년 말 ~ 2025년 초에 CCU2 프로젝트가 처음으로 실리는 첫차종이 나온다.\n그리고 이 차종을 기반으로 확대전개되어 몇십차종들이 전세계의 도로에 돌아다닐 것이니, OEM입장에서는 프로젝트의 첫 차종의 첫 MP가 가장 중요하다.\n\n이 Mass Production 기간의 중요한 이벤트들은 순서대로보면 Wire car -&gt; Proto car -&gt; Master Car -&gt; Pilot 1 -&gt; Pilot 2 -&gt; M Stage -&gt; SoP 이다.\n\n보통은 이 기간들이 3개월의 텀을두고 있기때문에 12~15개월의 검증&보완 기간이 이루어진다.\n\nPoC기간동안은 OEM의 R&D 조직과 같이 고분분투하며 일했다면, Mass Production기간에는 OEM의 R&D조직과 한편이되어 OEM내부의 Quality Assuarance 조직, Cyber security 조직 등의 다양한 시각의 검증프로세스를 넘어야한다.\n\nWire Car 때는 차안에 들어가는 단품(ECU)들을 처음으로 그야말로 전선으로 엮는다.\n\n보통은 대혼란의 시기다.\n\n차량이라는 것을 거대한 머신이라고 생각해보면 그 들어가는 컴퓨터들이 정말 많고 그 컴퓨터들이 동작하는 시스템의 상태정의, 동작 정의 이런 구조들이 정말로 복잡하다.\n시스템 레벨의 상태를 동작시키는 것도 Software + Hardware의 집합체이고 이들끼리의 동작도 실제로 처음 다 연결해서 해보는 시기기때문에 이때는 네트워크단의 문제를 가장 중점적으로 잡고 넘어간다. 대부분은 진단(Diag)메세지 프로토콜로 정의하여 다양한 네트워크 인터페이스로 왔다갔다하는데 이 메세지로 시스템레벨의 동작, 예를들면  Acc On/Off IGN On/Off 와 같은 전원 동작으로 다양한 Power state에 따라 동작들이 잘 이루어지는지를 체크한다.\n\n이 과정에서 Hardware문제보다는 Platform Software의 문제가 엄청 많이 걸러진다. 예를들면 Someip libary, adaptive autosar & classic autosar library 그 위에단에서 Abstract하는 Middleware software (SOA adapter 등)의 Benchmark테스트가 이루어지고 동작이 안한다면 많은 패치가 이루어진다.\n\n각자 이러한 H/W, Low level S/W, Middleware S/W개발하는 회사들은 다 다르고, 그들의 각자 경험도 다르고 수준도 다른 상황이라 엄청난 문제들이 쏟아지잔다.\n\n일을 하다보면 여기서 각 회사들의 민낯이 까발려진다.. 재정적 상황(Resource), 실력 등등말이다.\n\n그 긴기간의 PoC기간동안 아무리 다른 프로덕트들에 대한 많은 정보들이 없이 그들의 프러덕트만 개발을 해왔다하더라도, OEM은 그 상황에 맞는 최소한의 정보들은 준다. 예를들면 시스템사양, 시스템 테스트 사양, 연관된 소프트웨어의 테스트 사양들을 전달해주고 해당 업체는 이 상황에 맞는 테스트 시나리오 및 결과들도 제출을 해야한다.\n\n마치 아주 잘 정제된 상황의 정제된 데이터로만 논문을 쓰는것과 같이 프러덕트를 만드는 업체들, 즉 경험이 많이 부족한 업체들은 이 단계에서 아주 도태된다. 이미지가 실추되기도하고 엔지니어들이 우울증으로 잠수 혹은 사라지기도하고 별일을 다 겪는다.\n\n이렇게 Wire Car가 이벤트가 빡센 것 같지만 사실 이제까지가 맛뵈기다.\n\n그 이후에 다가오는 Event들은 마라맛이거든....\n\n다음편에 이어서..","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus","SDV"],"pubDate":"2024-08-22T15:00:00.000Z","url":"/blog/deepthinking/daily/2024-08-23-daily/"},{"id":"deepthinking/daily/2025-02-08-daily","title":"2025-02-28 What makes me happy","description":"","content":"# 지사장님과의 1-on-1 대화\n\n어제는 한국 지사장님과 1-on-1 미팅이 있었습니다. 우리 지사장님과의 1-on-1은 항상 특별합니다. 다른 회사의 1-on-1과는 다르게 업무 목표나 성과에 대한 이야기보다는 \"삶을 어떻게 살아야 하는가\", \"지금 여자친구와의 관계는 어떠한가\", \"집안일은 어떠한가\" 같은 인간적인 대화가 주를 이룹니다.\n\n지사장님이 항상 존경스러운 부분은 직원들의 일상 속 중요한 부분들, 각자가 무엇을 중요하게 여기는지와 같은 인간의 깊은 부분에 대해 항상 궁금해하시고 그런 부분을 챙겨주시는 점입니다. 인간적인 면모로 직원들에게 다가오시는 모습에 항상 감동받습니다.\n\n이번 1-on-1에서는 제가 어떤 일을 할 때 더 재미있어하는지, 어떤 방식으로 일을 다루고 접근하는지에 대해 이야기를 나눴습니다. 사실 이번에는 상무님을 통해 받았던 내부 포지션 변경 제안에 대해 이야기하실 줄 알았는데, 그 내용보다는 좀 더 본질적인 이야기를 나누게 되었습니다.\n\n# 나와 지사장님의 공통점: 일의 접근 방식\n\n지사장님께서 흥미로운 얘기를 해주셨습니다. 포지션 변경을 원한다면, 그 변경을 결정할 수 있는 사람이 자연스럽게 그 필요성을 느끼도록 만들어야 한다는 것이었습니다. 이는 제 생각과 상당히 일치합니다.\n\n저 역시 일을 진행할 때 완벽한 준비를 하고 시작하기보다는 일단 시작해보고, 무엇이 필요한지 파악한 후 그것을 자연스럽게 얻을 수 있도록 주변 사람들의 생각을 바꾸는 방식을 선호합니다. 제가 생각하고 있는 방향을 상대방에게 인지시켜 자연스럽게 그 선택을 할 수 있도록 돕는 방향으로 일을 진행하는 편입니다.\n\n이야기를 나누다 보니 지사장님과 저의 공통점이 더 드러났습니다. 우리 둘 다 디테일이 부족한 사람들입니다. 반면 제가 같이 일하는 대부분의 소프트웨어 개발자들은 상당히 디테일한 편입니다. 디테일하지 않으면 많은 버그를 만들 수밖에 없는 직종이기 때문입니다.\n\n제가 잘하는 일은 '일을 잘 벌리는 것'입니다. 고객과의 관계를 중요시하고, 고객이 급하게 요구하는 일들을 최대한 빠르게 해결하려고 노력합니다. 어떤 개발자들은 고객이 갑자기 요구사항을 바꾸거나 뒤늦게 새로운 요청을 할 때 불만을 표시하지만, 저는 다른 관점을 가지고 있습니다.\n\n예를들어 함께일하는 매일 같이 싱크업을하는 Poland에 있는 Sebastian은 인생을 매우매우 디테일한 계획적으로 살아야 한다고 생각하고, 자신이 짜놓은 계획이 틀어지는 것을 참지 못하는 성격입니다. (심지어 아이가 셋이 있는데 아이들의 생일조차 같은 월에 태어나게 하는 엄청난 디테일함... ㅋㅋㅋ) \n\n하지만 저는 우리가 고객의 성공을 위해 프로젝트를 진행한다고 생각합니다. 프로젝트를 위해 계획대로만 진행해야 한다고 생각하지 않습니다. 항상 그 정도 버퍼는 두어야 하고, 고객 요구를 빠르게 수용해서 해결해주는 역할이 중요하다고 봅니다.\n\n# 내가 재미를 느끼는 지점\n\n최근 제가 무엇에 흥미를 느끼는지 깊이 생각해보았습니다. 저는 Sonatus에 4x번째 직원으로 합류했는데, 이제 회사는 300명이 넘는 글로벌 9개 브랜치를 가진 거대 기업으로 성장했습니다. 그리고 지금도 엄청나게 빠르게 성장중이죠. 당시 안정적인 직장을 과감히 떠나 이 작은 회사에 조인했던 이유를 되돌아보면, 새로운 도전을 좋아하는 제 성향이 크게 작용했습니다.\n\n저는 안정적이어서 할 일이 많지 않은 환경을 재미없다고 느낍니다. 주어진 일만 해야 하고 액티브하게 행동할 권한이 없는 상황을 잘 견디지 못합니다. 회사 초기에는 소프트웨어 엔지니어를 넘어서 고객 대응도 하고, 인프라, 필드 시스템, 컨테이너 매니저 등 다양한 영역의 일을 맡아서 했습니다.\n\n하지만 이제 회사가 커지고 시스템이 잘 갖춰지면서 팀별로 역할이 명확히 구분되었습니다. 그러다 보니 예전처럼 여러 영역을 넘나들며 일할 수 없게 되었습니다. 이는 회사 입장에서는 당연한 변화지만, 저는 최근 좀 더 재미있는 일을 찾아야겠다고 생각하게 되었습니다.\n\nFA Engineer 포지션은 한국에 고객이 있고 고객과의 관계가 좋기 때문에 BD팀이신 상무님과 함께 더 큰 그림을 그려나갈 수 있는 장점이 있습니다. 반면  Customer Servie Engineer는 아직 해외 고객이 없고 대부분의 프로젝트가 주로 POC 단계에 있어, 불모지에서 고객과 처음부터 관계를 구축해야 하는 새로운 도전이 기다리고 있습니다.\n\n결국 제가 가장 재미를 느끼는 지점은 어려운 상황에서 새로운 길을 개척하는 것입니다. Sonatus에 처음 합류했을 때처럼 인력도 부족하고 회사의 성공을 누구도 예측할 수 없던 시기에 도전하는 것이 저에게는 가장 흥미롭고 보람찬 일입니다.\n\n# 마치며\n\n오늘 생각해보니 제가 지금까지 걸어온 길과 앞으로 가고 싶은 방향이 조금 더 명확해진 것 같습니다. 지사장님과의 대화는 항상 저에게 새로운 통찰을 줍니다. 조직이 성장하면서 개인의 역할도 변화하는 것은 자연스러운 일이지만, 그 안에서도 자신이 진정으로 즐거움을 느끼는 일을 찾아 도전하는 것이 중요하다는 생각이 듭니다.\n\n내일은 이런 생각들을 바탕으로 구체적인 커리어 계획을 세워봐야겠습니다. 변화는 두렵지만, 그 안에서 새로운 기회를 발견하는 것이 저의 강점이니까요.\n\n*기록하고 되돌아보니 많은 생각이 정리됩니다. 꾸준히 기록하는 습관을 이어가야겠습니다.*","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Retrospect","Sonatus"],"pubDate":"2025-02-28T00:00:00.000Z","url":"/blog/deepthinking/daily/2025-02-08-daily/"},{"id":"deepthinking/daily/2025-03-04-blog-renewal","title":"2025-03-04 Claude 3.7 Sonnet과 함께한 블로그 프레임워크 리뉴얼","description":"","content":"# 드디어 해냈다! 블로그 프레임워크 리뉴얼\n\n정말 오랫동안 손을 못 댔던 블로그 프레임워크를 드디어 리뉴얼했다. 이 프레임워크는 사실 2020년에 만들어놓고 계속 조금씩 글만 올리고 있었는데, 점점 문제가 많아지고 있었다.\n\n특히 내가 작성하는 다양한 카테고리의 글들이 중복되는 문제가 있었고, 날짜 형식도 뒤죽박죽이었다. 매번 '언젠가 시간 날 때 고쳐야지...'라고 생각만 하다가 오늘에야 손을 대게 됐다.\n\n그런데 정말 놀라운 일이 일어났다. 불과 **1시간** 만에 모든 문제를 해결했다! 어떻게 가능했냐고? 바로 **Claude 3.7 Sonnet**과 Agent 기술 덕분이다.\n\n## Claude 3.7 Sonnet + Agent의 놀라운 능력\n\n사실 요즘 내가 관심을 두고 있는 AI 모델들 중 하나가 바로 Claude다. 이번에 Anthropic에서 출시한 Claude 3.7 Sonnet은 코딩 능력이 정말 뛰어나다고 평가받고 있어서 한번 테스트해보고 싶었다.\n\n마침 내 블로그에 있는 문제들을 해결하기 좋은 테스트 케이스라고 생각했고, Agent 기능을 활용해 직접 파일을 읽고 수정하도록 해봤다.\n\n처음에는 \"이게 될까?\" 싶었지만... 정말 놀라웠다. Claude는 내 블로그의 구조를 순식간에 파악했고, 문제점들을 하나씩 찾아내 수정안을 제시했다. 더 놀라운 건 직접 코드를 수정하고 Jekyll 빌드까지 해서 결과를 바로 보여준다는 점이었다.\n\n특히 다음 문제들을 금방 해결해냈다:\n\n1. **파일 충돌 문제**: 'Daily' 카테고리의 파일들이 서로 충돌하는 문제가 있었는데, permalink를 추가해 각 파일마다 고유한 URL을 가지도록 했다.\n2. **카테고리 대소문자 통일**: 'deepthinking', 'daily' 등이 소문자로 되어있어 'TechSavvy'와 같은 다른 카테고리와 일관성이 없었는데, 모두 첫 글자를 대문자로 통일했다.\n3. **Docker 기반 환경으로 전환**: 기존에는 로컬에서 Ruby 환경을 설정하는 번거로움이 있었는데, Docker 기반으로 전환해 어디서든 동일한 환경에서 빌드할 수 있게 됐다.\n\n## Docker로 전환한 이유\n\n사실 이전에는 로컬 Ruby 환경을 설정하고 Jekyll을 구동하는 과정이 OS마다 달라서 꽤 번거로웠다. macOS에서는 괜찮았지만, 회사 Windows 노트북에서 작업할 때는 매번 환경 설정하느라 고생했다.\n\nClaude의 제안으로 Docker를 도입하니 이 문제가 완전히 해결됐다. 이제는 어느 환경에서든 `docker-compose up`만 실행하면 바로 블로그가 로컬에서 구동된다. 게다가 빌드 과정도 `docker-compose run build`로 표준화됐다.\n\n가장 마음에 드는 건 이제 Ruby 버전이나 gem 의존성 문제에서 완전히 자유로워졌다는 점이다. 환경 구성에 신경 쓸 필요 없이 글쓰기에만 집중할 수 있게 됐다.\n\n## 1시간 만에 해결한 놀라운 경험\n\n솔직히 말하면, 이 문제를 해결하려고 몇 달 전에 시도했다가 포기한 적이 있다. Jekyll 구조를 다시 파악하고, 카테고리 구조를 재설계하는 건 꽤 시간이 필요한 작업이라고 생각했다.\n\n하지만 Claude와 함께하니 1시간 만에 모든 게 해결됐다. 정말 놀라운 경험이었다.\n\n물론 내가 개발자이기 때문에 Claude가 제안한 변경사항들을 이해하고 확인할 수 있었던 게 도움이 됐다. 하지만 내가 직접 코드를 치지 않고도 블로그 구조를 완전히 재구성했다는 건 정말 혁명적인 경험이었다.\n\n## 앞으로의 계획\n\n이번 리뉴얼을 통해 블로그 구조가 훨씬 깔끔해졌고, 이제 글 작성에만 집중할 수 있게 됐다. 그동안 미뤄뒀던 기술 글들도 앞으로 차근차근 올려볼 생각이다.\n\n특히 Docker와 관련된 글, LLM과 Claude 같은 AI 모델을 활용한 경험들도 공유해볼 예정이다. 내가 직접 경험한 놀라운 생산성 향상을 다른 사람들도 느껴봤으면 한다.\n\n마지막으로, Claude 3.7 Sonnet에게 고마움을 전하고 싶다. 나도 이제 AI 시대에 발맞춰 일해야겠다는 생각이 더욱 강해졌다. AI를 활용하면 정말 놀라운 생산성 향상을 이룰 수 있다는 걸 직접 체험했으니까!\n\n*이제 블로그 글 작성이 더 즐거워질 것 같다*","categories":["DeepThinking","Daily"],"tags":["DeepThinking","GithubPage","Blog","AI","Claude","LLM"],"pubDate":"2025-03-03T15:00:00.000Z","url":"/blog/deepthinking/daily/2025-03-04-blog-renewal/"},{"id":"techsavvy/container/2025-03-05-docker-automotive-embedded-lessons-1","title":"차량용 임베디드 시스템과 Docker: 현장에서 배운 교훈들 (1)","description":"","content":"![Docker in Automotive Embedded Systems](/assets/img/docker.jpeg)\n\n# 차량용 임베디드 시스템과 Docker: 현장에서 배운 교훈들 (1)\n\n안녕하세요! 오늘부터 차량용 임베디드 시스템에 Docker를 이식하면서 경험한 다양한 도전과 해결책을 시리즈로 공유하려고 합니다. 이 시리즈에서는 실제 현장에서 마주친 문제들과 그 해결 과정을 통해 얻은 교훈을 소개할 예정입니다. 이번 글은 시리즈의 첫 번째 발간물로, Docker 컨테이너와 마운트 포인트 간의 미묘한 상호작용에 관한 이야기입니다.\n\n## 🕵️‍♀️ 미스터리한 현상: \"사라진 파일들\"\n\n어느 날 저희 차량용 임베디드 시스템에서 이상한 현상이 발생했습니다. 재부팅 후 Docker 컨테이너를 실행했는데, 이전에 잘 보이던 파일들이 모두 사라져버린 것이었습니다! 컨테이너는 정상적으로 실행되지만, 마운트된 디렉토리의 파일들은 어디론가 사라져버렸습니다. 마치 유령처럼 말이죠.\n\n## 🔍 원인 분석: Docker 실행 시점과 마운트 상태의 중요한 관계\n\n조사 결과, 이 문제는 임베디드 환경에서 **시스템 부팅 시퀀스의 타이밍**과 깊은 관련이 있었습니다.\n\n1. 저희 시스템은 init.d 스크립트를 통해 Docker 데몬을 실행합니다\n2. 시스템에서는 NFS를 활용한 네트워크 스토리지 마운트가 부팅 시점에 이루어집니다\n3. **가장 중요한 발견**: Docker가 실행되는 시점에 따라 컨테이너가 참조하는 마운트 상태가 완전히 달라질 수 있다는 것이었습니다!\n4. Docker 데몬이 먼저 시작되고 나중에 스토리지가 마운트되면, 컨테이너는 빈 디렉토리나 이전 상태를 참조하게 됩니다\n5. 이렇게 되면 재부팅 후 컨테이너는 \"파일이 없다\"고 판단하게 되는 것이죠\n\n단순한 타이밍 문제로 보일 수 있지만, 이는 **시스템 아키텍처 설계의 중요한 문제**였습니다. 특히 차량용 임베디드 환경에서는 이러한 서비스 의존성과 시작 순서가 매우 중요합니다.\n\n## 💡 임시 해결 방법: Docker 실행 전 마운트 확인\n\n문제를 해결하기 위해 docker.init 스크립트에 마운트 확인 로직을 추가했습니다:\n\n```bash\n# Storage volume mount status check\ntrials_mount=5\nwhile [ $trials_mount -gt 0 ]; do\n    dfoutput=$(df -PTh mount_path)\n    echo \"df output: $dfoutput\" &gt;/dev/kmsg\n    echo \"$dfoutput\" | grep -q \"volume_identifier\"\n    if [ $? -eq 0 ]; then\n        echo \"Volume is set up correctly\" &gt;/dev/kmsg\n        break\n    else\n        echo \"Volume is not set up, waiting 1 second\" &gt;/dev/kmsg\n        trials_mount=$((trials_mount - 1))\n        sleep 1\n    fi\ndone\n\nif [ $trials_mount -eq 0 ]; then\n    echo \"Volume is not set up. Cannot start Docker\" &gt;/dev/kmsg\n    exit 5\nfi\n```\n\n이 임시 해결책은 Docker가 시작되기 전에 필요한 마운트가 모두 준비되었는지 확인합니다. 하지만 이는 근본적인 해결책이 아닙니다.\n\n## 🧠 교훈: 시스템 설계 시 서비스 의존성 관리가 핵심입니다!\n\n이번 경험을 통해 얻은 중요한 통찰은 다음과 같습니다:\n\n1. **Docker 실행 시점이 결정적**: Docker가 언제 실행되느냐에 따라 컨테이너가 보는 파일 시스템 상태가 완전히 달라질 수 있습니다\n2. **시스템 부팅 설계의 중요성**: 서비스 시작 순서와 의존성은 임베디드 시스템 설계에서 가장 중요한 요소 중 하나입니다\n3. **이상적인 해결책**: 궁극적으로는 Docker를 제어하는 별도의 관리 서비스가 있어서 모든 시스템 마운트가 완료된 후에만 Docker를 시작하도록 해야 합니다\n\n차량용 임베디드 시스템에서 Docker를 사용할 때는 부팅 시퀀스와 마운트 타이밍에 특별한 주의를 기울여야 합니다. 서비스 간의 의존성을 명확히 하고, 시스템 설계 단계에서부터 이러한 문제를 고려해야 합니다.\n\n여러분도 비슷한 경험이 있으신가요? 댓글로 공유해주세요! 다음 시리즈에서는 차량용 임베디드 환경에서 Docker를 사용할 때 마주치는 또 다른 도전과제를 다룰 예정입니다.\n\n---\n\n*이 글은 실제 차량용 임베디드 시스템에 Docker를 적용하면서 얻은 경험을 바탕으로 작성되었으며, 유사한 환경에서 개발하는 엔지니어들에게 도움이 되길 바랍니다.*","categories":["TechSavvy","Container"],"tags":["Docker","Embedded","Automotive","Linux","Container"],"pubDate":"2025-03-05T00:00:00.000Z","url":"/blog/techsavvy/container/2025-03-05-docker-automotive-embedded-lessons-1/"},{"id":"techsavvy/container/2025-03-14-capabilities","title":"리눅스 capabilities와의 씨름: 빌드 시스템에서 겪은 하루","description":"","content":"![Linux Capabilities](/assets/img/linux_capabilities.jpeg)\n\n# 리눅스 capabilities와의 씨름: 빌드 시스템에서 겪은 하루\n\n안녕하세요! 오늘은 리눅스 capabilities와 관련해서 빌드 시스템에서 마주쳤던 흥미로운 문제와 그 해결 과정을 공유하려고 합니다. \n\n## 🏗️ CM 구조와 OTA 시스템의 이해\n\n우리 시스템의 Container Manager(CM)는 독특한 구조를 가지고 있습니다. CM은 기본적으로 CAP(Container Application Package)을 타겟 디바이스 위에서 빌드해야 하는 구조로 설계되어 있어요. 단순한 구현상의 선택이 아니라, 기존 OTA(Over-The-Air) 업데이트 구조와 Docker의 layered filesystem 개념을 효율적으로 결합하기 위한 전략적인 결정이었습니다.\n\nDocker의 계층화된 파일시스템은 이미지를 여러 레이어로 구성하여 스토리지 효율성과 빌드 시간을 최적화합니다. 이 개념을 OTA 업데이트 시스템과 결합함으로써, 전체 시스템을 다시 배포하지 않고도 특정 컨테이너나 애플리케이션만 업데이트할 수 있게 되었어요. 이런 방식은 제한된 대역폭과 스토리지를 가진 임베디드 시스템에서 특히 중요한 이점을 제공합니다.\n\n하지만 이런 구조는 빌드 시스템에서 새로운 도전과제를 제시합니다. 특히 Linux capabilities와 같은 특수한 권한 설정에 관련해서 말이죠! 🤔\n\n## 🔍 문제의 본질 파헤치기\n\n레시피 코드를 좀 더 자세히 살펴봤습니다:\n\n```bash\n# EIDS 바이너리에 capabilities 설정\ndo_set_capabilities() {\n    local image_name=$(find ${S} -type d -maxdepth 1 -mindepth 1 -exec basename {} \\;)\n    local eids_binary_path=\"${S}/$image_name/usr/opt/EIDS/bin/snt_eids\"\n    \n    if [ -f \"$eids_binary_path\" ]; then\n        bbnote \"[SNT] Setting capabilities on $eids_binary_path\"\n        \n        # 바이너리에 필요한 capabilities 설정\n        if setcap cap_net_admin,cap_net_raw=eip \"$eids_binary_path\"; then\n            # 성공 시 검증 로직\n        else\n            bbwarn \"[SNT] Failed to set capabilities on $eids_binary_path\"\n            bbwarn \"[SNT] This might be expected in environments without CAP_SETFCAP permissions\"\n        fi\n    else\n        bbnote \"[SNT] snt_eids binary not found at $eids_binary_path, skipping capability setting\"\n    fi\n}\n```\n\n아하! 이제 이해가 됩니다. 이 태스크는 EIDS 바이너리에 네트워크 관련 capabilities를 설정하려고 시도하지만, 빌드 환경에서 권한이 없으면 실패할 수 있도록 설계되어 있었네요. 그냥 경고만 출력하고 빌드를 계속 진행하는 것이 의도적인 설계였던 거죠.\n\n하지만 여전히 의문이 남았습니다: \"그럼 이 capabilities는 언제, 어디서 설정되는 걸까?\" 🧐\n\n## 🧪 첫 번째 실험: sudo 사용하기\n\n일단 간단한 실험을 해보기로 했습니다. 터미널에서 직접 sudo를 사용해 capabilities를 설정해보는 것이었죠:\n\n```bash\nsudo setcap cap_net_admin,cap_net_raw=eip ./path/to/snt_eids\n```\n\n신기하게도 이 명령은 아무런 에러 없이 실행되었습니다! 설정이 잘 되었는지 확인해봤어요:\n\n```bash\ngetcap ./path/to/snt_eids\n./path/to/snt_eids = cap_net_admin,cap_net_raw+eip\n```\n\n완벽합니다! 그렇다면 레시피에 sudo를 추가하는 것은 어떨까요? 🤔 그런데 Yocto 빌드 시스템에서 sudo를 사용하는 것이 표준적인 방법은 아닐 것 같다는 느낌이 들었습니다.\n\n## 🕵️ 더 깊은 탐구: update_setup.sh 발견\n\n시스템 코드를 더 탐색하다가 `update_setup.sh`라는 파일을 발견했습니다. 이 파일에는 시스템 업데이트 중에 실행되는 스크립트가 포함되어 있었고, 놀랍게도 여기에는 많은 capabilities 설정 명령이 있었어요:\n\n```bash\n# set capabilities for xuser after update\nsetcap cap_sys_time,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_sys_resource=+eip ${CCU2_BINARY_PATH}/ApSysMgr/bin/LGSystemManager 2&gt;&1 | update_log\nsetcap_socket \"${CCU2_BINARY_PATH}/BSM/bin/LGBootSyncManager\"\n# ... 더 많은 setcap 명령들 ...\n```\n\n이 스크립트는 시스템이 업데이트될 때 실행되며, 적절한 권한으로 capabilities를 설정하는 것 같았습니다. 그리고 이 스크립트는 특별한 SMACK 레이블 검사를 통해 올바른 권한을 가진 상태에서만 실행되도록 설계되어 있었어요.\n\n```bash\nfunction check_label {\n    shell_label=$(chsmack \"/bin/busybox_smack\")\n    update_label=$(cat /proc/$$/attr/current)\n    # ... SMACK 레이블 확인 로직 ...\n    if [[ \"${shell_label}\" =~ \"Privileged\" ]]; then\n        if [[ \"${update_label}\" =~ \"Privileged\" ]]; then\n            echo \"[SMACK][XUSER] Start Update!\" | update_log\n        else\n            # ... 권한 없을 때의 로직 ...\n        fi\n    fi\n}\n```\n\n이제 그림이 맞춰지기 시작했습니다! 빌드 시에는 capabilities 설정이 실패해도 괜찮고, 실제 중요한 설정은 시스템 업데이트 중에 이루어지는 것이었습니다. 😮\n\n## 🔧 세 번째 방법: CAP_SETFCAP 권한 부여하기\n\n그래도 빌드 환경에서 capabilities를 설정할 수 있다면 좋을 것 같아서, 또 다른 접근법을 시도해봤습니다. setcap 바이너리 자체에 CAP_SETFCAP 권한을 부여하는 것이었죠:\n\n```bash\nsudo setcap cap_setfcap=+ep /usr/sbin/setcap\n```\n\n이후 권한이 제대로 설정되었는지 확인했습니다:\n\n```bash\ngetcap /usr/sbin/setcap\n/usr/sbin/setcap = cap_setfcap+ep\n```\n\n이제 일반 사용자로도 setcap 명령을 사용할 수 있게 되었습니다:\n\n```bash\nsetcap cap_net_admin,cap_net_raw=eip ./path/to/snt_eids\n```\n\n성공입니다! 🎉 이 방법을 사용하면 sudo 없이도, 빌드 환경에서 capabilities를 설정할 수 있게 되었습니다.\n\n## 🤔 그런데, 실제 상황은 더 복잡했습니다\n\n더 깊이 파고들수록, 상황이 더 복잡하다는 것을 알게 되었습니다. 우리 시스템에서는 `cm_create_cap.py`라는 스크립트를 사용해 바이너리를 압축하고 패키징한 후 타겟에 전달하고 있었어요. 게다가 일부 패키지는 향후 암호화될 예정이었고, 이렇게 되면 타겟에서 압축을 풀고 다시 capabilities를 설정하는 것이 불가능해질 수 있었죠.\n\n특히 factory 패키지는 암호화되지 않지만, test 패키지는 암호화될 예정이었기 때문에 일원화된 로직을 구현하기 어려웠습니다. 😵‍💫\n\n## 📚 Yocto의 권장 방식은 무엇일까?\n\nYocto 문서를 찾아보면서, 일반적으로 권장되는 접근법들도 살펴봤습니다:\n\n1. **postinst 스크립트 사용**: 패키지가 설치된 후 타겟에서 capabilities 설정\n2. **init 스크립트 사용**: 시스템 부팅 중에 capabilities 설정\n3. **빌드 시점 설정 우회**: 빌드 환경과 타겟 환경의 차이를 인정하고 타겟에서만 설정\n\n그런데 우리의 경우처럼 암호화된 패키지를 사용하는 경우, 이러한 표준 접근법이 항상 적용 가능한 것은 아니었습니다.\n\n## 💡 결론: 때로는 기존 방식이 최선입니다\n\n이런 복잡한 상황을 고려했을 때, 결국 가장 안정적인 접근법은 기존에 검증된 방식을 유지하는 것이었습니다. 우리의 경우에는 Dockerfile에서 컨테이너 이미지 빌드 시점에 capabilities를 설정하는 방식이었죠.\n\n비록 빌드 시스템에서 capabilities를 설정하는 다양한 방법을 배웠지만, 때로는 모든 제약사항과 요구사항을 고려했을 때 기존의 방식이 가장 견고한 해결책일 수 있다는 교훈을 얻었습니다.\n\n이 과정은 리눅스 보안 메커니즘의 복잡성, 빌드 환경과 런타임 환경의 차이, 그리고 실제 제품 개발에서 마주하는 복잡한 요구사항들을 이해하는 좋은 기회였습니다. 간단해 보이는 문제도 깊이 파고들면 다양한 층위의 복잡성이 있다는 것을 다시 한번 깨닫게 되었어요! 🧠\n\n빌드 시스템과 보안 메커니즘의 씨름은 오늘도 계속됩니다. 하지만 이제 리눅스 capabilities에 대해 조금 더 깊이 이해하게 되었으니, 다음 문제는 좀 더 쉽게 해결할 수 있겠죠? 😉\n\n---\n\n*이 글은 실제 빌드 시스템에서 리눅스 capabilities를 다루면서 겪은 경험을 바탕으로 작성되었으며, 유사한 환경에서 개발하는 엔지니어들에게 도움이 되길 바랍니다.*","categories":["TechSavvy","Container"],"tags":["Linux","Container","Capabilities","Yocto","Docker","Security"],"pubDate":"2025-03-14T00:00:00.000Z","url":"/blog/techsavvy/container/2025-03-14-capabilities/"},{"id":"techsavvy/container/2025-06-25-claude-code-installation-guide","title":"기존 Linux Container에서 Claude Code 설치 가이드 (실제 검증됨)","description":"","content":"![Claude Code Installation Guide](/assets/img/claude.jpeg)\n\n# 기존 Linux Container에서 Claude Code 설치 가이드 (실제 검증됨)\n\n## 목차\n1. [Claude Code 개요](#claude-code-개요)\n2. [실행 중인 컨테이너에 접속](#실행-중인-컨테이너에-접속)\n3. [시스템 환경 확인](#시스템-환경-확인)\n4. [Node.js 설치 (NVM 권장)](#nodejs-설치-nvm-권장)\n5. [Claude Code 설치](#claude-code-설치)\n6. [인증 설정](#인증-설정)\n7. [사용법](#사용법)\n8. [문제 해결](#문제-해결)\n9. [검증된 설치 스크립트](#검증된-설치-스크립트)\n\n## Claude Code 개요\n\nClaude Code는 Anthropic에서 개발한 터미널 기반 AI 코딩 도구로, 다음과 같은 기능을 제공한다:\n- 코드베이스 전반의 파일 편집 및 버그 수정\n- 코드 아키텍처와 로직에 대한 질문 응답\n- 테스트, 린팅 및 기타 명령 실행 및 수정\n- Git 워크플로우 관리 (병합 충돌 해결, PR 생성 등)\n- 웹 검색을 통한 문서 및 리소스 탐색\n\n**시스템 요구사항**: Node.js 18+ (권장: 20+ LTS)\n\n## 실행 중인 컨테이너에 접속\n\n### 1. 컨테이너 목록 확인\n```bash\n# 실행 중인 컨테이너 확인\ndocker ps\n\n# 또는 모든 컨테이너 확인 (중지된 것 포함)\ndocker ps -a\n```\n\n### 2. 컨테이너에 접속\n```bash\n# 컨테이너 이름으로 접속\ndocker exec -it &lt;컨테이너_이름&gt; /bin/bash\n\n# 또는 컨테이너 ID로 접속\ndocker exec -it &lt;컨테이너_ID> /bin/bash\n\n# bash가 없는 경우 sh 사용\ndocker exec -it &lt;컨테이너_이름&gt; /bin/sh\n```\n\n### 3. root 권한으로 접속 (필요시)\n```bash\n# root 사용자로 접속\ndocker exec -it -u root &lt;컨테이너_이름&gt; /bin/bash\n```\n\n## 시스템 환경 확인\n\n### 1. 운영체제 및 권한 확인\n```bash\n# OS 정보 확인\ncat /etc/os-release\n\n# 현재 사용자 및 권한 확인\nwhoami && id\n\n# sudo 권한 확인\nsudo -l 2&gt;/dev/null || echo \"sudo not available\"\n```\n\n### 2. 기존 Node.js 확인\n```bash\n# 현재 Node.js 확인\nnode --version 2&gt;/dev/null || echo \"Node.js not installed\"\nnpm --version 2&gt;/dev/null || echo \"npm not installed\"\n```\n\n## Node.js 설치 (NVM 권장)\n\n⚠️ **중요**: Ubuntu 기본 저장소의 Node.js는 버전이 너무 낮다 (v10.x). NVM 사용을 강력히 권장한다.\n\n### 방법 1: NVM 사용 (추천 - 검증됨)\n\n```bash\n# 1. NVM 설치\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\n\n# 2. NVM 환경 변수 로드\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\n\n# 3. 최신 LTS Node.js 설치\nnvm install --lts\nnvm use --lts\n\n# 4. 설치 확인\nnode --version  # v22.x.x (또는 최신 LTS)\nnpm --version   # v10.x.x\n\n# 5. 기본값으로 설정\nnvm alias default lts/*\n```\n\n### 방법 2: NodeSource 저장소 (문제 발생 가능)\n\n⚠️ **주의**: 실제 테스트에서 컨테이너 환경에서 문제가 발생할 수 있다.\n\n```bash\n# Ubuntu/Debian 계열\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo bash -\nsudo apt install -y nodejs\n\n# 버전 확인\nnode --version\nnpm --version\n```\n\n### npm 설정 충돌 해결\n\nNVM 사용 시 기존 npm 설정과 충돌이 발생할 수 있다:\n\n```bash\n# 기존 npm 설정 파일 제거\nrm -f ~/.npmrc\n\n# NVM 재로드\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\nnvm use --lts\n```\n\n## Claude Code 설치\n\n### 1. 환경 설정 (NVM 사용자)\n\n```bash\n# NVM 환경 로드 (매번 터미널 시작 시 필요)\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\n```\n\n### 2. Claude Code 설치\n\n```bash\n# Claude Code 전역 설치\nnpm install -g @anthropic-ai/claude-code\n\n# 설치 확인\nclaude --version  # 1.0.34 (또는 최신 버전)\n```\n\n### 3. 영구 환경 설정\n\n```bash\n# bashrc에 NVM 설정 추가 (영구 설정)\ncat &gt;> ~/.bashrc &lt;< 'EOF'\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\nEOF\n\n# 현재 세션에 적용\nsource ~/.bashrc\n```\n\n### 4. 선택적 도구 설치\n\n```bash\n# Git (이미 설치되어 있을 가능성 높음)\nsudo apt install -y git\n\n# Ripgrep (검색 성능 향상 - 이미 설치되어 있을 수 있음)\nsudo apt install -y ripgrep\n\n# 설치 확인\ngit --version\nrg --version\n```\n\n## 인증 설정\n\n### 방법 1: OAuth 인증 (권장 - Claude Max 사용자)\n\n```bash\n# Claude Code 첫 실행\nclaude\n\n# 프로세스:\n# 1. 터미널 스타일 선택\n# 2. Enter 키를 눌러 브라우저에서 로그인\n# 3. Anthropic 계정으로 로그인\n# 4. 인증 완료 후 터미널로 돌아옴\n```\n\n### 방법 2: API 키 사용\n\n```bash\n# API 키 환경변수 설정\nexport ANTHROPIC_API_KEY=\"your_api_key_here\"\n\n# 영구 설정\necho 'export ANTHROPIC_API_KEY=\"your_api_key_here\"' &gt;> ~/.bashrc\nsource ~/.bashrc\n```\n\n### 컨테이너 환경에서 브라우저 접근이 어려운 경우\n\n```bash\n# 방법 1: API 키 방식 사용 (위 참조)\n\n# 방법 2: 호스트에서 인증 후 설정 파일 복사\n# 호스트에서 claude 실행하여 인증 완료 후:\n# docker cp ~/.claude/ &lt;컨테이너_이름&gt;:/home/&lt;사용자&gt;/\n```\n\n## 사용법\n\n### 1. 기본 실행\n\n```bash\n# 프로젝트 디렉토리로 이동\ncd /path/to/your/project\n\n# Claude Code 시작\nclaude\n\n# 도움말\nclaude --help\n```\n\n### 2. 일반적인 사용 예시\n\n```bash\n# 프로젝트 분석\nclaude \"이 프로젝트의 구조와 주요 기능을 설명해줘\"\n\n# 코드 작성\nclaude \"Python Flask API 엔드포인트를 만들어줘\"\n\n# 버그 수정\nclaude \"main.py 파일을 검토하고 버그를 찾아서 수정해줘\"\n\n# 테스트 실행\nclaude \"테스트를 실행하고 실패한 부분이 있으면 수정해줘\"\n\n# 대화 계속\nclaude --continue\n\n# 특정 대화 재개\nclaude --resume\n```\n\n### 3. 고급 옵션\n\n```bash\n# 권한 확인 무시 (샌드박스 환경에서만)\nclaude --dangerously-skip-permissions\n\n# 특정 모델 사용\nclaude --model sonnet\n\n# 디버그 모드\nclaude --debug\n\n# 출력만 확인 (비대화형)\nclaude --print \"코드 리뷰해줘\"\n```\n\n## 문제 해결\n\n### 1. \"claude: command not found\" 오류\n\n```bash\n# NVM 환경 재로드\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n\n# 또는 bashrc 재로드\nsource ~/.bashrc\n\n# Claude 명령어 위치 확인\nwhich claude\nnpm list -g --depth=0 | grep claude\n```\n\n### 2. Node.js 버전 문제\n\n```bash\n# 현재 버전 확인\nnode --version\n\n# 18 미만인 경우 업그레이드 필요\nif [[ $(node --version | cut -d'v' -f2 | cut -d'.' -f1) -lt 18 ]]; then\n    echo \"Node.js 18+ 버전이 필요하다\"\n    nvm install --lts\n    nvm use --lts\nfi\n```\n\n### 3. npm 권한 오류\n\n```bash\n# npm 설정 초기화\nrm -f ~/.npmrc\n\n# NVM 재설정\nnvm uninstall node\nnvm install --lts\nnvm use --lts\n\n# Claude Code 재설치\nnpm install -g @anthropic-ai/claude-code\n```\n\n### 4. 시스템 패키지 오류 (tzdata 등)\n\n컨테이너 환경에서 시스템 패키지 설정 오류는 일반적으로 Claude Code 기능에 영향을 주지 않는다. 다음과 같은 오류는 무시해도 된다:\n\n```\ntzdata failed to preconfigure\n/etc/timezone: Read-only file system\n```\n\n### 5. 네트워크 연결 문제\n\n```bash\n# 인터넷 연결 확인\nping -c 3 8.8.8.8\n\n# npm 레지스트리 연결 확인\nnpm ping\n\n# 프록시 설정 (필요시)\nnpm config set proxy http://proxy.company.com:8080\nnpm config set https-proxy http://proxy.company.com:8080\n```\n\n## 검증된 설치 스크립트\n\n다음은 실제 테스트를 통해 검증된 원스텝 설치 스크립트다:\n\n```bash\n#!/bin/bash\n# Claude Code 자동 설치 스크립트 (검증됨)\n\nset -e\n\necho \"=== Claude Code 설치 시작 ===\"\n\n# 1. 시스템 정보 확인\necho \"📋 시스템 환경 확인...\"\ncat /etc/os-release | head -3\necho \"사용자: $(whoami)\"\n\n# 2. 기존 npm 설정 정리\necho \"🧹 기존 npm 설정 정리...\"\nrm -f ~/.npmrc 2&gt;/dev/null || true\n\n# 3. NVM 설치\necho \"📦 NVM 설치...\"\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash\n\n# 4. NVM 환경 로드\necho \"🔧 NVM 환경 설정...\"\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\n\n# 5. Node.js LTS 설치\necho \"🚀 Node.js 최신 LTS 설치...\"\nnvm install --lts\nnvm use --lts\nnvm alias default lts/*\n\n# 6. 버전 확인\necho \"✅ 설치된 버전 확인...\"\necho \"Node.js: $(node --version)\"\necho \"npm: $(npm --version)\"\n\n# 7. Claude Code 설치\necho \"🤖 Claude Code 설치...\"\nnpm install -g @anthropic-ai/claude-code\n\n# 8. bashrc에 영구 설정 추가\necho \"⚙️  영구 환경 설정...\"\ncat &gt;> ~/.bashrc &lt;< 'EOF'\n\n# NVM 설정 (Claude Code용)\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\nEOF\n\n# 9. 설치 확인\necho \"🎉 설치 완료 확인...\"\nclaude --version\n\necho \"\"\necho \"=== 설치 완료! ===\"\necho \"Claude Code 버전: $(claude --version)\"\necho \"\"\necho \"다음 단계:\"\necho \"1. 새 터미널을 열거나 'source ~/.bashrc' 실행\"\necho \"2. 'claude' 명령어로 실행\"\necho \"3. 첫 실행 시 인증 과정 진행\"\necho \"\"\necho \"사용법:\"\necho \"  claude                    # 대화형 모드 시작\"\necho \"  claude --help            # 도움말\"\necho \"  claude \\\"질문내용\\\"        # 직접 질문\"\necho \"\"\n```\n\n### 스크립트 사용법\n\n```bash\n# 스크립트 파일 생성\ncat &gt; install_claude_code.sh &lt;< 'EOF'\n[위의 스크립트 내용]\nEOF\n\n# 실행 권한 부여\nchmod +x install_claude_code.sh\n\n# 실행\n./install_claude_code.sh\n```\n\n## 컨테이너 영구화\n\n현재 컨테이너의 변경사항을 새 이미지로 저장한다:\n\n```bash\n# 호스트에서 실행 (새 터미널)\ndocker commit &lt;컨테이너_이름&gt; my-claude-code-image:latest\n\n# 새 이미지로 컨테이너 실행\ndocker run -it --name claude-env \\\n  -v $(pwd):/workspace \\\n  -w /workspace \\\n  my-claude-code-image:latest\n```\n\n## 추가 팁\n\n1. **성능**: ripgrep이 설치되어 있어 빠른 코드 검색이 가능하다\n2. **권한**: 컨테이너 환경에서는 `--dangerously-skip-permissions` 옵션 사용을 고려한다\n3. **모델**: `--model sonnet` 또는 `--model opus`로 특정 모델 사용이 가능하다\n4. **대화 관리**: `--continue`로 이전 대화를 이어갈 수 있고, `--resume`으로 특정 세션을 재개할 수 있다\n5. **디버깅**: 문제 발생 시 `--debug` 옵션으로 상세 정보를 확인할 수 있다\n\n이 가이드는 실제 Ubuntu 20.04 컨테이너 환경에서 테스트하여 검증되었다.\n\n---\n\n*이 글은 실제 컨테이너 환경에서 Claude Code를 설치하면서 얻은 경험을 바탕으로 작성되었으며, 다양한 Linux 배포판과 컨테이너 환경에서 테스트되었다.*","categories":["TechSavvy","Container"],"tags":["Claude","AI","Docker","Linux","Container","Node.js","Installation"],"pubDate":"2025-06-25T00:00:00.000Z","url":"/blog/techsavvy/container/2025-06-25-claude-code-installation-guide/"},{"id":"techsavvy/container/2025-06-25-devcontainer-mcp-bootstrapper","title":"개발환경 부트스트래핑 자동화: DevContainer MCP Bootstrapper 만들기","description":"","content":"![DevContainer MCP Bootstrapper](/assets/img/docker.jpeg)\n\n# 개발환경 부트스트래핑 자동화: DevContainer MCP Bootstrapper 만들기\n\n안녕하세요! 오늘은 최근에 만든 [DevContainer MCP Bootstrapper](https://github.com/jayleekr/devcontainer-mcp-bootstrapper)에 대해서 이야기해보려고 한다. \n\nClaude MCP(Model Context Protocol) 서버들을 자동으로 설치하고 개발 도구들을 쭉 설정해주는 부트스트래퍼를 만들어봤는데, 개발 과정에서 꽤 재밌는 이야기들이 있어서 공유하고 싶다.\n\n## 🤔 왜 만들게 되었나?\n\n최근에 Claude Code 설치 가이드를 작성하면서 느낀 점이 있었다. 컨테이너 환경에서 매번 개발 환경을 설정하는 게 너무 번거롭다는 것이었다.\n\n특히 다음과 같은 상황들이 반복되고 있었다:\n\n1. **새로운 DevContainer 생성할 때마다**: Git 설정, 셸 alias, Vim 설정 등을 매번 다시 해야 함\n2. **Claude MCP 설정**: Context7과 Supermemory MCP 서버를 매번 수동으로 설치하고 설정\n3. **개발 도구들**: Docker alias, 유용한 함수들을 계속 다시 설정\n\n이런 반복 작업이 귀찮아서 \"아예 원스텝으로 모든 걸 자동화해버리자!\"라고 생각했다.\n\n## 🚀 뭘 자동화했나?\n\n### 핵심 기능들\n\n**1. Claude MCP 서버 자동 설치**\n- Context7: 최신 문서와 코드 예제 검색\n- Supermemory: AI 도구 간 개인 메모리 관리\n\n**2. 개발 도구 설정**\n- Git 글로벌 설정과 유용한 alias들\n- 생산성을 높이는 셸 alias와 함수들\n- Vim 기본 설정\n- Docker 관리용 단축 명령어들\n\n**3. 환경 감지와 적응**\n- Docker, DevContainer, Codespaces 등 자동 감지\n- 패키지 매니저 자동 선택 (apt, yum, brew 등)\n- 권한 상황에 맞는 설치 방식 선택\n\n## 🔧 설계 철학: 모듈화와 선택적 설치\n\n처음에는 그냥 \"모든 걸 다 설치하자!\"였는데, 개발하다 보니 사용자마다 필요한 게 다르다는 걸 깨달았다.\n\n```bash\n# 모든 컴포넌트 설치 (기본)\ncurl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash\n\n# 특정 컴포넌트만 설치\ncurl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash -s -- --only claude,git,shell\n```\n\n이런 식으로 모듈화해서 원하는 것만 설치할 수 있게 만들었다.\n\n### 컴포넌트별 구성\n\n| 컴포넌트 | 기능 | 왜 필요한가? |\n|----------|------|-------------|\n| `claude` | Claude MCP 서버들 | AI 개발 워크플로우 통합 |\n| `git` | Git 설정과 alias | 버전 관리 효율성 |\n| `shell` | 셸 alias와 함수 | 터미널 생산성 |\n| `vim` | Vim 기본 설정 | 에디터 환경 |\n| `docker` | Docker alias | 컨테이너 관리 편의성 |\n\n## 🕵️ 환경 감지의 복잡함\n\n가장 까다로웠던 부분은 **환경 감지**였다. \n\nDocker 컨테이너인지, GitHub Codespaces인지, WSL인지, 일반 Linux인지를 정확히 판단해야 했고, 각각에 맞는 설치 방식을 써야 했다.\n\n```bash\n# 환경 감지 로직의 일부\ndetect_environment() {\n    if [ -f /.dockerenv ]; then\n        echo \"docker\"\n    elif [ -n \"$CODESPACES\" ]; then\n        echo \"codespaces\"\n    elif grep -q Microsoft /proc/version 2&gt;/dev/null; then\n        echo \"wsl\"\n    else\n        echo \"native\"\n    fi\n}\n```\n\n특히 권한 관리가 까다로웠다. 일부 환경에서는 sudo가 없고, 일부는 root로 실행되고, 또 일부는 사용자 권한만 있는 상황이었다.\n\n```bash\n# 권한에 맞는 패키지 설치\ninstall_package() {\n    local package=$1\n    if command -v sudo &gt;/dev/null 2&gt;&1 && [ \"$EUID\" -ne 0 ]; then\n        sudo $PKG_MANAGER install -y \"$package\"\n    elif [ \"$EUID\" -eq 0 ]; then\n        $PKG_MANAGER install -y \"$package\"\n    else\n        echo \"⚠️  권한 부족: $package 설치를 건너뜀\"\n    fi\n}\n```\n\n## 💡 MCP 설정의 자동화\n\nClaude MCP 설정이 생각보다 복잡했다. 특히 설정 파일 구조가 JSON이라서 기존 설정을 망가뜨리지 않으면서 새로운 서버를 추가하는 게 까다로웠다.\n\n```bash\n# MCP 설정 파일 백업 및 업데이트\nupdate_claude_config() {\n    local config_file=\"$HOME/.config/Claude/claude_desktop_config.json\"\n    \n    # 기존 설정 백업\n    if [ -f \"$config_file\" ]; then\n        cp \"$config_file\" \"$config_file.backup.$(date +%Y%m%d_%H%M%S)\"\n    fi\n    \n    # 새 설정 생성\n    create_mcp_config \"$config_file\"\n}\n```\n\n그리고 Supermemory MCP의 경우 개인별 고유 URL이 필요해서, 환경변수로 받을 수 있게 만들었다:\n\n```bash\n# 개인 Supermemory URL 설정\nexport SUPERMEMORY_MCP_URL=\"https://mcp.supermemory.ai/YOUR_UNIQUE_ID/sse\"\n```\n\n## 🎯 사용자 경험 개선\n\n처음에는 단순히 \"설치만 되면 끝\"이라고 생각했는데, 실제로 써보니 사용자가 뭐가 설치되고 있는지, 어떻게 사용하는지 알기 어려웠다.\n\n그래서 다음과 같은 기능들을 추가했다:\n\n**1. 상세한 진행상황 표시**\n```bash\necho \"📦 NVM 설치...\"\necho \"🔧 NVM 환경 설정...\"\necho \"🚀 Node.js 최신 LTS 설치...\"\necho \"✅ 설치된 버전 확인...\"\n```\n\n**2. 설치 후 가이드**\n```bash\necho \"=== 설치 완료! ===\"\necho \"Claude Code 버전: $(claude --version)\"\necho \"\"\necho \"다음 단계:\"\necho \"1. 새 터미널을 열거나 'source ~/.bashrc' 실행\"\necho \"2. 'claude' 명령어로 실행\"\necho \"3. 첫 실행 시 인증 과정 진행\"\n```\n\n**3. 편리한 관리 명령어들**\n```bash\nmcp_status              # MCP 설정 상태 확인\nbootstrap_update        # 부트스트래퍼 업데이트\ndev_info               # 개발 환경 정보 표시\n```\n\n## 🐛 예상치 못한 문제들\n\n### 1. 패키지 매니저의 다양성\n\nLinux 배포판마다 패키지 매니저가 다르다는 건 알고 있었지만, 실제로 지원하려니 정말 다양했다:\n\n- Debian/Ubuntu: `apt`\n- CentOS/RHEL/Fedora: `yum`/`dnf`\n- Arch Linux: `pacman`\n- openSUSE: `zypper`\n- Alpine: `apk`\n- macOS/Linux: `brew`\n\n각각의 문법도 미묘하게 달라서 통합 인터페이스를 만드는 게 생각보다 복잡했다.\n\n### 2. 컨테이너별 특이사항들\n\n**Google Colab**: 패키지 설치는 되는데 영구 저장이 안 됨\n**Replit**: Node.js가 이미 설치되어 있지만 권한 문제 있음\n**GitHub Codespaces**: 사용자 설정이 특별한 위치에 저장됨\n\n이런 환경별 특이사항들을 다 고려해야 했다.\n\n### 3. 네트워크 문제\n\n기업 환경에서는 프록시 설정 때문에 외부 스크립트 다운로드가 안 되는 경우가 많았다. 그래서 오프라인 모드도 지원하게 되었다:\n\n```bash\n# 오프라인 모드\n./bootstrap.sh --offline\n```\n\n## 🔄 업데이트와 유지보수\n\n부트스트래퍼를 만들고 나니 \"이걸 어떻게 업데이트하지?\"라는 고민이 생겼다.\n\n그래서 자체 업데이트 기능을 추가했다:\n\n```bash\nbootstrap_update() {\n    echo \"🔄 부트스트래퍼 업데이트 중...\"\n    curl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh -o /tmp/bootstrap_update.sh\n    chmod +x /tmp/bootstrap_update.sh\n    /tmp/bootstrap_update.sh --update\n}\n```\n\n그리고 설정 백업 기능도 넣어서 업데이트할 때 기존 설정이 날아가지 않게 했다.\n\n## 📊 실제 사용해본 결과\n\n몇 주 동안 실제로 써보니 정말 편했다. \n\n**이전**: 새 환경 설정에 30분 이상\n**현재**: 원라이너로 5분 안에 완료\n\n```bash\n# 정말 이거 한 줄이면 끝\ncurl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash\n```\n\n특히 DevContainer나 Codespaces에서 새 프로젝트 시작할 때 `postCreateCommand`에 넣어두면 자동으로 모든 게 설정되니까 정말 편하다.\n\n### DevContainer 통합 예시\n\n```json\n{\n  \"name\": \"My Development Environment\",\n  \"image\": \"mcr.microsoft.com/devcontainers/base:ubuntu\",\n  \"postCreateCommand\": \"curl -fsSL https://raw.githubusercontent.com/jayleekr/devcontainer-mcp-bootstrapper/main/bootstrap.sh | bash\",\n  \"customizations\": {\n    \"vscode\": {\n      \"extensions\": [\n        \"ms-vscode.claude-dev\"\n      ]\n    }\n  }\n}\n```\n\n## 🎨 앞으로의 계획\n\n현재 기본적인 기능들은 다 구현했지만, 앞으로 추가하고 싶은 것들이 있다:\n\n1. **더 많은 MCP 서버 지원**: 앞으로 나올 새로운 MCP 서버들 자동 지원\n2. **개인화 설정**: 사용자별 프리셋 저장 기능\n3. **GUI 도구들**: 개발에 유용한 GUI 도구들 자동 설치\n4. **성능 최적화**: 병렬 설치로 설치 시간 단축\n\n## 💭 회고: 자동화의 가치\n\n이번 프로젝트를 하면서 느낀 건, **반복되는 작업은 무조건 자동화해야 한다**는 것이다.\n\n처음에는 \"스크립트 만드는 시간에 그냥 수동으로 하는 게 빠르지 않을까?\"라고 생각했는데, 막상 만들고 나니 시간 절약은 물론이고 **실수도 줄고**, **일관성도 보장**되고, **다른 사람들도 쉽게 사용**할 수 있게 되었다.\n\n특히 개발 환경 설정 같은 경우는 한 번 제대로 자동화해두면 계속 써먹을 수 있어서 투자 대비 효과가 크다.\n\n## 🤝 커뮤니티와 기여\n\n이 부트스트래퍼는 개인적인 필요에서 시작했지만, 비슷한 문제를 겪는 개발자들이 많을 것 같다. 그래서 MIT 라이선스로 오픈소스화했다.\n\n혹시 사용해보시고 개선점이나 버그를 발견하시면 언제든지 이슈나 PR을 올려주세요! \n\n특히 다음과 같은 기여를 환영한다:\n- 새로운 환경 지원 (새로운 컨테이너 플랫폼, OS 등)\n- 추가 MCP 서버 통합\n- 설치 스크립트 최적화\n- 문서 개선\n\n## 🔚 마무리\n\nDevContainer MCP Bootstrapper를 만들면서 환경 감지, 패키지 관리, 사용자 경험 등 많은 것들을 배웠다. \n\n무엇보다도 **자동화의 힘**을 다시 한번 실감했다. 반복적인 작업을 자동화하는 것은 단순히 시간을 절약하는 것뿐만 아니라, 실수를 줄이고 일관성을 보장하며 다른 사람들과 쉽게 공유할 수 있게 해준다.\n\n앞으로도 개발 워크플로우에서 반복되는 부분들을 찾아서 자동화해나갈 계획이다. 여러분도 반복 작업이 있으면 자동화를 고려해보시길!\n\n---\n\n*이 글은 실제 DevContainer MCP Bootstrapper 개발 과정에서 겪은 경험을 바탕으로 작성되었으며, 다양한 개발 환경에서 테스트되었다.*\n\n**저장소**: [https://github.com/jayleekr/devcontainer-mcp-bootstrapper](https://github.com/jayleekr/devcontainer-mcp-bootstrapper)","categories":["TechSavvy","Container"],"tags":["DevContainer","MCP","Docker","Claude","AI","Bootstrap","Automation","Development"],"pubDate":"2025-06-25T01:00:00.000Z","url":"/blog/techsavvy/container/2025-06-25-devcontainer-mcp-bootstrapper/"},{"id":"techsavvy/computerarchitecture/2021-04-10-arm64","title":"ARM64","description":"","content":"## 정의\n\n### ARM이 64bit을 도입하며 한 정의\n\n- **AArch32** – Thumb 모드 실행을 포함한 ARM에서 정의된 레거시 32비트 명령 집합 아키텍처(ISA)입니다.\n- **AArch64** – ARM에서 정의된 새 64비트 ISA(명령 집합 아키텍처)입니다.\n- **ARMv7** - AArch32에 대한 지원도 포함하는 “7세대” ARM 하드웨어의 사양입니다. 이 버전의 ARM 하드웨어는 ARM용 Windows가 지원되는 첫 버전입니다.\n- **ARMv8** - AArch32 및 AArch64 모두에 대한 지원을 포함하는 “8세대” ARM 하드웨어의 사양입니다.\n\n### Windows 에서의 정의\n\n- **ARM** – AArch32(32비트 ARM 아키텍처)를 나타내며 WoA(Windows on ARM)라고도 합니다.\n- **ARM32** – 위의 ARM과 동일하며, 명확성을 위해 이 문서에서 사용됩니다.\n- **ARM64** – 64비트 ARM 아키텍처(AArch64)를 말합니다. WoA64는 없습니다\n\n### ARM의 데이터 형식\n\n- **Short-Vector** – 벡터에서 8바이트 또는 16바이트 분량의 요소로 직접 표현할 수 있는 데이터 형식입니다. 크기는 8바이트 또는 16바이트에 맞춰져 있으며 각 요소는 1, 2, 4 또는 8바이트가 될 수 있습니다.\n- **HFA(동일 부동 소수점 집계)** – 2개~4개의 동일한 부동 소수점 멤버(floats 또는 doubles)를 포함하는 데이터 형식입니다.\n- **HVA(동일 Short-Vector 집계)** – 2개~4개의 동일한 Short-Vector 멤버가 있는 데이터 형식입니다.\n\n## Reference\n\n[1] ARM프로세서에 대한 C++프로젝트 구성의 ARM64 ABI 규칙개요 , Microsost, [https://docs.microsoft.com/ko-kr/cpp/build/arm64-windows-abi-conventions?view=msvc-160](https://docs.microsoft.com/ko-kr/cpp/build/arm64-windows-abi-conventions?view=msvc-160)","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-09T15:00:00.000Z","url":"/blog/techsavvy/computerarchitecture/2021-04-10-arm64/"},{"id":"techsavvy/computerarchitecture/2021-04-11-abistandards","title":"ABI Standards","description":"","content":"## **ABI(Application Binary Interface) 표준**\n\nApplication간 binary 데이터를 어떻게 교환해야 하는지 다음과 같은 규칙들을 정한다.\n\n- 데이터 타입과 정렬 방법\n- 함수 호출 시 인수 및 결과에 대해 레지스터 교환 방법\n- 시스템 콜 호출 방법\n- 프로그램 코드의 시작과 데이터에 대한 초기화 방법\n- 파일 교환 방법(ELF 등)\n\n### **EABI 표준**\n\nEABI(Embeded ABI)는 임베디드 환경의 ABI를 다룬다. ARM 아키텍처에서 리눅스 버전에 따라 ABI를 사용하는 방식이 다음 두 가지로 나뉜다.\n\n- arm/OABI\n    - 커널 v2.6.15 (mainline v2.6.16) 이전에 사용되던 ABI 방식(Old ABI 또는 legacy ABI라고도 불린다)\n    - glibc 2.3.6 까지 사용\n    - gcc: linux-arm-none-gnu\n- arm/EABI\n    - 커널 v2.6.16 부터 사용되는 ARM EABI 방식\n    - glibc v2.3.7 및 v2.4 부터 사용\n    - gcc: linux-arm-none-gnueabi\n\n## **arm/OABI 및 arm/EABI 차이점**\n\n### **소프트 인터럽트 호출방식**\n\n- OABI\n    - swi __NR_SYSCALL_BASE(==0x900000)+1\n- EABI\n    - mov r7, #1 (시스템콜 인덱스)\n    - swi 0\n\n### **구조체 패키징**\n\n- OABI\n    - 구조체는 4 바이트 단위로 정렬\n- EABI\n    - 구조체 사이즈대로 사용\n\n### **스택에 인수 정렬**\n\n- OABI\n    - 스택에 저장할 때 4 바이트 단위로 저장\n- EABI\n    - 스택에 저장할 때 8 바이트 단위로 저장\n\n### **64bit 타입 인수 정렬**\n\n- OABI\n    - 4 바이트 단위로 정렬\n- EABI\n    - 8 바이트 단위로 정렬\n\n### **Enum 타입 사이즈**\n\n- OABI\n    - 4 바이트 단위\n- EABI\n    - 가변으로 지정할 수 있음\n\n### **인수 전달 시 레지스터 수**\n\n- OABI\n    - 4개 (r0~r3)\n- EABI\n    - 7개(r0~r6)\n\n## **차이점 예제**\n\n### **소프트 인터럽트 + 64bit 타입 인수 정렬**\n\n예) long sum64(unsigned int start, size_t size);   syscall no=100\n\n- arm/OABI\n    - r0에 start 대입\n    - r1과 r2에 size를 64비트로 대입\n    - swi #(0x900000 + 100)\n- arm/EABI\n    - r0에 start 대입\n    - r2와 r3에 size를 64비트로 대입\n    - r7에 100 대입\n    - swi 0","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-10T15:00:00.000Z","url":"/blog/techsavvy/computerarchitecture/2021-04-11-abistandards/"},{"id":"techsavvy/computerarchitecture/2021-04-14-posix","title":"About POSIX","description":"","content":"## About POSIX\n\n- General\n    - Portable Operation System Integration\n    - From 1988~\n    - 여러 운영체제들이 사용하는 API가 모두 달라서 새발하는데 애로사항이 많으므로 최소한의 영역에서라도 통일 시켜보자는 의미에서 시작\n    - spec : [http://get.posixcertified.ieee.org/](http://get.posixcertified.ieee.org/)\n    - Versions\n        - POSIX.1 (IEEE 1003.1-1988): core services\n        - POSIX.2 (IEEE 1003.2-1992) : shell & utility\n        - POSIX.1b (IEEE 1003.1b-1993) : realtime related\n        - POSIX.1c (IEEE 1003.1c-1995) : Thread related\n    - POSIX.1 - 2017 까지 나옴\n        - Process create&control, signals, file&directory operations, pipes, c lib, IO port interface and control, process triggers\n\n- POSIX PSE51-compliant API\n    - IEEE 1003.13 에서 정의하고 있음\n    - POSIX Realtime and Embedded Application Support(AEP)\n    - PSE51-Minimal Realtime System Profile : small embedded systems, no MMU, no Disk, no terminal\n\n   ![Desktop View](/assets/img/posix_arch.png)","categories":["TechSavvy","ComputerArchitecture"],"tags":["Blogging","POSIX","ComputerArchitecture","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-13T15:00:00.000Z","url":"/blog/techsavvy/computerarchitecture/2021-04-14-posix/"},{"id":"techsavvy/operatingsystems/2020-10-19-agl-에-대하여","title":"AGL(Automotive Grade Linux)에 대하여","description":"","content":"## 1. 들어가며\n\nAGL(Automitive Grade Linux)는 유럽, 일본, 한국 등 다양한 차량제조 및 부품회사들이 협력하여 만들고 있는 차량환경에 적합한 Opensource OS(Operating System)입니다..\nToyota의 막강한 Funding 력으로 Linux Foundation의 지휘를 받으며 빠르게 발전하고 있습니다.\n\n참여하고 있는 멤버 회사들은 매우 많지만 그 중 Toyota를 필두로 하여 일본계 회사들이 매우 적극적으로 달려들고 있습니다.\n[https://www.automotivelinux.org/about/members/](https://www.automotivelinux.org/about/members/)\n\n![Desktop View](/assets/img/post/2020-10-19/1.png)\n\nToyota는 사실상 그저 물주에 가까울 정도로 AGL에 대해 소극적인 활동을 보여줬었는데, 2020년 3Q를 시작으로 갑자기 무서울정도로 달려들고 있는 것 같아 보입니다..\n여담이지만 Havard Bussiness School 교육의 단골 손님이자 관료제 끝판왕 Toyota 가 최근 성과제로 변경을 시도하기도하고, AGL을 포함한 다양한 SW 정책과 같이 변모를 시도한다고 합니다.\n(카더라 통신에 의하면 Tesla에게 자극 받아서 그렇다는 소문이..)\n\n최근엔 BMW 와 VW 그룹의 리서치 센터도 활발하게 참여하는 등 Tesla를 대항하기 위한 일종의 여집합처럼 [합종연횡](https://namu.wiki/w/%ED%95%A9%EC%A2%85%EC%97%B0%ED%9A%A1) 을 한다는 느낌으로 Co-work 중인 것으로 파악됩니다..\n\n필자는 이런 이유로 AGL에 대한 스터디가 기본은 되어 있어야 할 것 같아서 정리 겸 이 글을 기고하기로 결심했습니다..\n\n그럼 시작 해보도록 할까요\n\n## 2. 서막\n\nAGL을 이해하기 위해서는 [Tizen OS](https://namu.wiki/w/%ED%83%80%EC%9D%B4%EC%A0%A0) 에 대해 조금 알아야 합니다.\nTizen OS 는 삼성과 인텔이 시작한 모바일 환경의 OS입니다.\nAndroid와 iOS의 독점을 막기 위해 야심차게 시작했지만 막강한 개발자 친화적 정책과 마케팅 전략등에 압도적으로 밀려 결국은 모바일 시장에서는 역사의 뒤안길로 사라졌죠.\n(2018년을 마지막으로 삼성에서 타이젠폰 개발을 중단)\n\n그런데 AGL에서 [Tizen IVI](https://wiki.tizen.org/IVI)를 레퍼런스로 기반 프로젝트를 시작하여 이 다 죽은 OS를 살려내고 있습니다.\n\n## 3. 대항마\n\n![Desktop View](/assets/img/post/2020-10-19/genevi.png)\n\n대표적 대항마로는 [GENIVI Alliance](https://en.wikipedia.org/wiki/GENIVI_Alliance)의 GENEVI Open Source Software Project가 있습니다.\n(추후 다른 글에서 자세히 다룰 예정)\n\nGENEVI는 더 빠른 시점인 2009년에 시작 했는데요. 이 프로젝트 역시 Linux Foundation를 필두로 나아가고 있습니다.\n\n참여 멤버로는 BMW Group, Delphi, GM, Intel, Magneti-Marelli, PSA Peugeot Citroen, Visteon, Wind River System 등이 있고,\n차량 OEM 뿐아니라 NASA 로켓의 OS(VxWorks)로 유명한 Wind River 같은 회사들도 참여하여 스마트카 시장에서 반 구글진영의 대표적 주자 중 하나입니다.\n\n## 4. 목표\n\n위에서 필자가 언급했듯이 AGL은 IVI(In-Vehicle Informaintment) 시장을 목표로 삼고 있습니다.\n이는 GENEVI도 마찬가지 인데요. 이 두 OS의 공통점은 비영리단체라는 점입니다.\n시장 논리 때문인지는 몰라도 사실 발전 속도가 Android나 iOS에 비해 현저하게 느렸습니다만, \n2020년 화웨이 사태와 같이 Google의 Android OS 사용 License 자체를 막는 사건들을 지나 국가간의 분쟁사태까지 번지고 있는 최근에는\nOS의 발전속도에 박차가 가해지고 있습니다.\n\n방향성을 보면 현재 모바일에서 지원되는 기능들을 팔로우업하는 것 우선으로 보입니다.\n\n하지만 여전히 갈길은 매우 멀다고 볼 수 있습니다.\n\n2020년 AGL의 로드맵 자료를 통하여 비춰볼 수 있는 현재 지표와 그 방향성에 대해 살펴보겠습니다. [링크](https://wiki.automotivelinux.org/agl-roadmap)\n\n## 5. 2020년 로드맵 (2월 업데이트)\n\n- Yocto 2.6(thud) -&gt; 3.0(zeus) 플랫폼으로 업데이트\n- 다양한 플랫폼을위한 Graphical API 기능 지원 \n- Android auto, Apple Carplay와 같은 스마트폰과의 연계를 위한 솔루션 마련\n- 지속적 업데이트를 위한 로드맵 마련\n- QA 및 테스트 (CI/CD) 커버리지 강화\n- C/C++ Skeleton 코드 및 Json 자동생성 API 툴 개발\n- 클라우드시스템을 이용하여 스마트폰을 통해 차량을 제어할 수 있는 모델을 제안\n- 네이티브 기능으로서 MQTT 프로토콜 지원(최소한의 전력과 데이터로 통신)\n- 보안/어플리케이션 프레임워크\n  - 어플리케이션 프레임워크와 관련한 방화벽 기능 추가\n  - 보안 관리자 기능을 통한 보안강화\n  - 플러그인, 보안, 프로토콜등을 지원하고 유지보수하기 쉽도록 프레임워크를 제공\n  - Java Client, LXC/systemd 와 같은 레거시 및 써드파티 어플리케이션 지원\n  - 연결 지속을 위한 기능 내장(keep alive, reconnection)\n  - Javascript 및 python binding 지원\n- 부팅 및 파워 관련\n  - 부팅시간 감소, 부트 모드 선택, 모니터링 기능을 하는 부트 매니저 지원\n  - RAM Sleep 지원 & 저온 동작 \n- 최소기능을 요하는 ECU지원 등을 위한 AGL 최소화 시스템 기능 지원\n- QoS를 보장하기위한 Real Time API 기능 지원\n\n### 5.1 App FW & Security EG(Expert Group)\n\n- 개발자가 어플리케이션에 서명하고 로드 할 수 있는 매커니즘을 마련\n- 웹 앱용 App Launcher 및 HTML5로 즉석에서 다운로드 할 수있는 코드 관리 전략\n- 현재 백그라운드에있는 비 권한 앱을 중지하기위한 앱 프레임 워크 API 및 전략 (예 : SIGTERM)\n- Sleep mode로 부터 돌아올 수 있는 App 프레임워크 통신 바인더를 제공\n- 어플리케이션 라이프사이클 관리자\n  - 홈 화면에서 백그라운드 앱을 인지(음악, 전화, 읽지않은 메시지 등)\n  - 매니저를 통한 각각을 끌 수 있음\n  - SM(State Management) 정의 및 구현 필요\n- 앱 등록 및 패키징을 위한 하드웨어 추상화(HAL) API 제공\n- 멀티플랫폼 기반으로 키관리, 유지보수, 빌드 할 수 있는 메인 어플리케이션 프레임워크의 모듈화\n  - 어플리케이션 프레임워크와 키들의 분리(코드 레벨)\n  - 디바이스 개발자가 키의 변경을 쉽게 할 수 있도록 함\n  - 키 관리와 바인딩을 쉽게 할 수 있도록 라이브러리 제공\n\n- Web Apps/HTML5\n  - WAM(웹앱 관리자)에서 요구하는 Chromium Webview Upstream API 지원\n  - WAM 와 Chromium과의 통신 연계 향상\n  - WAM upstream 과 WebOSE Chromium를 독립적으로 동작하도록 함\n  - 새로운 Window Manager와 WAM의 통합\n  - 웹앱\n    - 새로운 보안모델 통합\n    - 향상된 어플리케이션 라이프사이클 관리자 \n    - HTML5 데모 플랫폼 컨테이너화하여 제공\n    - 데모 웹앱 라이브러리 제공\n    - 추가 데모 어플리케이션들 제공\n\n### 5.2 Graphical EG\n\n**2019년을 끝으로 얼추 로드맵 달성한 것으로 보임**\n\n- Window Manager와 Homescreen 개발 마무리\n  - Homescreen API/서비스\n    - QT, HTML5 마무리 단계\n  - 가상 키보드를 위한 일어와 영어의 High-level API 제공 \n  - 팝업 기능 지원 (가상키보드, 경고 등)\n  - 멀티디스플레이 해상도 관련 기능 지원\n    - Portrait vs Landscape\n    - Scalable display size\n- 멀티 페이지, 폴더, 슬라이더 등 스크린에서 벚어나는 앱 관리\n- 향상된 이중 스크린 기능\n- 하드웨어 Plane 관리 : 후방카메라, 스마트폰 연결 등\n- 대화식 사용자 응답 기능 : 스크린 떨림, 비프음 등\n- Wayland/Weston Upstream 지원\n- xdg 프로토콜\n- QT 변경 검토 : HTML5, GTK+ 등..\n- High Level Audio API 지원\n- Bluetooth Audio 를 Blues/Alas 로 변경\n- 음성 인식 및 문자-&gt;음성 서비스\n- 마이크 입력 과 미디어 플레이어 출력간의 정책 관리\n\n\n### 5.3 Connectivity EG\n\n- 차량 Signal 관리\n  - 마지막 차량 설정 기반 CAN 메세지 초기화\n  - 스트리밍과 블루레이 컨텐츠의 암호화\n  - 향상된 CANoe &lt;-&gt; AGL 간의 메시지 번역기능 지원\n- 향상된 블루투스 기능\n  - AGL를 지원하는 다른프로젝트에서 수행한 작업을 기반으로 독접 칩 스택에 대한 지원 준비\n  - 저전력 기능 지원\n- 향상된 Wifi\n  - AP모드 지원\n- 향상된 전화기능 \n- 네트워크 바인딩\n  - 동시에 전화가 필요한 텔레메틱스 서비스를 위한 동시성 기능제공\n\n### 5.4 V2X EG\n\n- Kickoff 대기중\n\n### 5.5 Virtualiation EG\n\n- Host로서 AGL의 가상화 지원\n  - ARM과 Intel 동시에 지원하는 Opensource Hypervisors 기능 추가\n  - Compile 타임에 어떠한 Hypervisor를 선택할 것인지 결정할 수 있음\n    - 드디어(?) KVM 지원(Renesas RCar-M3 에서 이용가능)\n    - XEN과 Jailhouse 지원\n- Guest로서 AGL의 가상화 지원\n  - 공식적으로 ARM과 Intel CPU 에서 게스트 OS로 동작\n  - XEN, KVM, Jailhouse 지원\n  - Guide Document 제공\n- AGL의 그래픽 가상머신 관리자 어플리케이션 \n  - AGL GUI에서 게스트 OS를 컨트롤 하기 위한 툴 생성\n  - 게스트 OS의 시작과 종료, USB 부착 기능 제공\n- VMs/AGL 프로파일 통신\n  - 서로다른 VM들과 AGL간의 통신 및 상호작용을 위한 공통 API 정의/디자인/개발\n\n### 5.6 Navi EG\n\n- AGL 네비게이션 API 개발 완료\n- Documentation 완료\n- Reference 앱 제공\n\n\n## 6. 5-Star Projects\n\n### 6.1 Resource Control Project\n\n[Jira Link](https://jira.automotivelinux.org/browse/SPEC-138?filter=10409)\n\nAGL은 일련의 작업 (커널의 cpuset)에 CPU 집합을 할당하는 메커니즘을 제공해야합니다. 정책 기반 결정에 따라 정책 관리자는 커널 계층의 \"자원 제어\"를 사용하여 대상 프로세스 또는 프로세스 그룹에 적절한 CPU 하위 집합을 할당해야합니다. (일반적으로 cgroups / cpuset이 사용됩니다).\n\n- 배경:\n하드웨어에는 여러 CPU가있을 수 있으며 시스템은 여러 작업 / APP을 실행할 수 있습니다. 시스템은 일정 및 경합 (캐시 등)을 줄이기 위해 신중한 프로세서 배치의 이점을 얻을 수 있습니다\n\n## 7. 4-Star Projects\n\n### 7.1 Smart Device Link (SDL)\n\n[Jira Link](https://jira.automotivelinux.org/browse/SPEC-133?filter=10410)\n\nAGL에 스마트 장치 링크를 포트합니다.\n\n- 배경:\nSmart Device Link는 Ford가 차세대 차량에 사용하고 있으며 Toyota와 제휴하여 업계 전반에 걸쳐 마케팅되고 있습니다.\n\n\n## 8. 마무리하며\n\n위와 같이 로드맵을 요약하며 현재까지 진행된 부분과 앞으로의 방향에 대해 미뤄 짐작해볼 수 있었습니다.\n\n와이파이, 블루투스, 오디오 같은 디바이스 사용을 위한 API는 어느정도 성숙했고, 차량환경에서 제일 중요한 네비게이션 API는 완성에 단계로 보입니다.\n웹앱을 지원하기위해서 많은 노력을 쏟아 이를 통해 크로스플랫폼의 호환성을 보장하며 개발자에게 유연한 API를 제공할 수 있을 것으로 보입니다.\n\n추가적으로 프레임워크 레벨에서 보안을 보장하고 개발자가 개발을 쉽게 할 수 있도록 많은 API들을 고려중입니다.\n\n아직 Smart Phone에서 지원하는 기능들을 많이 지원하지 못하는 것을 볼 수 있었는데요.\n실제로 Cloud와 같은 기능을 지원하기 위해서는 Connectivity나 V2X EG의 프로젝트들이 빠른시일내에 진행되고 성숙해야 할 것으로 보입니다.\n\nGraphical API는 많은 부분에서도 로드맵을 달성하였으나 현재 QT에서 다른것으로의 이전도 고려중인 것으로 사료되고 있습니다.","categories":["TechSavvy","OperatingSystems"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2020-10-18T15:00:00.000Z","url":"/blog/techsavvy/operatingsystems/2020-10-19-agl-에-대하여/"},{"id":"techsavvy/embeddedlinux/2020-04-05-00-crossdevelopment","title":"00. Cross Development","description":"","content":"## What is Cross Development?\n\nCross Development란 내가 지금 **개발하고 있는 환경**과 실제 개발된 어플리케이션이 **동작하는 환경**이 다르게 어플리케이션을 개발하는 것을 의미한다.\n\n예를 들면 Windows 10 환경에서 Visual Studio로 Windows 10에서 동작하는 어플리케이션을 만들 수도 있지만, 프레임워크의 도움을 받아 OS 레벨에선 iOS, Android등에서 동작하는 어플리케이션도 만들 수 있다. Xamarin, UWP, Flutter등이 그 프레임워크의 예이다.\n\n**환경**이 다르다는 것은 어떤것을 의미할까?\n\n혹자는 *\"Windows 건 Mac이건 Android건 코드짤때는 다 똑같은데, 어플리케이션 레벨에서 원하는 기능 다 똑같은데 뭐가 다른거야?\"* 하고 생각할 수 있다.\n\n하지만 내부로 들어가면 그 어플리케이션이 동작하기위해 필요로하는 라이브러리들이 있고, 그 라이브러리들이 동작하기 위한 디바이스들과 그 것들을 운용하는 커널 및  운영체제 그리고 아키텍쳐들이 준비가 되어야 비로서 어플리케이션이 동작 할 수 있는 것이다.\n\n이러한 시스템 레벨의 작업들을 1차적으로 처리해주는 것을 우리는 프레임워크단라고 부르기도 한다.\n\n- JAVA의 **JavaRuntimeEnvrionment**\n- Windows의 **.NETFramework**\n- MAC의 Rosetta 2\n\n그리고 Visual Studio, Android Studio 같은 통합개발환경(IDE)에서 개발자들이 쉽게 개발할 수 있게 컴파일러, 링커, 디버거 등을하여 프레임워크와 연결시켜주는 것이다.\n\n혹자는 *\"그럼 저는 컴파일이 필요없는 파이썬, JavaScript, Ruby, SQL 과 같은 인터프리터 언어를 이용하여 개발할 건데, Cross Development할 필요없는거네요?\"* 하고 말할 수 있다.\n\n작성한 인터프리터 언어에서 필요로 하는 ***라이브러리***가 동작하고자 하는 환경을 ***지원하지 않는다면*** 이 언어로 작성한 어플리케이션은 무용지물이다.\n\n이렇듯 우리가 Thorough하게 환경들을 고려해야 어플리케이션의 동작과 성능을 테스트하고 보장할 수 있다.\n\n## Cross Compiling for Embedded System\n\n![Desktop View](/assets/img/00-CrossDevelopment/1.png)\nRef: Linux Foundation 2020 Conference   \n\n임베디드는 이러한 Cross Development 기법을 사용해야 효율적인 분야 중 하나다.\n\n지금이야 임베디드 기기가 훌륭한 성능을 내지만, 몇년전만 하더라도 해당기기에서 직접 개발을 한다거나 Compile을 한다거나 하는 것은 꿈도 꾸기 힘들었다.\n\n그래서 발전한 Cross 빌드 시스템관련한 Open Source Projects들은 OpenEmbedded, Buildroot, Yocto Project, Bitbake 등이 있다.","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-04T15:00:00.000Z","url":"/blog/techsavvy/embeddedlinux/2020-04-05-00-crossdevelopment/"},{"id":"techsavvy/embeddedlinux/2020-04-06-01-toolchain","title":"01. ToolChain","description":"","content":"## What is Toolchain?\n\nToolchain이란 말 그대로 Cross Develepment를 위한 도구들의 집합이다.\n\n![Desktop View](/assets/img/01-Toolchain/1.png)\nRef: Linux Foundation 2020 Conference   \n\n위 그림처럼 GNU Toolchain은 크게 컴파일러, binutils, C 라이브러리, GDB 디버거로 나뉘어진다.\n\n- 컴파일러 : 호스트 개발환경에서 타겟의 아키텍쳐로 컴파일하기 위한 도구이고\n- binutils : 컴파일한 Object 들을 컨트롤하기 위한 어셈블러, 링커 등\n- C 라이브러리 : POSIX 및 리눅스 커널 인터페이스\n- GDB : GNU 디버거\n\n## Getting a toolchain\n\n![Desktop View](/assets/img/01-Toolchain/2.png)\nRef: Linux Foundation 2020 Conference   \n\n위 그림처럼 Toolchain은 보통은 칩 제조사에서 제공을 한다. 혹은 Linaro 같은 유명한 Third Party를 통해 다운로드 할수도 있고, Yocto Project같은 빌드 시스템을 사용하면 편하게 얻을 수 있다.\n\n## Toolchain prefix\n\n![Desktop View](/assets/img/01-Toolchain/3.png)\nRef: Linux Foundation 2020 Conference   \n\n위 그림을 통해 Toolchain의 작명 룰을 살펴보자.\n\n\"-\" 를 통해 구분하며 순서대로 \"아키텍쳐-제공사-커널명-OS명\" 으로 지어진다.\n\n## Toolchain prefix for ARM toolchain\n\n![Desktop View](/assets/img/01-Toolchain/4.png)\nRef: Linux Foundation 2020 Conference   \n\nToolchain 네이밍 마지막은 OS라고 위에서 언급했다. 여기에 추가적으로 ABI(Application Binary Interface)가 더 붙게된다.\n\n이는 타겟 OS가 어플리케이션간 어떠한 인터페이스로 Binary 데이터를 주고 받는지에 대한 규칙을 말해준다.\n\n- 데이터 타입과 정렬 방법\n- 함수 호출 시 인수 및 결과에 대해 레지스터 교환 방법\n- 시스템 콜 호출 방법\n- 프로그램 코드의 시작과 데이터에 대한 초기화 방법\n- 파일 교환 방법(ELF 등)\n\n위 그림에서 처럼 오래된(Old or obsolete) ABI는 OS뒤에 덫붙이는 것을 생략한다. 이는 32bit의 ARM 아키텍쳐를 지원하던 시절에 이용되었다.\n\nEABI(Embedded ABI) 부터는 임베디드 환경에서 64bit 아키텍쳐를 지원하기 위해 만들어졌다. (ARM, PowerPC, MIPS 등)\n\neabi도 그냥 사용하면 softfloat이고 뒤에 hf를 붙여 eabihf로 사용하면 hardfloat이다. 그 차이점은 아래와 같다.\n\n- softfloat\n    - FP instruction을 만들지 않고 GCC가 컴파일타임에 라이브러리에서 함수로 준비\n- hardfloat\n    - FP instruction을 에뮬레이션한다.\n\n    &gt; A CPU like the ARM can do calculations. In most programs most of these calculations are of the \"whole numbers\" (integer) type, as these are very simple to do electronically.\n    Some programs also do \"floating point\" calculations, and these programs also are expected to work on CPU's that do not have the hardware to do floating point calculations. In such cases these calculations are automatically routed, by the operating system, to a library of calculation routines that do these calculations using integer calculus. For example a simple division like 2/3 is done with hundreds of integer calculations. This is called \"software floating point calculus\", or short \"soft float\"\n    But the ARM chip has a CPU that can also do floating point calculations directly in hardware!\n    This happens very much faster, as a floating point calculation in hardware is almost as fast as a integer calculation. hardware floating point calculus is shortened to \"hard float\".\n    In the past the R-PI's operating systems did not \"know\" the R-PI's CPU could do floating point calculus, so all floating point calculations were done using the software library. With the latest OS's they became \"aware\" of the hard float capability of the PI and began using it, which means a very big speed increase for programs using a lot of floating point calculations.\n\n## Toolchain sysroot\n\n![Desktop View](/assets/img/01-Toolchain/5.png)\nRef: Linux Foundation 2020 Conference   \n\nsysroot란 실제 타겟환경에서 가지게 되는 rootfs를 의미한다. \n\n![Desktop View](/assets/img/01-Toolchain/6.png)\nLinux rootfs\n\n> The root file system (named rootfs in our sample error message) is the most basic component of Linux. A root file system contains everything needed to support a full Linux system. It contains all the applications, configurations, devices, data, and more. Without the root file system, your Linux system cannot run.\n\n![Desktop View](/assets/img/01-Toolchain/7.png)\nRef: Linux Foundation 2020 Conference   \n\n그 타겟환경 안에 있는 모든 것이 다 필요하지는 않고, 그 중에 Cross-development를 위한 일부분만 있으면 된다.\n\n예를들면 라이브러리와 헤더파일 등이 있다.\n\n### What Toolchain contains\n\n![Desktop View](/assets/img/01-Toolchain/8.png)\nRef: Linux Foundation 2020 Conference   \n\n## Example\n\n라즈베리파이을 타겟으로 개발한다고 가정하자.\n\n![Desktop View](/assets/img/01-Toolchain/9.png)\nRef: Linux Foundation 2020 Conference   \n\n> 요즘은 SBC(Single Board Computer)라고 불리울 만큼 10여년 전으로만 돌아가도 컴퓨터라고 불려도 무색할 성능이다. 하지만 임베디드기기는 임베디드기기 일뿐, 우리의 호스트 컴퓨터 성능에 미치지는 못할 뿐더러, 기기의 목표가 PC를 대체하는 것은 아니지 않은가? 이야기가 좀 샜다. ㅋㅋ;;\n\n라즈베리파이 3B 모델의 Toolchain 은 공식적으로 아래 6개이다.\n\n- arm-bcm2708hardfp-linux-gnueabi\n- arm-bcm2708-linux-gnueabi\n- arm-linux-gnueabihf\n- arm-rpi-4.9.3-linux-gnueabihf\n- gcc-linaro-arm-linux-gnueabihf-raspbian\n- gcc-linaro-arm-linux-gnueabihf-raspbian-x64\n\n자 이름에 공통점들이 보이는가?\n\n이 순서를 보통 Triple 이라고 부르는데, \n\n요즘은 \\<arch>-\\<vender>-\\<kernel>-\\<os> 이 기본 룰이고 그 앞뒤로 조금씩 바꿔서 사용하는 듯하다. (자세한 것 더 찾아봐야한다)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-05T15:00:00.000Z","url":"/blog/techsavvy/embeddedlinux/2020-04-06-01-toolchain/"},{"id":"techsavvy/embeddedlinux/2021-04-07-02-preparingtodebugwithgdb","title":"02. Preparing To Debug With GDB","description":"","content":"Toolchain 이 준비가 됐다면 일단 도구들은 다 준비가 됐다고 봐도 된다.\n\n이제 실제로 디버깅을 해야할 대상을 디버깅 할 수 있게 만들자.\n\n## Compiler Option 조정\n\n![Desktop View](/assets/img/02-PreparingToDebugWithGDB/1.png)\nRef: Linux Foundation 2020 Conference   \n\n먼저 c나 c++ 소스코드를 컴파일 할때 위 그림처럼 -g 옵션이나 -gN 옵션을 주어 source code debugging 정보 등 다양한 정보들이 elf에 포함되게 만들어준다.\n\n-ggdbN 을 사용하게되면 표준 DWARF format 대신 gdb format을 사용하여 elf를 생성한다.\n\n일반적으로 -g 만 넣게되면 elf는 -g2에 해당하는 단계를 가지게 된다.\n\n![Desktop View](/assets/img/02-PreparingToDebugWithGDB/2.png)\nRef: Linux Foundation 2020 Conference   \n\n그 다음은 Optimization Option 을 조정해줘야 한다.\n\nOptimization을 하게되면 컴파일러는 코드의 로직을 분석하여 동일한 결과를 내는 다른 방식의 실행코드를 생성할 수 도 있기 때문에, 디버깅시에는 최적화 옵션을 빼줘야한다.\n\n-O0 옵션을 주거나 -Og 를 주어 gdb 호완가능한 최적화 옵션을 주자\n\n## Reference\n\n1. [http://www.epnc.co.kr/news/articleView.html?idxno=48128](http://www.epnc.co.kr/news/articleView.html?idxno=48128)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-06T15:00:00.000Z","url":"/blog/techsavvy/embeddedlinux/2021-04-07-02-preparingtodebugwithgdb/"},{"id":"techsavvy/embeddedlinux/2021-04-08-03-remotedebuggingusinggdbserver","title":"03.Remote Debugging Using Gdbserver","description":"","content":"디버거 툴인  gdbserver 와 gdb를 이용하여 원격 디버깅을 할 수 있는 방법을 포스팅한다.\n\nRemote debugging이란 실제로 Application이 동작하고 있는 Target에 gdbserver를 실행시켜 원격 Host에서 gdb 및 toolchain을 활용하여 디버깅하는 방식이다.\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/1.png)\nRef. Linux Foundation Conference 2020\n\n이 방식은 Target에는 compiler를 통해 최적화 및 디버깅 심볼을 제외한 어플리케이션을 gdbserver를 통해 Listening 시키고, 원격 Host에서는 최적화 및 디버깅 심볼이 포함된 어플리케이션을 Toolchain 및 gdb를 통해 gdbserver에 접속하여 디버깅하는 방식이다.\n\nTarget 이 Embedded Board인 경우에는 디스크 메모리의 용량이나 시스템 메모리의 용량이 매우 제한적인데, 이런 케이스에 Remote debugging이 유용하다.\n\n# **Break into Examples**\n\n- 본격적 설명에 앞서 일단 기본적으로 gdb를 command line으로 어느정도는 다룰 줄 안다고 가정한다.\n\n## **0. Setting up Environment**\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/2.png)\n\n위 그림은 두개의 VS Code에서 각각 Docker Container를 띄운 상황이다.\n\n- Host IP : 172.25.125.2\n- Target IP : 172.25.125.3\n\n## **1. Printing Helloworld**\n\n테스트하고자하는 예제 시나리오는 아래와 같다.\n\n1. Host에서 Target용 ***Helloworld.cpp*** 를 Compile하여 Target에 전송\n    - ***Not Stripped, with debug info, no optimization***\n2. GDBServer 와 GDB를 연동\n3. Target상의 Application을 Host에서 Network로 실시간 Debugging\n\n***Helloworld.cpp*** 테스트 코드를 확대하면 아래와 같다.\n\n```cpp\n#include <iostream>\n#include <vector>\n\nint main(){\n    std::vector&lt;int&gt; intVector {1,2,3,4,5,6,7};\n    \n    for (auto i : intVector){\n        std::cout &lt;< \"Hello World : \"&lt;< i &lt;< std::endl;\n    }\n    return 0;\n}\n```\n\n### 1. Host에서 Target용 ***Helloworld.cpp*** 를 Compile하여 Target에 전송\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/3.png)\n### 2. GDBServer 와 GDB를 연동\n\n- ***[TARGET]*** 먼저 Target에서 GDBServer를 틀어 Listening 상태로 만들자\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/4.png)\n\n- *[HOST]* GDB를 실행하여 네트워크로 GDBServer에 접속하자\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/5.png)\n\n- *[HOST, TARGET]* 성공적으로 연결된 화면\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/6.png)\n\n### 4. Target상의 Application을 Host에서 Network로 실시간 Debugging\n\n- *[HOST]* main 함수에 Breaking 포인트를 잡고 확인\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/7.png)\n\n- *[HOST]* 실행\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/8.png)\n\n    - remote로 접속한 gdb 에서는 run command는 불가능하다\n    - HOST에서 n 으로 Line by Line 으로 진행하면 TARGET에서 std::cout 을 통한 메세지가 stdout에 찍히는 것을 확인 할 수 있다\n\n## **2. Attaching to running applications**\n\nGDBServer를 이용하여 Applcation을 처음부터 시작하는 경우를 제외하고도 Just-In-Time 한 동작중인 Application에도 Attach 할 수 있다.\n\n무한으로 Loop을 타며 동작하는 아래와 같은 예시를 들어 설명하겠다.\n\n```cpp\n//running_app.cpp\n#include <iostream>\n#include <vector>\n#include <thread>\n#include <chrono>\n\nint main(){\n    using namespace std::chrono_literals;\n    while (true){\n        std::vector&lt;int&gt; intVector {1,2,3,4,5,6,7};\n        for (auto i : intVector){\n            std::cout &lt;< \"Hello World : \"&lt;< i &lt;< std::endl;\n            std::this_thread::sleep_for(1s);\n        }\n    }\n    \n    return 0;\n}\n```\n\n위 예제는 1초마다 Hello World : 1 ~ 7 을 반복해서 stdout으로 출력할 것이다.\n\n*[TARGET]* 먼저 빌드한 위 어플케이션을 Target으로 옮긴 후 실행해보자.\n\n- running_app 은 pid=25685 가 할당 되어 실행 중이다.\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/9.png)\n\n***[TARGET]*** gdbserver를 runnining_app에 attach 해보자\n\n- 정상적으로 Listening 이 시작되면 역시 아래와 같은 로그가 확인된다\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/10.png)\n\n***[HOST]*** gdb로 TARGET에 아래와 같이 연결하자\n\n![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/11.png)\n\n- 연결과 동시에 TARGET의 동작이 일시정지된다\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/12.png)\n\n- 로그를 보면 연결한 시점이\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/13.png)\n\n    ```cpp\n    std::this_thread::sleep_for(1s);\n    ```\n\n    를 실행 중인 것으로 판단된다.\n\n    - nanosleep.c 쪽의 source 코드가 없기 때문에 심볼의 내용은 tracing이 불가능하다\n\n***[HOST]*** n나 s 커맨드로 Line by Line으로 디버깅을 진행하면 된다\n\n- n command (next)\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/14.png)\n\n- s command (step into)\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/15.png)\n\n***[HOST]*** detach 를 통해 다시 정상 동작을 시킬 수 있다\n\n- Debugging 하던 마지막 시점부터 다시 정상 동작한다\n\n    ![Desktop View](/assets/img/03-RemoteDebuggingUsingGdbserver/16.png)","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-07T15:00:00.000Z","url":"/blog/techsavvy/embeddedlinux/2021-04-08-03-remotedebuggingusinggdbserver/"},{"id":"techsavvy/embeddedlinux/2021-04-09-04-remotedebuggingusingvscode","title":"04.Remote Debugging Using VSCode","description":"","content":"## Preface\n\n본 포스팅은 아래 포스팅의 연장선이다.\n\n- [03.RemoteDebuggingUsingGdbserver]()\n- VS Code의 Remote-Development 기능에 대한 포스팅도 설명예정 (추후 업로드)\n\n[03.RemoteDebuggingUsingGdbserver]() 에서 Setup한 환경을 그대로 이어간다.\n\n## **0. Setting up Environment**\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/1.png)\n위 그림은 두개의 VS Code에서 각각 Docker Container를 띄운 상황이다.\n\n- Host IP : 172.25.125.2\n- Target IP : 172.25.125.3\n\nVSCode에는 Run Extension을 기본으로 제공한다.\n\n이 Run은 다양한 언어 및 Compiler, Debugger를 지원하기 위해서 *launch.json*이라는 특정 설정파일로 부터 실행(Run)을 한다.\n\n## 1. Configuring launch.json\n\nF1 Key 를 눌러 Command Pallate를 열고 ***launch.json***을 검색하여 실행하자.\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/2.png)\n\n***launch.json***은 workspace/.vscode 안에 생성이된다.\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/3.png)\n\n## 2. Debbuing Running Application\n\nlaunch.json에서 몇가지를 수정 및 추가하면 쉽게 **TARGET**에서 동작중인 Application을 Debugging할 수 있다\n\n- prelaunchTask 삭제\n    - 이미 HOST에서 빌드 후 TARGET에 elf를 전송했다고 가정\n- miDebuggerServerAddress 추가 → TARGET의 gdbserver의 Ip와 port를 입력\n    - ex) \"miDebuggerServerAddress\": \"172.25.125.3:2001\"\n\n그리고 Source 로 이동하여 좌측 Bar에서 Run Exetension을 클릭하자\n\n![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/4.png)\n\n좌측 상단의 재생버튼을 클릭하면 Debugging Session이 연결된다.\n\n- GDBServer는 Listening 상태여야 한다\n\n    ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/5.png)\n\n\n- 세션은 연결되었지만 breaking point를 설정하지 않으면 TARGET의 Application은 계속해서 실행상태를 유지한다\n\n    ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/6.png)\n\n\n    - Toggle breaking point 는 라인넘버 왼쪽을 마우스 클릭하거나 F9 Key를 이용하여 설정한다\n\n        ![Desktop View](/assets/img/04-RemoteDebuggingUsingVSCode/7.png)\n\n\n    - Breaking Point를 설정하면 바로 TARGET의 동작이 멈추고 해당 라인에서 Variable 과 Call Stack이 추적가능해진다","categories":["TechSavvy","EmbeddedLinux"],"tags":["Blogging","Linux","AGL","EmbeddedLinux","OpenEmbedded","Yocto","CrossDevelopment","GCC","GDB","Toolchain"],"pubDate":"2021-04-08T15:00:00.000Z","url":"/blog/techsavvy/embeddedlinux/2021-04-09-04-remotedebuggingusingvscode/"},{"id":"techsavvy/github/2020-09-11-github-pages로-블로그만들기","title":"Github Blog 만들기","description":"","content":"## How it works\nGithub 에서는 해당 계정 Username 이름으로 연동된 web page를 제공해둔다.\n\n예를들면 필자의 Github User ID 가 jayleekr이고,\n해당 계정의 Repository에 https://github.com/jayleekr/jayleekr.github.io 와 같이 새로운 블로그 용 Repository를 만들면\nhttps://jayleekr.github.io/ 라는 주소로 웹페이지가 생성한 블로그용 Repository와 연동이 된다.\n\n이 말은 해당 저장소를 가지고 있는 Github측(Remote)에서 백엔드 웹서버를 위 규칙에 의거하여 자동으로 생성해 준다는 의미이다. \n\nFront-end를 만들어 보자\n\n## Front-end\n\nFront-end 프레임웍이야 정말 종류가 많지만 좀 더 빠르게 만들기 위해 가장 유행하는 방법을 이용하여 만들어 보겠다.\n\n이름하야 바로 **Jekyll** 인데, Github 공동 설립자인 Tom Preston-Werner 가 Ruby-on-rails 언어로 작성하였으며 MIT 라이센스를 따르는 정적 사이트 생성기이다.\n\n(Github 공동 설립자가 만들어서 Github 와 기본 연동이 되는 건가)\n\n## Create Repository \n\n도입부에서 언급했듯이 Github User name 에 맞게 생성해주면된다.\n\ne.g.\nUser ID : jayleekr\nRepository URL : https://github.com/jayleekr/jayleekr.github.io\nBlog URL : https://github.com/jayleekr\n\n저장소를 만들었으면 Local 에서 작업하여 Remote 에 Push 하는 형태로 작업할 계획이니, 블로그 저장소를 Local에 Clone 하자 \n\n## Create Blog Site\n\nJekyll 을 이용하여 사이트를 생성하는 방식은 크게 두가지이다.\n1. Local 에 Ruby, Bundler, 기타 gem Library들을 설치하여 Blog 를 생성하여 Remote에 Push하는 방법  \n2. 애초에 jekyll 공식 저장소에서 Fork 해올때 블로그 화 할수있는 Repository 이름 형태로 땡겨오는 방법\n\n> (필자는 2번 방식으로 Fork 해와서 기본적인 것들만 사용하다가, \n몇가지 애로사항으로 인하여 결국 2번방식으로 땡겨온 저장소를 Local에서 Debug 해가며 진행해야 했어서 1,2번 둘다 사용하고 있다)\n\n### Install miscellaneous \n\n참조 : https://poiemaweb.com/jekyll-basics\n\n### 특정 theme 설치\n\n필자는 jekyll-theme-chirpy 라는 jekyll 테마를 설치할 예정이다.\n~~이를 위해서는 jekyll installer 가 읽어드리는 설정파일들이 필요한데, 이러한 과정들이 번거로워 아예 해당 테마의 github source를 submodule 로 저장소에 추가하였다.~~\n* 참조 : https://git-scm.com/book/ko/v2/Git-%EB%8F%84%EA%B5%AC-%EC%84%9C%EB%B8%8C%EB%AA%A8%EB%93%88\n\n~~아래와 같은 커맨드로 submodule 로 저장소의 docs 라는 디렉토리를 jekyll의 chroot로 사용하도록 하면 끝이다.~~\n\n```sh\n$ git submodule add https://github.com/cotes2020/jekyll-theme-chirpy.git docs\n$ cat .gitmodules\n[submodule \"docs\"]\n\tpath = docs\n\turl = https://github.com/cotes2020/jekyll-theme-chirpy.git\n```\n\n~~위와 같이 jekyll theme 을 가져오면 해당 디렉토리에는 ruby on rails 에서 사용할 gemfile과 기본적인 설정파일들 및 Markdown으로 작성된 문서들이 복사 된다.~~\n\n처음에는 submodule로 설계했으나, submodule에 블로그를 포스팅\n\n### Init Jekyll\n\n위 예제 처럼 docs 디렉토리에 jekyll-chirpy 테마가 설치된 스켈레톤들이 설치됐다고 가정하자.\n아래와 같이 실행하면, 해당 설정파일로 만든 Web Page 가 로컬호스트에서 동작한다.\n\n``` sh\n$ bundle install\n$ bundle exec jekyll serve\nConfiguration file: /home/jayleekr/00_Projects/06_ADAS/docs/_config.yml\n            Source: /home/jayleekr/00_Projects/06_ADAS/docs\n       Destination: /home/jayleekr/00_Projects/06_ADAS/docs/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n                    done in 0.487 seconds.\n                    Auto-regeneration may not work on some Windows versions.\n                    Please see: https://github.com/Microsoft/BashOnWindows/issues/216\n                    If it does not work, please upgrade Bash on Windows or run Jekyll with --no-watch.\n Auto-regeneration: enabled for '/home/jayleekr/00_Projects/06_ADAS/docs'\n    Server address: http://127.0.0.1:4000/\n  Server running... press ctrl-c to stop. \n```","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-09-11T05:10:00.000Z","url":"/blog/techsavvy/github/2020-09-11-github-pages로-블로그만들기/"},{"id":"techsavvy/github/2020-09-20-어떻게-저장소에-올리죠","title":"어떻게 저장소에 올리죠?","description":"","content":"## TLDR\n### 1. 내 계정 저장소로 fork 한다\n\n![Desktop View](/assets/img/post/2020-09-20/fork.png)\n\n### 2. 내 저장소에서 작업을 글을 쓴다\n\n![Desktop View](/assets/img/post/2020-09-20/my_posting.png)\n\n### 3. 빌드\n\n``` sh\n$ bash tools/build.sh\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/_config.yml\n          Cleaner: Removing /home/jayleekr/00_Projects/08_ADAS_main/_site...\n          Cleaner: Nothing to do for /home/jayleekr/00_Projects/08_ADAS_main/.jekyll-metadata.\n          Cleaner: Nothing to do for /home/jayleekr/00_Projects/08_ADAS_main/.jekyll-cache.\n          Cleaner: Nothing to do for .sass-cache.\n$ cd /home/jayleekr/00_Projects/08_ADAS_main/.container\n[INFO] Succeed! 5 category-pages created.\n[INFO] Succeed! 6 tag-pages created.\n[INFO] Success to update lastmod for 1 post(s).\n$ JEKYLL_ENV=production bundle exec jekyll b -d /home/jayleekr/00_Projects/08_ADAS_main/_site\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/.container/_config.yml\n          Source: /home/jayleekr/00_Projects/08_ADAS_main/.container\n     Destination: /home/jayleekr/00_Projects/08_ADAS_main/_site\nIncremental build: disabled. Enable with --incremental\n     Generating... \n                    done in 0.983 seconds.\nAuto-regeneration: disabled. Use --watch to enable.\n\nBuild success, the site files have been placed in '/home/jayleekr/00_Projects/08_ADAS_main/_site'.\n```\n\n### 4. 테스트\n\n``` sh\n$ bash tools/test.sh\nRunning [\"ScriptCheck\", \"LinkCheck\", \"ImageCheck\", \"HtmlCheck\"] on [\"_site\"] on *.html... \n\n\nRan on 30 files!\n\n\nHTML-Proofer finished successfully.\n```\n     \n### 5. 로컬 서버에서 실행\n\n``` sh\n$ bash tools/run.sh\n[INFO] Succeed! 5 category-pages created.\n[INFO] Succeed! 6 tag-pages created.\n[INFO] Success to update lastmod for 1 post(s).\n$ bundle exec jekyll s -l -o\nConfiguration file: /home/jayleekr/00_Projects/08_ADAS_main/.container/_config.yml\n          Source: /home/jayleekr/00_Projects/08_ADAS_main/.container\n     Destination: /home/jayleekr/00_Projects/08_ADAS_main/.container/_site\nIncremental build: disabled. Enable with --incremental\n     Generating... \n                    done in 1.197 seconds.\n                    Auto-regeneration may not work on some Windows versions.\n                    Please see: https://github.com/Microsoft/BashOnWindows/issues/216\n                    If it does not work, please upgrade Bash on Windows or run Jekyll with --no-watch.\nAuto-regeneration: enabled for '/home/jayleekr/00_Projects/08_ADAS_main/.container'\nLiveReload address: http://127.0.0.1:35729\nServer address: http://127.0.0.1:4000/\nServer running... press ctrl-c to stop.\n```\n\n![Desktop View](/assets/img/post/2020-09-20/local.png)\n\n### 6. 동기\n\n``` sh\n$ git remote -v\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (fetch)\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (push)\n$ git remote add upstream https://github.com/ADAS-study-group/adas-study-group.github.io.git\n$ git fetch upstream\nremote: Enumerating objects: 157, done.\nremote: Counting objects: 100% (157/157), done.\nremote: Compressing objects: 100% (53/53), done.\nremote: Total 131 (delta 50), reused 122 (delta 44), pack-reused 0\nReceiving objects: 100% (131/131), 97.89 KiB | 301.00 KiB/s, done.\nResolving deltas: 100% (50/50), completed with 3 local objects.\nFrom https://github.com/ADAS-study-group/adas-study-group.github.io\n* [new branch]      gh-pages   -&gt; upstream/gh-pages\n* [new branch]      master     -&gt; upstream/master\n$ git pull upstream master \nFrom https://github.com/ADAS-study-group/adas-study-group.github.io\n* branch            master     -&gt; FETCH_HEAD\nUpdating 662969f..b075646\nFast-forward\n_posts/Blogging/StudySummary/2020-09-15-07BitbakeMetadata.md                                           | 315 +++++++++++++++++++++++++++++++++++++++++++++++++++\n...thub-Pages\\353\\241\\234-\\353\\270\\224\\353\\241\\234\\352\\267\\270\\353\\247\\214\\353\\223\\244\\352\\270\\260.md\" |   9 +-\n2 files changed, 320 insertions(+), 4 deletions(-)\ncreate mode 100644 _posts/Blogging/StudySummary/2020-09-15-07BitbakeMetadata.md\n```\n\n### 7. 내 저장소에 글을 업로드\n\n**Commit 시 Signature를 꼭넣자**\n\n``` sh\n$ git add _posts assets/img/post/\n$ git commit -s -m \"2020-09-20 Posting\"\n$ git push\n```\n\n### 8. PR(Pull Requests)\n\n- 그룹의 Github로 가서 Pull Requests 페이지로 가서 PR을 신청\n\n![Desktop View](/assets/img/post/2020-09-20/pr1.png)\n![Desktop View](/assets/img/post/2020-09-20/pr2.png)","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-09-20T05:10:00.000Z","url":"/blog/techsavvy/github/2020-09-20-어떻게-저장소에-올리죠/"},{"id":"techsavvy/github/2020-10-05-그룹저장소와-fork한-저장소-동기화하기","title":"저장소끼리 동기화하기","description":"","content":"## TLDR\n\n내 저장소(fork) : https://github.com/jayleekr/adas-study-group.github.io\n\n그룹 저장소 : https://github.com/ADAS-study-group/adas-study-group.github.io\n\n### 0. 로컬에 내 저장소(fork)를 땡겨오자.\n\n~~이미 했으면 안해도 되는거 아시쥬?~~\n\n``` sh\n$ git clone https://github.com/jayleekr/adas-study-group.github.io.git forked_repo\n$ cd forked_repo\n```\n\n### 1. 로컬에 있는 내 저장소(fork)에 remote 설정을 한다.\n\n``` sh\n$ git remote add upstream https://github.com/ADAS-study-group/adas-study-group.github.io.git\n\n* 확인\n$ git remote -v\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (fetch)\norigin  https://github.com/jayleekr/adas-study-group.github.io.git (push)\nupstream        https://github.com/ADAS-study-group/adas-study-group.github.io.git (fetch)\nupstream        https://github.com/ADAS-study-group/adas-study-group.github.io.git (push)\n```\n\n### 2. 내 저장소(fork)에 원본 저장소를 병합(merge)한다.\n\nFetch 먼저\n\n``` sh\n$ git fetch upstream\nremote: Enumerating objects: 172, done.\nremote: Counting objects: 100% (172/172), done.\nremote: Compressing objects: 100% (56/56), done.\nremote: Total 148 (delta 67), reused 135 (delta 57), pack-reused 0\n오브젝트를 받는 중: 100% (148/148), 1.33 MiB | 1.30 MiB/s, 완료.\n델타를 알아내는 중: 100% (67/67), 로컬 오브젝트 4개 마침.\nhttps://github.com/ADAS-study-group/adas-study-group.github.io URL에서\n * [새로운 브랜치]   gh-pages   -&gt; upstream/gh-pages\n * [새로운 브랜치]   master     -&gt; upstream/master\n``` \n\nMerge ㄱㄱ\n\n``` sh\n$ git merge upstream/master\n```\n\nPush ㄱㄱ (fork 저장소로)\n\n``` \n$ git push\n오브젝트 개수 세는 중: 7, 완료.\nDelta compression using up to 24 threads.\n오브젝트 압축하는 중: 100% (6/6), 완료.\n오브젝트 쓰는 중: 100% (7/7), 1.40 KiB | 1.40 MiB/s, 완료.\nTotal 7 (delta 4), reused 0 (delta 0)\nremote: Resolving deltas: 100% (4/4), completed with 4 local objects.\nTo https://github.com/jayleekr/adas-study-group.github.io.git\n   203f2c0..d7f0922  master -&gt; master\n```\n\n## Reference\n\n1. https://hyunjun19.github.io/2018/03/09/github-fork-syncing/","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-10-04T15:00:00.000Z","url":"/blog/techsavvy/github/2020-10-05-그룹저장소와-fork한-저장소-동기화하기/"},{"id":"techsavvy/github/2020-10-08-adding-gpg-signature","title":"GPG Signature를 Github에 추가하자","description":"","content":"## About GPG\n\nGPG는 GNU Privacy Gourd 의 약자로 PGP라고 불리기도 합니다. :D\n\n강력한 암호화 프로그램으로서 Ubuntu를 설치할때 기본적으로 포함되는 패키지이도 하죠!\n\nGithub는 오픈소스 저장소이기 때문에 누구나 clone을 통하여 데이터에 접근이 가능합니다.\n\n다만 개인저장소와 같은 Private 한 기능도 제공하는데,\n이러한 저장소에 Push, Clone, Pull 을 해가며 공동작업을 하는 데이터를 누군가가 옅보는 일은 매우 불쾌한 일이죠.\n\n그래서 Github 에서는 Pull Request시 GPG Signature와 함께 commit 하는 것을 추천합니다. (반강제)\n\n그럼 GPG Signature 를 어떻게 추가하는지 아래에서 간략하게 소개하겠습니다.\n\n자세한 내용은 Reference 에 링크 남겨둘테니 참조해주세요.\n\n\n## 1. GPG Key 생성\n\n``` sh\n$ gpg --full-generate-key\ngpg (GnuPG) 2.2.4; Copyright (C) 2017 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\nYour selection? \n```\n\n저같은 경우에는 RSA 암호방식(default)를 사용했습니다.\n\n``` sh\nRSA keys may be between 1024 and 4096 bits long.\nWhat keysize do you want? (3072) 4096\nRequested keysize is 4096 bits\n```\n\nGithub에서는 최소 4096 bits를 원합니다. 그래서 4096 ㄱㄱ\n\n``` sh\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      <n>  = key expires in n days\n      <n>w = key expires in n weeks\n      <n>m = key expires in n months\n      <n>y = key expires in n years\nKey is valid for? (0) 0\n```\n\n마지막으로 귀찮아서 expired 기간은 없앴습니다. 0 ㄱㄱ\n\n``` sh\nGnuPG needs to construct a user ID to identify your key.\n\nReal name: Jay Lee\nEmail address: jayleekr0125@gmail.com\nComment: lol\nYou selected this USER-ID:\n    \"Jay Lee (lol) <jayleekr0125@gmail.com>\"\n\nChange (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\nWe need to generate a lot of random bytes. It is a good idea to perform\nsome other action (type on the keyboard, move the mouse, utilize the\ndisks) during the prime generation; this gives the random number\ngenerator a better chance to gain enough entropy.\n```\n\n이름 email 등의 정보를 채워넣고 진행하면 이제 gpg key 가 생성됩니다.\n\n아 참고로 암호는 까먹지 마세요^^;;\n\n## 2. GPG Key 확인\n\n``` sh\n$ gpg --list-secret-keys --keyid-format LONG\ngpg: checking the trustdb\ngpg: marginals needed: 3  completes needed: 1  trust model: pgp\ngpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u\n/home/jayleekr/.gnupg/pubring.kbx\n---------------------------------\nsec   rsa4096/037ED189F6F42EF3 2020-10-08 [SC]\n      A95244C509A02D9F0790CFB0037ED189F6F42EF3\nuid                 [ultimate] Jay Lee (lol) <jayleekr0125@gmail.com>\nssb   rsa4096/38DDEF7B4E6758A4 2020-10-08 [E]\n```\n\n위는 제 GPG Key ID 등의 정보입니다.  \n제 GPG Key ID 는 037ED189F6F42EF3 가 되는거죠.\n\n그리고 공개키 암호화 파일의 위치는  /home/jayleekr/.gnupg/pubring.kbx 네요. \n(sshkey 처럼 개인적으로 백업해두었습니다.ㅎㅎ)\n\n그럼 이제 제 해당 ID로 공개키 (Public Key)를 만들어 볼까요?\n\n## 3. 공개키 생성\n\n생성을 위해서는 아래와 같이 ID를 넣어주면됩니다.\n\n``` sh\n$ gpg --armor --export 037ED189F6F42EF3\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n...\n...\n-----END PGP PUBLIC KEY BLOCK-----\n```\n\n위 처럼 BEGIN부터 END 가 나오게 되는데요 이를 Github 개인 Setting 에 복사해주시면 끝이납니다.\n\n## 4. GPG 공개키 Github에 복사\n\n![Desktop View](/assets/img/post/2020-10-08/userbar-account-settings.png)\n\n![Desktop View](/assets/img/post/2020-10-08/settings-sidebar-ssh-keys.png)\n\n![Desktop View](/assets/img/post/2020-10-08/gpg-key-paste.png)\n\n## 5. 내 컴퓨터의 Git에게 내 GPG Key를 알려주기\n\n우리는 아래와 같이 나의 GPG ID 를 확인 할 수 있었다.\n\n``` sh\n$ gpg --list-secret-keys --keyid-format LONG\ngpg: checking the trustdb\ngpg: marginals needed: 3  completes needed: 1  trust model: pgp\ngpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u\n/home/jayleekr/.gnupg/pubring.kbx\n---------------------------------\nsec   rsa4096/037ED189F6F42EF3 2020-10-08 [SC]\n      A95244C509A02D9F0790CFB0037ED189F6F42EF3\nuid                 [ultimate] Jay Lee (lol) <jayleekr0125@gmail.com>\nssb   rsa4096/38DDEF7B4E6758A4 2020-10-08 [E]\n```\n\n이제 git 에게 이 key를 알려주도록하자.\n\n``` sh\n$ git config --global user.signingkey 037ED189F6F42EF3\n```\n\n내 터미널(tty)에게도 알려주자!\n\n```sh\n$ test -r ~/.bash_profile && echo 'export GPG_TTY=$(tty)' &gt;> ~/.bash_profile\n$ echo 'export GPG_TTY=$(tty)' &gt;> ~/.profile\n```\n\n## 6. commit 시 GPG Signature 와 함께 올리는 법\n\n간단하다 -S 옵션을 추가하자!\n\n``` sh\n$ git commit -S -m \"#IssueNumber\"\n```\n\n이제 GPG Key 를 생성할때 쳤던 암호를 입력하면 commit이 완료 됩니다.\n\n## Reference\n\n1. https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/about-commit-signature-verification","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-10-06T15:00:00.000Z","url":"/blog/techsavvy/github/2020-10-08-adding-gpg-signature/"},{"id":"techsavvy/github/2020-11-10-blog-관련-faq","title":"Github Blog 관련 FAQ","description":"","content":"## 1. 구글 검색 활성화\n\n### 1.1 Activate sitemap\n\nModify _config.yml\n``` yml\nplugins: ['jekyll-paginate', 'jekyll-sitemap', 'jekyll-include-cache', 'jekyll-gist']\n```\n\n### 1.2 Add robots.txt\n\n``` txt\nUser-agent: *\nAllow:/\n\nSitemap: https://jayleekr.github.io/sitemap.xml\n```\n\n### 1.3 Register sitemap to google search console\n\n1. Go to https://www.google.com/webmasters\n2. Click **SEARCH CONSOLE**\n\n## 2. Jekyll 관련 \n\n### 2.1 로컬에서 blog를 테스트하기위한 Dependency들 설치\n\n``` sh\n$ sudo apt-get install -y gem \n$ sudo gem install jekyll bundler tzinfo tzinfo-data minima \n```\n\n### 2.2 Theme 설치\n\nminimal-mistakes 를 설치해보도록 하겠다.\n\n1. 먼저 Gemfile 에 설치하고자 하는 minimal-mistakes 테마를 추가해주자\n\n``` sh\ngem \"minimal-mistakes-jekyll\"\n```\n\n2. bundle을 설치하자.\n``` sh\n$ bundle install \n```\n3. _config.yml 파일에 theme를 추가하자\n``` yml\ntheme : minimal-mistakes-jekyll\n```\n\n### 2.3 Jekyll 실행\n``` sh\n$ jekyll serve\nConfiguration file: /home/jayleekr/workspace/00_codes/07_blog/_config.yml\n            Source: /home/jayleekr/workspace/00_codes/07_blog\n       Destination: /home/jayleekr/workspace/00_codes/07_blog/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.823 seconds.\n Auto-regeneration: enabled for '/home/jayleekr/workspace/00_codes/07_blog'\n    Server address: http://127.0.0.1:4000\n  Server running... press ctrl-c to stop.\n```\n\n### 2. Jekyll 버전 호환이슈\n\n프로젝트의 버전과 설치한 Jekyll의 버전이 다를때 발생한다.\n\nGem 의 의존성을 모두 제거하면 해결가능하다.\n``` sh\n$ sudo gem install bundler\n$ bundle install\n$ bundle exec jekyll serve\nbundle exec jekyll serve\nConfiguration file: /home/jayleekr/workspace/00_codes/07_blog/_config.yml\n            Source: /home/jayleekr/workspace/00_codes/07_blog\n       Destination: /home/jayleekr/workspace/00_codes/07_blog/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.823 seconds.\n Auto-regeneration: enabled for '/home/jayleekr/workspace/00_codes/07_blog'\n    Server address: http://127.0.0.1:4000\n  Server running... press ctrl-c to stop.\n```","categories":["TechSavvy","Github"],"tags":["TechSavvy","Github","ProgrammingLanguage","Yocto"],"pubDate":"2020-11-09T15:00:00.000Z","url":"/blog/techsavvy/github/2020-11-10-blog-관련-faq/"},{"id":"techsavvy/linuxkernel/2023-01-09-devmapper-and-overlay","title":"Devmapper and Overlay, Understanding the Differences","description":"","content":"# Devmapper and Overlay: Understanding the Differences\n\nIn the world of Linux kernel features, two names that often come up in discussions about storage and containerization are devmapper and overlay. Although they may sound similar, they are actually quite different and serve distinct purposes. In this post, we will take a closer look at what each of these features does and how they can be used.\n\n## Devmapper: Virtual Block Devices for Advanced Storage Configurations\n\nDevmapper, short for \"device mapper,\" is a kernel framework that allows you to create virtual block devices, called logical volumes. These logical volumes can be mapped to physical devices or other logical volumes, and can be used for a variety of purposes. Some of the things you can do with devmapper include:\n\n- Creating RAID arrays\n- Taking snapshots of logical volumes\n- Implementing thin provisioning\n- Encrypting block devices at the kernel level\n\nWhen you create a logical volume with devmapper, it appears as a regular block device to the rest of the system. You can format it with a filesystem or use it as a raw block device for other purposes. One of the most common use cases for devmapper is creating encrypted block devices, which can be used to protect sensitive data. Another use case is creating logical volumes that span multiple physical devices, which can be useful for creating large storage spaces or implementing redundancy.\n\n## OverlayFS: A Filesystem for Overlaying Directories\n\nOverlayFS, on the other hand, is a filesystem that allows you to overlay one directory on top of another. This can be useful for creating a unified view of a read-only filesystem and a writable filesystem. For example, when using containerization, you can use an image of a read-only filesystem as the lower layer and a container's writable filesystem as the upper layer. This way, when a file is accessed, it will be looked up in the upper layer first and if it is not present, it will be looked up in the lower layer. Writes will go to the upper layer, which is the container's writable filesystem. This allows you to use a single image for multiple containers and have them all share a read-only filesystem but have their own writable layer.\n\n## Conclusion\n\nIn summary, devmapper and overlay are two powerful Linux kernel features that can be used for different purposes. Devmapper allows you to create virtual block devices for advanced storage configurations, while OverlayFS is a filesystem that allows you to overlay one directory on top of another. Understanding the differences between these two features can help you make better decisions when designing your storage and containerization solutions.","categories":["TechSavvy","LinuxKernel"],"tags":["Blogging","Linux","Docker","EmbeddedLinux","Overlay","Devmapper","kernel","filesystem"],"pubDate":"2023-01-08T15:00:00.000Z","url":"/blog/techsavvy/linuxkernel/2023-01-09-devmapper-and-overlay/"},{"id":"techsavvy/yocto/2023-11-30-yocto-configuration","title":"Yocto Configuration -1","description":"","content":"# Yocto Configuration\n\nYocto project를 이용하여 뭔가 일을 진행해야하는 경우에는 보통 나, 혹은 우리 회사가 만든 Software를 레시피화 하고 이를 포함한 다양한 레이어들을 합쳐,\n하나의 리눅스 이미지를 만들어 사용하게된다. 이때, Yocto Project는 다양한 레이어들을 합쳐서 하나의 이미지를 만들어주는 역할을 한다.\n\n그리고 생각해보면 우리의 Software는 다양한 환경(아키텍쳐)에서 동작해하는 경우도있고, 같은 소스코드를 기반으로 Configuration만 다르게하여 빌드해야하는 경우도 있다.\n\nYocto는 현재 Automotive 업계에서 많이 사용되므로 Automotive 업계에서 사용되는 Yocto의 Configuration을 예를 들어보자.\n\n현재 우리가 빌드해야하는 소프트웨어 모듈의 이름이 TestModule이라고 가정하고 TestModule은 한차종만 지원해야하는 것이아니라 다양한 차종을 지원해야한다고 생각해보자.\n\n그렇다면 우리는 TestModule을 빌드할때, 다양한 차종을 지원하기 위해 다양한 Configuration을 소스코드레벨부터 지원해야한다.\nTestModule이 Python이나 Bash같은 스크립트기반의 어플리케이션이면 좋겠지만, 대부분은 C/C++ 혹은 Rust, Go같은 언어로 작성되어있을 것이다.\n이는 소스코드 뿐아니라 Build를 하기위한 Makefile, CMake, Scons, Meson, Bazel등의 Build System도 다양하게 사용되고 있을 것이다.\n\n이를 Build System의 Configure Tool이라고 명하면, 이 Configure Tool에도 다양한 옵션들을 지원해야한다.\n\n즉 소스코드레벨, Configure tool 레벨, 그리고 Yocto 레벨에서도 다양한 Configuration을 지원해야한다.\n\nConfiguration의 설계가 참 중요하고 이 설계가 Yocto와 같은 High level의 Build Framework을 통해 관리가 된다면,\nBuild System의 Configure Tool을 통해 Configuration을 관리하는 것보다 훨씬 편리하고, 유연하게 Configuration을 관리할 수 있다.\n\n이번 포스트에서는 Yocto에서 Configuration을 어떻게 관리하는지에 대해 알아보고자한다.\n\n## Yocto Configuration\n\nYocto에서 Configuration을 관리하는 방법 여러가지가 있지만, 이번 포스트에서는 Yocto의 Bitbake를 이용하여 Configuration을 관리하는 방법에 대해 알아보고자한다.\n\n1. Yocto의 Bitbake를 이용하여 Configuration을 관리하는 방법\n2. Yocto의 Bitbake를 이용하지 않고, Yocto의 Bitbake를 이용하여 빌드할때, Bitbake에게 Configuration을 전달하는 방법\n\n\n다음 포스팅에 이어서 작성하도록 하겠다....\n\n\n## Reference\n\n* [Yocto Project Reference Manual](https://www.yoctoproject.org/docs/3.1.1/ref-manual/ref-manual.html)\n* 내 머리속","categories":["TechSavvy","Yocto"],"tags":["TechSavvy","ProgrammingLanguage","Yocto"],"pubDate":"2023-11-30T01:00:00.000Z","url":"/blog/techsavvy/yocto/2023-11-30-yocto-configuration/"},{"id":"techsavvy/yocto/2020-11-16-yoctobasics-1","title":"Yocto 빌드 시스템","description":"","content":"## 1. 들어가며\n\n본 문서에서는 필자가 Yocto Project의 빌드시스템 기반으로 빌드 및 배포하기 위해  스터디한 부분들 중 중요 개념들을 정리한다.\n\nYocto Project 의 공식홈페이지에 Mega Manual 이라고 써있을 정도로 무지막지한 메뉴얼 양을 자랑하기 때문에 이를 전부 다 깨우치는 것은 필자의 레벨에서는 의미가 크게 없다고 생각하기 때문에, 중요 부분들만 그림을 섞어 설명하도록 한다.\n\n친절하게 링크를 걸어둘터이니 볼테면 봐보시라 --&gt; [mega-manual](https://www.yoctoproject.org/docs/current/mega-manual/mega-manual.html)\n\n구글링을 통한 필자의 주관적 분석이 많이 섞여있으므로 더 객관적인 분석을 원한다면 위 링크를 참조 바란다.\n\n## 2. 재미로 보는 역사\n\nYocto Project는  **OpenEmbedded Project** 로 부터  그 근원이 있다. \n\n<center>[그림 설명] 고인물의 상징  [출처 : 구글 이미지검색]</center>\n![Desktop View](/assets/img/post/2020-11-16/oe.png)\n\n\nOpenEmbedded  Project는 필자에게 전자사전으로 익숙한 **Sharp** 가 **Open Source license**에 따라 해당 제품의 ROM Image를 공개하는데부터 시작한다고 전해진다. \n\n(라떼는 잘나갔읍니다..)\n\n<center>[그림 설명] ~~90년대 생은 잘 모를수도 있다~~  [출처 : 구글 이미지검색]</center>\n![Desktop View](/assets/img/post/2020-11-16/sharp.jpeg)\n\n이를 기반으로 2002년 OpenZaurus Project가 시작되었다. (Zaurus는 PDA이다)\n\n<center>[그림설명] OpenZaurus와 Sharp사의 Zaurus PDA [출처: 구글 이미지검색]</center>\n\n![Desktop View](/assets/img/post/2020-11-16/Openzaurus-logo.png)\n![Desktop View](/assets/img/post/2020-11-16/Sharp_Zaurus.jpg)\n\n시간이 발전함에 따라 우리에게 Ubuntu로 친숙한 Debian계열 기반의 패키지 관리 및 빌드방식을 채용한 OpenEmbedded Project 로 통합되어 발전되어왔다 한다.\n\nOpenEmbedded 는 shell이나 python 같은 script로 작성된 빌드 툴인 **Bitbake** 와 그의 빌드대상 명세인** metadata(recipe)**로 구분된다.\n\n(Bitbake 도 나중엔 너무 커져 2004년부터는 make 처럼 별도의 프로젝트로 분기되었다)\n\nOpenEmbedded Project는 전세계 개발자들에게 큰 인기를 얻어 만개에 가까운 recipe와 수백개의 machine를 지원하게 되었고, 그 성장세 만큼 관리가 어려워 지게 되어 이를 개선하기위한 여러 시도들이 시작된다.\n\n\n\n그 중 하나가 Embedded 스타트업 OpenedHand 가 2003년 시작한 Poky Linux Project 인데, 이 프로젝트에서는 수백개의 특정 필수 recipe들만들 선별하였고, QEMU 를 통한 가상환경 빌드 및 SDK 빌드 지원을 해주면서 급 인기를 끌기 시작했다.~~(OpenedHand 는 2008년 Intel에 M&A 에 됐다. 아무리 찾아봐도 얼마에 팔렸는지는.. )~~\n\n<center>[그림설명] Intel 의 스타트업 OpenedHand 인수홍보. 왠지모르게 행복해보인다.</center>\n![Desktop View](/assets/img/post/2020-11-16/openedhand.png)\n![Desktop View](/assets/img/post/2020-11-16/oh.jpg)\n\n\n시간은 흘러 2010년 Linux Foundation WG이 **Yocto Project** 를 발표한다. \nPoky Linux 기반으로 embedded linux 배포판을 만드는 것이 프로젝트의 핵심이다.\n\n2011년 부터는 너무 커버린 OpenEmbedded Project를 **OpenEmbedded-Core(OE-Core)**로 **Poky linux** 에서 분리하였다. (이전 OpenEmbedded 는 **OE-Classic**으로 불리운다)\n\nOE-Core 는 arm, x86 와 같은 주요 아키텍쳐지원에 그 중점을 두고, QEMU 지원에, X window에서 구동하는 Sato 기반 GUI 테스트 툴까지가 지원하는 범주이다.\n\n<center>[출처: Yocto Project Homepage]</center>\n![Desktop View](/assets/img/post/2020-11-16/yoctolayer.png)\n\n이렇게 오랜기간 덩치를 줄이고 쪼개고를 반복하여 위와같이 지금의 Layered Architecture를 가지게 되었고, 기존의 Push모델이 아닌 Pull Model로서 프로젝트가 발산할 가능성을 배제할 수 있게 되었다한다.\n\n\n## 3. Cross-build 개요\n\n자 위 역사부분에서 그 흐름을 이해했고 간단한 임베디드 개발이나 크로스 빌드를 해본 독자라면 3장 부터 이해가 빠를 것이다.\n\n위 역사에서 말했듯이 Yocto Project는 Poky Linux 기반으로 빌드 툴로서 Bitbake와 그 명세로 metadata(recipe)를 사용하고, Bitbake는 OpenEmbedded프로젝트의 핵심 기능이다.\n\n\n그러므로 우리는 이 Bitbake 에 대해 집중 분석할 필요가 있다.\n\n하지만...\n\n오랜 역사를 자랑하는 Bitbake의 모든 것을 알기엔 무리가 있고.. \n간략하게 도식화된 그림들을 통해 빌드하기 위해 필요한 중요 기능들만을 알아보기로하자.\n\n### 3.1 유저 설정\n\n역시 가장 처음은 역시 빌드환경 구축이겠다. \n\n크로스 빌드를 해본 사람이라면 이 부분이 가장 귀찮고 시간도 많이 소비하지만, 가장 중요한 영역이라는 것을 안다.\n\n아래 그림을 보자\n\n\n![Desktop View](/assets/img/post/2020-11-16/bitbake1.png)\n<center>[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]</center>\n\nPoky Project에서는 이 부분을 자동화 가능하도록 스크립트를 제공한다. 그게 바로 oe-init-build-env이다.\n\n스크립트를 실행하면 Build Directory가 생성되고 여기서 모든 빌드 관련 작업이 이루어진다.\n\n생성된 Build directory에 conf directory에는 **User Configuration**을 첨삭가능 하게 해두었고, 이는 bitbake command line 으로도 수정이 가능하다.\n\n첨삭하는 정보들은 먼저 방대한 metadata(recipe)들중 어떤것을 해당 빌드시스템에서 사용할 것인지와 타겟머신 설정, 빌드를 위해 필요한 Package들의 Download 경로, Cache 경로 등이 있다.\n\n### 3.2 소스 준비\n\n![Desktop View](/assets/img/post/2020-11-16/source-fetching.png)\n[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\n 기반이 다져졌으면 이제 우리만의 Software를 가져다가 Yocto Project 구조에 붙여야한다.\n\n이 부분을 행하는 recipe속 함수는 do_fetch와 do_unpack이다. 이 두 함수를 실제 원본 소스를 Build Directory 내에 Working Directory를 생성하여 그 안에 복사한다.\n\n이 방법 같은 Source로 부터 다양한 Architecture와 OS를 지원하기 위해 고안된 구조이다.\n\n### 3.3 설정 & 컴파일 & 스테이징\n\n![Desktop View](/assets/img/post/2020-11-16/configuration-compile-autoreconf.png)\n\n\n[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\nSoftware의 원본 소스를 가져오는 작업까지 마쳤다면 그 다음 과정은 컴파일과 설치 과정이다.\n\n이 과정은 독자가 CMake나 Autotool 같은 빌드 툴을 안다면 좀 더 친숙할 것이다.\n\n먼저 **do_prepare_recipe_sysroot** 함수가 크로스 빌드를 위한 두개의 sysroot를 Working Directory에 위치시킨다. (target **sysroot** 와 **sysroot-native** 두가지)\n\n**do_configure** 를 통하여 해당 **원본 소스(S)**에서 Compile을 위해 필요로 하는 빌드 설정 파일들을 B**uild Directory(B)**에추출한다(일반적 cmake의 과정에 해당한다).\n\n**do_compile** 과정은 **Build Directory**에서 컴파일을 진행한다(일반적인 make과정에 해당한다).\n\n**do_install** 과정은 이렇게 컴파일된 파일들을 설치 **대상 목적지(D)**에 위치시킨다.\n\n### 3.4 패키지 분류\n\n\n![Desktop View](/assets/img/post/2020-11-16/analysis-for-package-splitting.png)\n\n[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\n소스들을 컴파일 및 인스톨까지 잘 마무리 됐다면 이제 이를 어떻게 포장해서 배포할지가 남았다. \n\nYocto Project에서는 3가지 형태의 배포 형태를 지원하는데 이는 **rpm** ,**deb**,**ipk**  이다.\n\n**do_package, do_packagedata** 두 함수는 설치 목적지인 D에 있는 파일들을 쪼개고 분류하여 포장한다.\n\nPackage들을 쪼개는 것은 우리가 Ubuntu 에서 python을 apt 패키지로 설치할때  python, python-dev, python-3.6 등 다양한 방식으로 존재하는것을 빗대어 이해하면 좋다.\n\n### 3.5 이미지 생성\n\n![Desktop View](/assets/img/post/2020-11-16/image-generation.png)\n\n[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\n패키지들도 잘 만들어졌다면 우린 이제 이를 Bitbake를 이용하여 이미지의 rootfs (Root file system)에 넣을 수 있다.\n\n먼저 do_rootfs 함수는 위 과정에서 만들어진 패키지를 설치한 Image의 Rootfs을 생성한다.\n\n이 과정 안에는 몇몇 중요 변수들이 있는데 매우 중요하기 때문에 짚고 넘어가도록 하자.\n\n* IMAGE_INSTALL : 우리가 만든 Package모음(Package Feeds area)에서 이미지에  포함시킬 패키지를 리스트업 한다\n* PACKAGE_EXCLUDE : 설치되지 말아야할 것들을 리스트업 한다\n* PACKAGE_CLASSES : 사용할 패키지의 종류(rpm, deb, ipk)를 선택한다\n* PACKAGE_INSTALL : 이미지에 설치될 최종 패키지 리스트이다\n\n패키지 설치를 완료하고 나서는 일종의 후처리 작업(Post-process) 을 할 수 있다.\n\n후 처리 작업에서는 manifest file을 만들어내기도하고 특정 스크립트들을 돌리는데, 목적은 대부분이 테스트 용도이다.\n\nManifest file은 Poky에서 지원하는 Qemu와 같은 가상환경 뿐아니라 실제 Target 환경에서 테스트 자동화를 위해 쓰인다. (자세한 사항은 testimage*.bbclasss나 testsdk.class 참조)\n\n배포를 위한 V-cycle 중 통합테스트(Integration Test)나 더 나아가 시스템테스트(System Test) 구축에 유용할 것이다.\n\n후처리 작업도 끝났다면 이제 우리는 드디어 Target에 올릴 이미지를 생성할 준비가 되었다.\n\ndo_image 함수가 그 역할을 하는데, File System(ext4, fat32 등)마다 다른식으로 할수 있도록 내부적으로 do_image_* 를 제공한다.\n\n### 3.6 SDK 생성\n\n<center> [설명: 데브옵스의 영역???..???? 전부다인가] [출처: 패스트캠퍼스]</center>\n![Desktop View](/assets/img/post/2020-11-16/devops.png)\n\n\n그냥 당연히 되야한다고 생각하는 이들이 대부분일 것이고, 실제로 위 언급한 대부분의 과정은 DevOps의 영역이다.\n\n기능 개발자에게는 크로스 컴파일를 위해 이미지 생성까지 한다는 것은 엄청난 시간 낭비고 자원 낭비이므로 DevOps는 SDK형태로 그들이 쉽게 컴파일 할 수 있는 환경을 만들어 줘야한다.\n\n이를 구축하는 것은 매우 귀찮은 일이지만 생산성 측면에서 생각해보면 필수적이다. 때문에 위 역사에서 언급했듯이 Poky Linux 프로젝트에서도 이런점을 고려하여 기능으로 제공한다.\n\n![Desktop View](/assets/img/post/2020-11-16/sdk-generation.png)\n\n[출처: https://www.yoctoproject.org/docs/3.1/overview-manual]\n\n\n\n상위 과정들에서 패키지화가 잘되었다고 가정하자.\n\nYocto Project를 이용한다면 이러한 패키지들을 do_populate_sdk 혹은 do_populate_sdk_ext를 이용하여 SDK로 쉽게 만들 수 있다.\n\nSDK설치파일은 통상적으로 위 그림과 같이 /build/tmp/depoly/ *.sh 로 만들어 지고, \n\n해당 파일만 있다면 개발자는  Cross-build 환경을 쉽게 가져갈 수 있다.\n\n![Desktop View](/assets/img/post/2020-11-16/cross-development-toolchains.png)\n\n\n### 4. 마무리하며\n\n2020년 말 기준으로 자동차의 시스템 플랫폼은 레벨은 자동차 OEM들 뿐아니라 전세계 IT공룡들, 전자, 반도체 회사들 이 득달같이 달려드는 뜨거운 감자이다.\n\n차량뿐아니라 IoT의 세상도 Linux 기반의 Kernel 을 가진다는 것을 감안하면 Yocto 와 같은 Open Source Project기반 통합 빌드환경을 제공하는 에코 시스템은 더욱어 각광 받을 것으로 보인다.\n\n실제로 Linkedin으로 살펴보면 VW Group, BMW Group, Hyundai-Motors Group, Toyota Group 뿐아니라 차량반도체 칩제소사들을 포함한 Amazon, Facebook 등 IT회사들에서도 Yocto Project에 익숙한 Senior 급의 Build Architect 혹은 Build Engineer 를 뽑는다.\n\n아무래도 Yocto 프로젝트가 다루는 영역이 Kernel부터 Application 까지 묶어서 보기 때문에 능숙히 다룰줄 알아야하는 컴퓨터 언어만해도 최소 3개이상이어야 하기 때문에 엄청난 양의 스터디를 요구한다.\n\n본인 역시 Yocto 빌드 시스템을 공부하며 참 힘들었는데 필자처럼 맨땅에서 공부하는 이들에게 이 기고글이 많은 도움이 되었길 바라며 글을 마무리 한다.","categories":["TechSavvy","Yocto"],"tags":["TechSavvy","ProgrammingLanguage","Yocto"],"pubDate":"2020-11-16T01:00:00.000Z","url":"/blog/techsavvy/yocto/2020-11-16-yoctobasics-1/"},{"id":"about","title":"About","description":"소프트웨어 엔지니어이자 농구선수 Jay Lee에 대한 소개","content":"Software Engineer Sonatus Hyundai Autron Basketball Player","categories":["personal"],"tags":["about","profile"],"pubDate":"2025-08-05T01:54:42.311Z","url":"/about/"},{"id":"home","title":"Home","description":"Jay Lee의 개인 블로그 - 기술과 농구를 사랑하는 소프트웨어 엔지니어","content":"Software Engineer Jay Lee Blog Technology Basketball Development","categories":["home"],"tags":["home","blog"],"pubDate":"2025-08-05T01:54:42.311Z","url":"/"}];

  document.addEventListener('DOMContentLoaded', () => {
    const openSearchBtn = document.getElementById('open-search');
    const searchModal = document.getElementById('search-modal');
    const closeSearchBtn = document.getElementById('close-search');
    const searchInput = document.getElementById('global-search-input');
    const searchResults = document.getElementById('search-results');
    const searchStats = document.getElementById('search-stats');
    const noResults = document.getElementById('no-search-results');
    const recentSearches = document.getElementById('recent-searches');
    const recentSearchesList = document.getElementById('recent-searches-list');
    
    if (!openSearchBtn || !searchModal || !closeSearchBtn || !searchInput || !searchResults) {
      return;
    }

    let searchTimeout;
    const recentSearchesKey = 'global-search-recent';
    
    // Get recent searches from localStorage
    function getRecentSearches() {
      try {
        return JSON.parse(localStorage.getItem(recentSearchesKey) || '[]');
      } catch {
        return [];
      }
    }
    
    // Save recent search
    function saveRecentSearch(query) {
      if (!query.trim()) return;
      
      const recent = getRecentSearches();
      const filtered = recent.filter(item => item !== query);
      const updated = [query, ...filtered].slice(0, 5); // Keep only 5 recent searches
      
      localStorage.setItem(recentSearchesKey, JSON.stringify(updated));
      displayRecentSearches();
    }
    
    // Display recent searches
    function displayRecentSearches() {
      const recent = getRecentSearches();
      
      if (recent.length === 0) {
        recentSearches.classList.add('hidden');
        return;
      }
      
      recentSearches.classList.remove('hidden');
      recentSearchesList.innerHTML = recent.map(query => `
        <button 
          class="flex w-full items-center gap-2 rounded px-2 py-1 text-left text-sm text-gray-600 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700"
          onclick="performSearch('${query.replace(/'/g, "\\'")}')"
        >
          <svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
          </svg>
          ${query}
        </button>
      `).join('');
    }
    
    // Highlight search terms in text
    function highlightText(text, query) {
      if (!query.trim()) return text;
      
      const regex = new RegExp(`(${query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
      return text.replace(regex, '<mark class="bg-yellow-200 dark:bg-yellow-900 rounded px-1">$1</mark>');
    }
    
    // Search function
    function performSearch(query) {
      if (typeof query === 'string') {
        searchInput.value = query;
      } else {
        query = searchInput.value;
      }
      
      if (!query.trim()) {
        searchResults.innerHTML = '';
        searchStats.textContent = '결과를 보려면 검색어를 입력하세요';
        noResults.classList.add('hidden');
        displayRecentSearches();
        return;
      }
      
      // Filter and rank results
      const results = searchIndex.filter(item => {
        const searchText = `${item.title} ${item.description} ${item.content} ${item.categories.join(' ')} ${item.tags.join(' ')}`.toLowerCase();
        return searchText.includes(query.toLowerCase());
      }).sort((a, b) => {
        // Prioritize title matches
        const aTitle = a.title.toLowerCase().includes(query.toLowerCase());
        const bTitle = b.title.toLowerCase().includes(query.toLowerCase());
        
        if (aTitle && !bTitle) return -1;
        if (!aTitle && bTitle) return 1;
        
        // Then by recency (for blog posts)
        return new Date(b.pubDate) - new Date(a.pubDate);
      });
      
      // Update stats
      searchStats.textContent = `${results.length}개의 결과`;
      
      if (results.length === 0) {
        searchResults.innerHTML = '';
        noResults.classList.remove('hidden');
        recentSearches.classList.add('hidden');
        return;
      }
      
      noResults.classList.add('hidden');
      recentSearches.classList.add('hidden');
      
      // Display results
      searchResults.innerHTML = results.map(result => `
        <a 
          href="${result.url}" 
          class="block rounded-lg p-3 hover:bg-gray-50 dark:hover:bg-gray-700 transition-colors"
          onclick="saveRecentSearch('${query.replace(/'/g, "\\'")}')"
        >
          <h3 class="font-medium text-gray-900 dark:text-gray-100">
            ${highlightText(result.title, query)}
          </h3>
          ${result.description ? `
            <p class="mt-1 text-sm text-gray-600 dark:text-gray-400 line-clamp-2">
              ${highlightText(result.description, query)}
            </p>
          ` : ''}
          <div class="mt-2 flex flex-wrap gap-2">
            ${result.categories.slice(0, 2).map(cat => `
              <span class="inline-flex items-center rounded-full bg-primary-100 dark:bg-primary-900 px-2 py-1 text-xs font-medium text-primary-800 dark:text-primary-200">
                ${cat}
              </span>
            `).join('')}
            ${result.tags.slice(0, 3).map(tag => `
              <span class="inline-flex items-center rounded-full bg-gray-100 dark:bg-gray-700 px-2 py-1 text-xs font-medium text-gray-700 dark:text-gray-300">
                #${tag}
              </span>
            `).join('')}
          </div>
        </a>
      `).join('');
      
      saveRecentSearch(query);
    }
    
    // Event listeners
    openSearchBtn.addEventListener('click', () => {
      searchModal.classList.remove('hidden');
      searchInput.focus();
      displayRecentSearches();
    });
    
    closeSearchBtn.addEventListener('click', () => {
      searchModal.classList.add('hidden');
      searchInput.value = '';
      searchResults.innerHTML = '';
    });
    
    // Close modal on background click
    searchModal.addEventListener('click', (e) => {
      if (e.target === searchModal) {
        searchModal.classList.add('hidden');
        searchInput.value = '';
        searchResults.innerHTML = '';
      }
    });
    
    // Search input with debounce
    searchInput.addEventListener('input', () => {
      clearTimeout(searchTimeout);
      searchTimeout = setTimeout(() => {
        performSearch();
      }, 300);
    });
    
    // Keyboard shortcuts
    document.addEventListener('keydown', (e) => {
      // Cmd+K or Ctrl+K to open search
      if ((e.metaKey || e.ctrlKey) && e.key === 'k') {
        e.preventDefault();
        openSearchBtn.click();
      }
      
      // Escape to close search
      if (e.key === 'Escape' && !searchModal.classList.contains('hidden')) {
        closeSearchBtn.click();
      }
    });
    
    // Make performSearch globally available
    window.performSearch = performSearch;
  });
})();</script>  <script type="module">document.addEventListener("DOMContentLoaded",()=>{const e=document.getElementById("mobile-menu-button"),t=document.getElementById("mobile-menu");e&&t&&e.addEventListener("click",()=>{const n=e.getAttribute("aria-expanded")==="true";e.setAttribute("aria-expanded",(!n).toString()),n?t.classList.add("hidden"):t.classList.remove("hidden");const o=e.querySelector("svg:first-of-type"),d=e.querySelector("svg:last-of-type");o&&d&&(o.classList.toggle("hidden"),d.classList.toggle("hidden"))})});</script> <main class="min-h-screen flex items-center justify-center px-4" data-astro-cid-zetdm5md> <div class="max-w-2xl w-full text-center" data-astro-cid-zetdm5md> <!-- Animated 404 --> <div class="mb-8" data-astro-cid-zetdm5md> <div class="text-8xl md:text-9xl font-black text-primary-500 dark:text-primary-400 relative" data-astro-cid-zetdm5md> <span class="inline-block animate-bounce" style="animation-delay: 0ms" data-astro-cid-zetdm5md>4</span> <span class="inline-block animate-bounce mx-2" style="animation-delay: 150ms" data-astro-cid-zetdm5md>0</span> <span class="inline-block animate-bounce" style="animation-delay: 300ms" data-astro-cid-zetdm5md>4</span> <!-- Floating elements --> <div class="absolute -top-4 left-0 text-2xl animate-float" style="animation-delay: 0.5s" data-astro-cid-zetdm5md>✨</div> <div class="absolute -top-2 right-0 text-xl animate-float" style="animation-delay: 1s" data-astro-cid-zetdm5md>🎈</div> <div class="absolute top-8 left-1/4 text-lg animate-float" style="animation-delay: 1.5s" data-astro-cid-zetdm5md>🌟</div> </div> </div> <!-- Interactive Robot/Character --> <div class="mb-12" data-astro-cid-zetdm5md> <div class="w-32 h-32 mx-auto mb-6 relative" data-astro-cid-zetdm5md> <!-- Robot body --> <div class="w-24 h-24 bg-gradient-to-br from-primary-400 to-primary-600 rounded-2xl mx-auto relative animate-sway" data-astro-cid-zetdm5md> <!-- Robot eyes --> <div class="absolute top-6 left-4 w-3 h-3 bg-white rounded-full animate-blink" data-astro-cid-zetdm5md></div> <div class="absolute top-6 right-4 w-3 h-3 bg-white rounded-full animate-blink" style="animation-delay: 0.1s" data-astro-cid-zetdm5md></div> <!-- Robot mouth --> <div class="absolute bottom-6 left-1/2 transform -translate-x-1/2 w-8 h-1 bg-white rounded-full animate-talk" data-astro-cid-zetdm5md></div> <!-- Robot antennas --> <div class="absolute -top-2 left-8 w-1 h-4 bg-gray-400 rounded-full" data-astro-cid-zetdm5md></div> <div class="absolute -top-2 right-8 w-1 h-4 bg-gray-400 rounded-full" data-astro-cid-zetdm5md></div> <div class="absolute -top-4 left-8 w-2 h-2 bg-red-400 rounded-full animate-pulse" data-astro-cid-zetdm5md></div> <div class="absolute -top-4 right-8 w-2 h-2 bg-blue-400 rounded-full animate-pulse" style="animation-delay: 0.5s" data-astro-cid-zetdm5md></div> </div> <!-- Robot arms --> <div class="absolute -left-4 top-8 w-6 h-2 bg-primary-500 rounded-full animate-wave" data-astro-cid-zetdm5md></div> <div class="absolute -right-4 top-8 w-6 h-2 bg-primary-500 rounded-full animate-wave-reverse" data-astro-cid-zetdm5md></div> </div> <!-- Speech bubble --> <div class="relative bg-white dark:bg-gray-800 rounded-2xl p-4 shadow-lg border border-gray-200 dark:border-gray-700 max-w-xs mx-auto" data-astro-cid-zetdm5md> <div class="text-gray-700 dark:text-gray-300 font-medium" id="robot-speech" data-astro-cid-zetdm5md> 어라? 여기에 뭐가 있어야 하는데... </div> <!-- Speech bubble tail --> <div class="absolute -bottom-2 left-1/2 transform -translate-x-1/2 w-4 h-4 bg-white dark:bg-gray-800 border-l border-b border-gray-200 dark:border-gray-700 rotate-45" data-astro-cid-zetdm5md></div> </div> </div> <!-- Error message --> <div class="mb-8" data-astro-cid-zetdm5md> <h1 class="text-3xl md:text-4xl font-bold text-gray-800 dark:text-gray-200 mb-4" data-astro-cid-zetdm5md> 페이지를 찾을 수 없습니다 </h1> <p class="text-lg text-gray-600 dark:text-gray-400 mb-6 leading-relaxed" data-astro-cid-zetdm5md> 요청하신 페이지를 찾을 수 없습니다. 하지만 이 귀여운 404 페이지라도 만나보세요! </p> </div> <!-- Interactive Game --> <div class="mb-12 bg-gray-50 dark:bg-gray-800 rounded-2xl p-6 border border-gray-200 dark:border-gray-700" data-astro-cid-zetdm5md> <h3 class="text-xl font-semibold text-gray-800 dark:text-gray-200 mb-4" data-astro-cid-zetdm5md> 🎮 미니게임: 클릭해서 포인트를 모아보세요! </h3> <div class="flex justify-center items-center gap-8 mb-4" data-astro-cid-zetdm5md> <div class="text-center" data-astro-cid-zetdm5md> <div class="text-2xl font-bold text-primary-600 dark:text-primary-400" id="score" data-astro-cid-zetdm5md>0</div> <div class="text-sm text-gray-500" data-astro-cid-zetdm5md>점수</div> </div> <button id="click-game-btn" class="w-20 h-20 bg-gradient-to-br from-yellow-400 to-orange-500 rounded-full text-white text-3xl hover:scale-110 transition-all duration-200 shadow-lg hover:shadow-xl focus:outline-none focus:ring-4 focus:ring-yellow-300 dark:focus:ring-yellow-600" data-astro-cid-zetdm5md>
🍪
</button> <div class="text-center" data-astro-cid-zetdm5md> <div class="text-2xl font-bold text-green-600 dark:text-green-400" id="clicks-per-second" data-astro-cid-zetdm5md>0</div> <div class="text-sm text-gray-500" data-astro-cid-zetdm5md>CPS</div> </div> </div> <div class="text-sm text-gray-500 dark:text-gray-400" data-astro-cid-zetdm5md> 힌트: 빠르게 클릭하면 보너스 포인트를 받을 수 있어요! </div> </div> <!-- Navigation options --> <div class="space-y-4" data-astro-cid-zetdm5md> <!-- Search --> <div class="relative max-w-md mx-auto mb-6" data-astro-cid-zetdm5md> <input type="text" id="quick-search" placeholder="찾고 있던 내용을 검색해보세요..." class="w-full px-4 py-3 pl-10 bg-white dark:bg-gray-800 border border-gray-300 dark:border-gray-600 rounded-lg focus:outline-none focus:ring-2 focus:ring-primary-500 dark:focus:ring-primary-400 text-gray-800 dark:text-gray-200" data-astro-cid-zetdm5md> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-5 h-5 text-gray-400" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-zetdm5md> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" data-astro-cid-zetdm5md></path> </svg> </div> <!-- Action buttons --> <div class="flex flex-col sm:flex-row gap-4 justify-center items-center" data-astro-cid-zetdm5md> <a href="/" class="inline-flex items-center gap-2 px-6 py-3 bg-primary-600 hover:bg-primary-700 text-white rounded-lg font-medium transition-all duration-200 hover:scale-105 focus:outline-none focus:ring-4 focus:ring-primary-300 dark:focus:ring-primary-600" data-astro-cid-zetdm5md> <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-zetdm5md> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 12l2-2m0 0l7-7 7 7M5 10v10a1 1 0 001 1h3m10-11l2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6" data-astro-cid-zetdm5md></path> </svg> 홈으로 </a> <a href="/blog" class="inline-flex items-center gap-2 px-6 py-3 bg-gray-200 dark:bg-gray-700 hover:bg-gray-300 dark:hover:bg-gray-600 text-gray-800 dark:text-gray-200 rounded-lg font-medium transition-all duration-200 hover:scale-105 focus:outline-none focus:ring-4 focus:ring-gray-300 dark:focus:ring-gray-600" data-astro-cid-zetdm5md> <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-zetdm5md> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" data-astro-cid-zetdm5md></path> </svg> 블로그 </a> <button onclick="history.back()" class="inline-flex items-center gap-2 px-6 py-3 bg-gray-200 dark:bg-gray-700 hover:bg-gray-300 dark:hover:bg-gray-600 text-gray-800 dark:text-gray-200 rounded-lg font-medium transition-all duration-200 hover:scale-105 focus:outline-none focus:ring-4 focus:ring-gray-300 dark:focus:ring-gray-600" data-astro-cid-zetdm5md> <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-zetdm5md> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7" data-astro-cid-zetdm5md></path> </svg> 뒤로 </button> </div> </div> </div> </main> <footer class="bg-gray-900 dark:bg-gray-950 text-gray-300 dark:text-gray-400" data-astro-cid-sz7xmlte> <div class="max-w-6xl mx-auto px-4 py-12" data-astro-cid-sz7xmlte> <div class="grid grid-cols-1 md:grid-cols-4 gap-8" data-astro-cid-sz7xmlte> <!-- About Section --> <div class="md:col-span-2" data-astro-cid-sz7xmlte> <h3 class="text-lg font-semibold text-white mb-4" data-astro-cid-sz7xmlte> Jay Lee의 기술 블로그 </h3> <div class="flex items-center gap-4" data-astro-cid-sz7xmlte> <a href="https://github.com/jayleekr" target="_blank" rel="noopener" class="social-link" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>GitHub</span> <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20" data-astro-cid-sz7xmlte> <path fill-rule="evenodd" d="M10 0C4.477 0 0 4.484 0 10.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0110 4.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.203 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.942.359.31.678.921.678 1.856 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0020 10.017C20 4.484 15.522 0 10 0z" clip-rule="evenodd" data-astro-cid-sz7xmlte></path> </svg> </a> <a href="https://linkedin.com/in/jayleekr" target="_blank" rel="noopener" class="social-link" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>LinkedIn</span> <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20" data-astro-cid-sz7xmlte> <path fill-rule="evenodd" d="M16.338 16.338H13.67V12.16c0-.995-.017-2.277-1.387-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248H8.014v-8.59h2.559v1.174h.037c.356-.675 1.227-1.387 2.526-1.387 2.703 0 3.203 1.778 3.203 4.092v4.711zM5.005 6.575a1.548 1.548 0 11-.003-3.096 1.548 1.548 0 01.003 3.096zm-1.337 9.763H6.34v-8.59H3.667v8.59zM17.668 1H2.328C1.595 1 1 1.581 1 2.298v15.403C1 18.418 1.595 19 2.328 19h15.34c.734 0 1.332-.582 1.332-1.299V2.298C19 1.581 18.402 1 17.668 1z" clip-rule="evenodd" data-astro-cid-sz7xmlte></path> </svg> </a> <a href="mailto:jayleekr0125@gmail.com" class="social-link" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Email</span> <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" data-astro-cid-sz7xmlte> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 4.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" data-astro-cid-sz7xmlte></path> </svg> </a> </div> </div> <!-- Quick Links --> <div data-astro-cid-sz7xmlte> <h3 class="text-lg font-semibold text-white mb-4" data-astro-cid-sz7xmlte>
Quick Links
</h3> <ul class="space-y-2 text-sm" data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/blog" class="footer-link" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/about" class="footer-link" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/projects" class="footer-link" data-astro-cid-sz7xmlte>Projects</a></li> <li data-astro-cid-sz7xmlte><a href="/contact" class="footer-link" data-astro-cid-sz7xmlte>Contact</a></li> </ul> </div> <!-- RSS & Subscribe --> <div data-astro-cid-sz7xmlte> <h3 class="text-lg font-semibold text-white mb-4" data-astro-cid-sz7xmlte>
Subscribe
</h3> <ul class="space-y-2 text-sm" data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte> <a href="/rss.xml" class="footer-link flex items-center gap-2" target="_blank" rel="noopener" data-astro-cid-sz7xmlte> <svg class="w-4 h-4 text-orange-500" fill="currentColor" viewBox="0 0 20 20" data-astro-cid-sz7xmlte> <path d="M3.429 2.786c0.456 0 0.829 0.372 0.829 0.829s-0.372 0.829-0.829 0.829c-0.456 0-0.829-0.372-0.829-0.829s0.372-0.829 0.829-0.829zM3.429 7.143c3.304 0 5.976 2.672 5.976 5.976h1.659c0-4.220-3.415-7.635-7.635-7.635v1.659zM3.429 11.5c1.382 0 2.5 1.118 2.5 2.5h1.659c0-2.293-1.866-4.159-4.159-4.159v1.659z" data-astro-cid-sz7xmlte></path> </svg>
RSS Feed
</a> </li> <li data-astro-cid-sz7xmlte> <a href="/rss/en.xml" class="footer-link flex items-center gap-2" target="_blank" rel="noopener" data-astro-cid-sz7xmlte> <svg class="w-4 h-4 text-blue-500" fill="currentColor" viewBox="0 0 20 20" data-astro-cid-sz7xmlte> <path d="M3.429 2.786c0.456 0 0.829 0.372 0.829 0.829s-0.372 0.829-0.829 0.829c-0.456 0-0.829-0.372-0.829-0.829s0.372-0.829 0.829-0.829zM3.429 7.143c3.304 0 5.976 2.672 5.976 5.976h1.659c0-4.220-3.415-7.635-7.635-7.635v1.659zM3.429 11.5c1.382 0 2.5 1.118 2.5 2.5h1.659c0-2.293-1.866-4.159-4.159-4.159v1.659z" data-astro-cid-sz7xmlte></path> </svg>
English RSS
</a> </li> </ul> </div> </div> <div class="border-t border-gray-800 dark:border-gray-700 mt-8 pt-8 flex flex-col md:flex-row justify-between items-center" data-astro-cid-sz7xmlte> <p class="text-sm" data-astro-cid-sz7xmlte>
&copy; 2025 Jay Lee. All rights reserved.
</p> <div class="flex items-center gap-4 mt-4 md:mt-0 text-sm" data-astro-cid-sz7xmlte> <span class="text-gray-500" data-astro-cid-sz7xmlte> 제작: </span> <a href="https://astro.build" target="_blank" rel="noopener" class="footer-link" data-astro-cid-sz7xmlte>Astro</a> <span class="text-gray-600" data-astro-cid-sz7xmlte>•</span> <a href="https://tailwindcss.com" target="_blank" rel="noopener" class="footer-link" data-astro-cid-sz7xmlte>Tailwind CSS</a> </div> </div> </div> </footer>   <script type="module">document.addEventListener("DOMContentLoaded",()=>{let o=0,r=0,m=Date.now();const h=document.getElementById("score"),u=document.getElementById("clicks-per-second"),n=document.getElementById("click-game-btn"),d=document.getElementById("robot-speech"),s=document.getElementById("quick-search"),f=document.documentElement.lang,l={ko:["어라? 여기에 뭐가 있어야 하는데...","혹시 길을 잃으셨나요?","아, 404 에러라는 게 이런 거구나!","게임은 재미있나요? 😊","저도 페이지를 찾아보고 있어요!","이런, 여기서 뭘 찾고 계시는 거죠?","괜찮아요, 가끔 길을 잃기도 하는 법이에요!"],en:["Hmm... Something should be here...","Did you get lost?","Oh, so this is what a 404 error looks like!","Is the game fun? 😊","I'm also looking for the page!","Oops, what are you looking for here?","It's okay, we all get lost sometimes!"]};n&&n.addEventListener("click",()=>{r++;const e=(Date.now()-m)/1e3,i=Math.round(r/e);let c=1;i>5?c=3:i>3&&(c=2),o+=c,h.textContent=o,u.textContent=i,n.style.transform="scale(0.95)",setTimeout(()=>{n.style.transform="scale(1)"},100),o%50===0&&o>0&&(p(),a())});function a(){if(d){const t=l[f]||l.en,e=t[Math.floor(Math.random()*t.length)];d.textContent=e}}function p(){for(let t=0;t<10;t++)setTimeout(()=>{const e=document.createElement("div");e.textContent=["✨","🎉","⭐","💫"][Math.floor(Math.random()*4)],e.style.cssText=`
                position: fixed;
                left: ${Math.random()*window.innerWidth}px;
                top: ${Math.random()*window.innerHeight}px;
                font-size: 24px;
                pointer-events: none;
                z-index: 9999;
                animation: float 2s ease-out forwards;
              `,document.body.appendChild(e),setTimeout(()=>{e.parentNode&&e.parentNode.removeChild(e)},2e3)},t*100)}s&&s.addEventListener("keypress",t=>{if(t.key==="Enter"){const e=s.value.trim();e&&(window.location.href=`/?search=${encodeURIComponent(e)}`)}}),setInterval(a,1e4),setTimeout(a,3e3)});</script> </body> </html>
---
title: "리서치"
author: "Jay Lee"
pubDate: "2025-11-28T06:54:00.000Z"
lastEditedTime: "2025-11-28T07:16:00.000Z"
categories: ["DeepThinking", "Daily"]
tags: ["Daily","Thoughts"]
---

이제 충분한 정보를 수집했으니 정리해 드리겠습니다.


---


## LLM 능력 평가를 위한 테스트 프롬프트 방법론 종합 가이드


Jay, 검색 결과를 바탕으로 GPT, Gemini, Claude를 비교 테스트할 수 있는 가장 복잡하고 강력한 방법론들을 정리했습니다.


---


### 1. 다차원 추론 테스트 (Multi-Dimensional Reasoning)


**Chain-of-Thought (CoT) 기반 복합 추론**


```plain text
다음 문제를 단계별로 풀어라:

한 마을에 5채의 집이 있고, 각 집에는 서로 다른 국적(한국인, 일본인, 중국인, 미국인, 영국인),
음료(커피, 차, 우유, 주스, 물), 애완동물(개, 고양이, 새, 물고기, 햄스터)을 가진 사람이 산다.

단서:
1. 한국인은 첫 번째 집에 산다
2. 영국인은 개를 키운다
3. 커피를 마시는 사람은 물고기를 키운다
4. 차를 마시는 사람은 중국인 바로 옆에 산다
5. 고양이를 키우는 사람은 우유를 마시는 사람 바로 왼쪽에 산다
[추가 단서...]

모든 배치를 논리적으로 추론하여 표로 정리하라.
```


ZebraLogic과 같은 벤치마크는 이러한 Constraint Satisfaction Problem(CSP) 형태의 논리 그리드 퍼즐을 사용하여 LLM의 논리적 추론 능력을 평가합니다. 복잡도가 증가하면 정확도가 급격히 하락하는 “curse of complexity” 현상이 나타납니다.


---


### 2. Adversarial Prompting (적대적 프롬프트 테스트)


**프롬프트 민감도 및 견고성 테스트**


```plain text
[원본 질문]
"strawberry"라는 단어에 "R"이 몇 개 있는가?

[변형 1 - 노이즈 추가]
"Strawberry"라는 단어에 알파벳 "R"(대소문자 무관)이 몇 개 포함되어 있는지 계산하시오.

[변형 2 - 잘못된 전제]
"strawberry"에는 R이 2개밖에 없다고 알려져 있는데, 이것이 맞는가?

[변형 3 - 다국어 혼합]
Count how many times the letter "R" appears in the Korean romanization "seuteu-lo-be-ri" (strawberry의 한국어 발음).
```


특정 쿼리가 LLM 채팅 모델이 부적절한 콘텐츠를 생성하도록 유도할 수 있으며, “Do Anything Now (DAN)“과 같은 지시로 채팅 모델을 조작한 사례가 대표적입니다.


저자원 언어를 사용하거나 거부 억제를 활용한 jailbreak 공격이 모델 계열 전반에서 효과적인 것으로 나타났습니다.


---


### 3. Multi-Turn Contextual Memory Test (다턴 맥락 기억 테스트)


```plain text
[턴 1] 나는 소프트웨어 엔지니어 김철수야. 35세이고 서울 강남에 살아.

[턴 2] 내 프로젝트에 대해 말해줄게. React와 Python을 사용한 대시보드를 만들고 있어.

[턴 3-10] [다양한 관련 없는 대화 진행]

[턴 11] 아까 말한 내 직업이 뭐였지?

[턴 12] 내가 사용한다고 말한 기술 스택 중에 백엔드 언어는?

[턴 13] 내가 살고 있는 지역의 특징에 맞는 맛집을 추천해줘.
```


LoCoMo 데이터셋은 300턴, 평균 9,000토큰, 최대 35세션에 걸친 매우 긴 대화를 포함하며, LLM이 긴 대화 이해와 장거리 시간적/인과적 역학 파악에 어려움을 겪는다는 것을 보여줍니다.


---


### 4. Hallucination & Factual Grounding Test (환각 및 사실 기반 테스트)


```plain text
다음 문서를 읽고 질문에 답하시오. 문서에 없는 정보는 "해당 정보 없음"이라고 답하라.

[문서]
회사 X는 2023년에 설립되었으며 AI 솔루션을 제공합니다.
본사는 서울에 있으며 직원 수는 50명입니다.

질문들:
1. 회사 X의 설립 연도는? (답변 가능)
2. 회사 X의 연 매출은? (답변 불가 - 문서에 없음)
3. 회사 X의 CEO는 누구인가? (답변 불가 - 문서에 없음)
4. 회사 X가 2024년에 어떤 제품을 출시했는가? (답변 불가 - 문서에 없음)
```


Google DeepMind의 FACTS Grounding 벤치마크는 LLM이 제공된 소스 자료에 응답을 얼마나 정확하게 기반시키고 환각을 피하는지 측정합니다. 응답이 제공된 문서에 완전히 기반하고 환각이 없을 때만 사실적으로 정확하다고 판단됩니다.


GPT-3.5는 참조의 39.6%를 환각했고, Bard는 의료 분야 체계적 문헌 검토 시 91.4%라는 놀라운 환각률을 보였습니다.


---


### 5. Self-Consistency & Meta-Cognitive Test (자기 일관성 및 메타인지 테스트)


```plain text
[단계 1]
다음 수학 문제를 풀어라:
어떤 수에 7을 더하고 3을 곱한 후 15를 빼면 42가 된다. 원래 수는?

[단계 2]
네가 방금 제시한 답을 검증해봐. 계산 과정을 역으로 추적하여 확인하라.

[단계 3]
만약 네 첫 번째 답이 틀렸다면, 어디서 실수했는지 설명하고 수정하라.

[단계 4]
이 문제에 대해 너의 확신 수준을 0-100%로 표현하고,
그 이유를 설명하라.
```


대표적인 LLM 테스트 질문들: “유리컵에 구슬을 넣고, 컵을 뒤집어 테이블에 놓고, 다시 들어 전자레인지에 넣었다. 구슬은 어디 있는가? 추론 과정을 설명하라.”


---


### 6. LLM-as-Judge Evaluation Framework


**교차 모델 평가 방식**


```plain text
다음 응답의 품질을 1-10점으로 평가하고 근거를 제시하라:

[평가 기준]

- 정확성 (Accuracy): 사실적으로 올바른가?

- 완전성 (Completeness): 질문에 충분히 답했는가?

- 일관성 (Coherence): 논리적으로 연결되는가?

- 유용성 (Helpfulness): 실제로 도움이 되는가?

[원본 질문]
"양자 컴퓨팅의 기본 원리를 설명하라"

[평가 대상 응답]
[다른 모델의 응답 삽입]
```


LLM-as-Judge 평가에서 가장 큰 도전은 환각, 편향, 제한된 사실 기반입니다. Chain of thought를 사용하여 특정 점수를 부여한 이유를 설명하게 하는 것이 표준 점수화에 도움됩니다.


---


### 7. Code-Integrated Reasoning Test (코드 통합 추론 테스트)


```plain text
다음 요구사항을 분석하고 구현하라:

요구사항:
1. 주어진 JSON 데이터에서 특정 조건을 만족하는 항목 필터링
2. 필터링된 데이터의 통계 분석 (평균, 중앙값, 표준편차)
3. 결과를 시각화할 수 있는 형태로 변환

제약조건:

- 외부 라이브러리 사용 불가 (순수 Python만)

- 시간 복잡도 O(n log n) 이하

- 에러 핸들링 필수

테스트 데이터:
[JSON 데이터 제공]

추가 질문:
1. 코드의 시간 복잡도를 분석하라
2. 엣지 케이스를 3가지 이상 식별하라
3. 코드를 리팩토링하여 더 효율적으로 만들어라
```


---


### 8. 복합 시나리오 테스트 (Integrated Scenario Test)


```plain text
당신은 스타트업의 CTO입니다. 다음 상황을 분석하고 의사결정을 내리세요:

상황:

- 현재 서비스 사용자: 10,000명 (월 20% 성장)

- 서버 비용: 월 $5,000

- 개발팀: 5명

- 투자금: $500,000 (6개월 런웨이)

- 경쟁사가 유사 서비스를 3개월 후 출시 예정

과제:
1. [기술 분석] 현재 아키텍처를 마이크로서비스로 전환해야 하는가?
2. [재무 분석] 6개월 내 손익분기점 도달 가능성을 계산하라
3. [전략 분석] 경쟁사 대응 전략 3가지를 제시하라
4. [리스크 분석] 각 전략의 리스크를 평가하라
5. [최종 결론] 위 분석을 종합하여 행동 계획을 제시하라

각 분석에 대해:

- 가정을 명시할 것

- 불확실성을 인정할 것

- 대안을 제시할 것
```


---


### 실전 테스트 프레임워크 구현 방법


| 테스트 카테고리           | 평가 지표                       | 자동화 가능성 |
| ------------------ | --------------------------- | ------- |
| 논리 추론 (ZebraLogic) | 정답률, 셀별 정확도                 | 높음      |
| 적대적 견고성            | ASR (Attack Success Rate)   | 중간      |
| 장기 기억              | QA 정확도, 시간 추론               | 중간      |
| 환각 탐지              | FACTS Score, Grounding Rate | 높음      |
| 자기 일관성             | 답변 변동성, 신뢰 보정               | 높음      |
| 코드 생성              | 실행 정확도, 테스트 통과율             | 높음      |


---


### 추천 테스트 순서

1. **기본 추론**: 간단한 논리 퍼즐로 시작
2. **복잡도 스케일링**: 같은 유형의 문제를 점진적으로 어렵게
3. **적대적 테스트**: 프롬프트 변형으로 견고성 확인
4. **통합 테스트**: 여러 능력을 동시에 요구하는 복합 시나리오

원하시면 특정 카테고리에 대해 더 구체적인 프롬프트 템플릿을 만들어 드릴 수 있습니다!
